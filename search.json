[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management and Digital Archaeology",
    "section": "",
    "text": "Course overview\n040468 Datenmanagement und digitale Archäologie (ÜB, unterrichtet in Englisch) Übungen (3 CP)\nTime slot / Zeitfenster: Fr 14-16 Uhr c.t.\nPlace / Ort: Raum 2\nCourse instructors / Kursleiter:\nAndreas Angourakis\nTim Klingenberg\nSupport / Unterstützung: Thomas Rose",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#course-summarykurszusammenfassung",
    "href": "index.html#course-summarykurszusammenfassung",
    "title": "Data Management and Digital Archaeology",
    "section": "Course summary/Kurszusammenfassung:",
    "text": "Course summary/Kurszusammenfassung:\nThe “Data Management and Digital Archaeology” MA course offers a comprehensive overview of data management and digital tools essential for archaeological research under the open science paradigm. It begins with foundational sessions on data management and the use of Git and GitHub for version control, helping students organize, maintain, and share their research data and analyses.\nThe course then delves into programming with R, covering basic R programming skills and best practices, followed by advanced topics like count data analysis, seriation techniques, and compositional data analysis tailored for archaeology. We then explore essential concepts in database management, offering two sessions to provide introductory and more advanced knowledge necessary for handling archaeological databases. The course concludes with a brief, but practical introduction to Geographic Information Systems (GIS), introducing students to GIS software and techniques used to analyse spatial data in archaeological research.\nBy the end, students will be equipped with enough digital skills to effectively manage, analyse, and interpret archaeological data under an open science framework while also being prepared to continue their own learning journey in this ever-evolving field.",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#course-schedulekursplan",
    "href": "index.html#course-schedulekursplan",
    "title": "Data Management and Digital Archaeology",
    "section": "Course schedule/Kursplan:",
    "text": "Course schedule/Kursplan:\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\nGetting started\n\n\n1\n2024-10-18\nData and research data management\n\n\n2\n2024-10-25\nGit and GitHub\n\n\nProgramming in R\n\n\n3\n2024-11-08\nIntroduction to R\n\n\n4\n2024-11-15\nBest practices in programming\n\n\nData Science in R\n\n\n5\n2024-11-22\nData Science Workflow\n\n\n6\n2024-11-29\nCount data and seriation\n\n\n7\n2024-12-06\nCompositional data\n\n\nDatabases\n\n\n8\n2024-12-13\nDatabases (I)\n\n\n9\n2024-12-20\nDatabases (II)\n\n\nGIS\n\n\n10\n2025-01-10\nGIS (I)\n\n\n11\n2025-01-17\nGIS (II)",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#evaluation-kursbewertung",
    "href": "index.html#evaluation-kursbewertung",
    "title": "Data Management and Digital Archaeology",
    "section": "Evaluation / Kursbewertung",
    "text": "Evaluation / Kursbewertung\n(Attendance and final examination)",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Management and Digital Archaeology",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe conception of the course structure, as well as the short summaries, exercises, and images shown in each chapter, greatly benefited from Large Language Models used as companion writer and programmer. As such, we own greatly to the current richness of reference information freely available on Internet.\nThe models and services used are:\n\nChatGPT (GPT-4o) by OpenAI for brainstorming, text and code writing suggestions, collection and articulation of references.\nWebChatGPT, a free browser extension that enhances ChatGPT by providing Internet access directly within the chat interface, used to aid Internet search.\nLeonardo.ai (user tokens) for generating purely aesthetic visual assets.",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "session-rdm.html",
    "href": "session-rdm.html",
    "title": "1  Data and research data management",
    "section": "",
    "text": "1.1 Introduction to Research Data",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data and research data management</span>"
    ]
  },
  {
    "objectID": "session-rdm.html#introduction-to-research-data",
    "href": "session-rdm.html#introduction-to-research-data",
    "title": "1  Data and research data management",
    "section": "",
    "text": "1.1.1 What is data? What is it for?\nData refers to factual information, often quantitative or qualitative, used as a basis for reasoning, discussion, or calculation. It is the foundational analysis element and supports decision-making across diverse fields. In research, data helps to generate new insights, verify hypotheses, and contribute to the overall body of knowledge in a specific area.\n\n\n1.1.2 Archaeological Data: Particularities\nIn archaeology, data encompasses various types, such as field notes, artefacts, images, geospatial coordinates, etc. By systematically collecting, organizing, and analysing this data, researchers can reconstruct past human behaviours, understand environmental contexts, and explore cultural practices.\nArchaeological data is unique due to its diverse formats and the complexity of its collection. It often includes material remains like pottery, bones, tools, and structures, as well as environmental data like pollen samples and soil types. Since archaeological data often comes from excavation sites, it is usually non-renewable; once excavated, a site cannot be restored to its original state.\nArchaeologists rely heavily on careful documentation to preserve as much information as possible for future study. Furthermore, the context in which artefacts are found is crucial, as it helps interpret their use, significance, and the broader cultural setting.\n\n\n1.1.3 Open Science\nOpen Science is a general framework that promotes transparency and collaboration in research by making methodologies, data, and findings accessible to the public and other researchers. In archaeology, it involves strong encouragements to share excavation reports, datasets, and analysis methods to facilitate broader understanding and scrutiny of findings and interpretations.\nThe Open Science agenda, widely agreed and recognised by national and international organisations, includes open research data and open source as main areas of practice and policy.\n\n\n\n\n1.1.4 FAIR Principles\nFAIR stands for Findable, Accessible, Interoperable, and Reusable (“What Is FAIR?” n.d.) and applies to data, metadata, and supporting infrastructure to ensure data is systematically organized and readily available for ongoing use and analysis.\n\n\n\n\n\n\nWhat is metadata?\n\n\n\n\n\nMetadata refers to data that provides information about other data rather than the content of the data itself. It describes various attributes such as the data’s structure, format, location, and context (“Metadata” 2024; Editor n.d.). Examples include creation dates, file sizes, authorship details, and keywords, which help organize, understand, and retrieve the associated data (“What Is Metadata and How Does It Work?” n.d.).\n\nExample:\n\n\n\nData: identification number, site of origin, and dates of items of a museum exhibition.\n\nMetadata: title and description of this data, author(s), date of last update, criteria for fixing site of origin, method(s) for establishing dates.\n\n\n\nNotice that, in this case, we could also choose to specify the information about the dating method in the dataset, as long as we want or can specify this information for each item independently.\n\n\n\n\nA short breakdown of the concepts behind FAIR:\n\nFindable: Data should be easy to locate for humans and machines. This includes having:\n\nA unique and persistent identifier.\nDescriptive metadata.\nMetadata containing data identifiers.\nRegistration in searchable resources.\n\nAccessible: Data should be easy to access once found. This includes:\n\nRetrieval via a standardized, open protocol.\nSupport for authentication and authorization when necessary.\nAvailability of metadata even if data itself is no longer available.\n\nInteroperable: Data should integrate with other data and tools. This requires:\n\nUsing a standardized, accessible language for data representation.\nUsing FAIR-aligned vocabularies.\nIncluding references to related data.\n\nReusable: Data should be well-described for reuse and replication in various contexts. This involves:\n\nDetailed, relevant metadata.\nClear data usage licensing.\nDocumentation of data provenance.\nConformance to community standards.\n\n\nThese principles aim to enhance the usefulness of digital assets by ensuring they can be easily located, understood, and utilized by others (Lamprecht et al. 2020; Wilkinson et al. 2016). Applying FAIR principles in archaeology involves creating well-documented, datasets that other researchers can readily access openly or through transparent authentication procedures, in case of sensitive data (Hiebel et al. 2021; Lien-Talks 2024; Nicholson et al. 2023).\n\n\n\nA summary breakdown of the FAIR data principles (reproduction from Lien-Talks (2024), Figure 1)\n\n\n\n\n1.1.5 CARE Principles\nThe CARE Principles for Indigenous Data Governance were defined more recently (Carroll et al. 2020) and emphasize ethical and responsible use of data:\n\nCollective Benefit: Data should benefit Indigenous communities collectively.\n\nAuthority to Control: Indigenous communities should have control over how their data is collected, used, and shared.\n\nResponsibility: Data practices must uphold the well-being and interests of Indigenous peoples.\n\nEthics: Data use must be guided by ethical considerations, respecting cultural protocols and privacy.\n\nThese principles aim to protect Indigenous rights to self-determination and ensure data contributes positively to community development and innovation. Researchers involved in archaeology should remain sensitive to the concerns raised by CARE, especially when conceptualising a new research project and later, when publishing data and research outputs.\nApplying the CARE principles in archaeological research will strongly depend on whether an indigenous group (self-identified and/or legally recognised) is directly involved as researchers or research enablers (e.g., creators of material culture, subjects to surveys and interviews, consulting experts, etc). In countries with a recent colonial past, this criteria tends to be clear for all parts involved. However, the concept of indigenous is hardly consensual when speaking about cultural groups not self-identified in oposition to a colonising cultural group. Initiatives, such as the Traditional Knowledge Labels by Local Contexts, continue to be developed to help to clarify this and other points.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“CARE Principles for Indigenous Data Governance” (2024)\n\n“CARE Principles for Indigenous Data Governance” (n.d.)\n\n“CARE Principles for Indigenous Data Governance  ARDC” (2022)\n\n\n\n\n\n\n1.1.6 Reproducibility\nReproducibility refers to the ability to replicate a study’s results using the original author’s assets or following their methodology. It ensures that findings can be independently verified under similar conditions, reinforcing the reliability of scientific research (National Academies of Sciences et al. 2019).\nIn scientific research, reproducibility is crucial for confirming the validity of experimental findings and building upon existing knowledge. It enhances transparency and trustworthiness in scientific practices, promoting better peer review and collaboration “GRN · German Reproducibility Network” (n.d.).\nMost archaeological research does not necessarily involve controlled experiments, and archaeological surveys and excavations are destructive and thus unrepeatable. However, the reproducibility of data collection, processing, and analysis is not a trivial concern.\nMaking research reproducible must be considered a spectrum of practices in which researchers should strive to improve despite the challenges (Marwick 2017).\n\nBen Marwick, CC-By Attribution 4.0 International, available at OSF\n\n\n1.1.7 Open Source\nOpen Source refers to software and tools that are freely available for anyone to use, modify, and distribute (“Open Source” 2024; “The Open Source Definition” n.d.).\nKey criteria include:\n\nAccess to Source Code: Users can access the software’s source code.\n\nFree Redistribution: The software can be freely redistributed.\n\nModifications and Derived Works: Users can modify the software and create derived works based on it.\n\nIntegrity of the Author’s Source Code: License terms ensure modifications do not restrict access to the original code.\n\nNo Discrimination Against Persons or Groups: The license must not discriminate against any person or group.\n\nNo Discrimination Against Fields of Endeavor: The license must apply to all fields of use.\n\nDistribution of License: The rights attached to the software must apply to all who receive it without needing to acquire additional licenses.\n\nFor archaeologists, open-source tools can offer affordable solutions for data analysis, visualization, and management, promoting a more inclusive research environment. Additionally, there is a growing community of archaeological software engineers who identify entirely with open-source principles (Batist and Roe 2024; Open Source Archaeology: Ethics and Practice 2015).\n\n\n1.1.8 The lay of the land: Organizations, tools, data repositories and where to find them\nSeveral organizations and platforms support Open Science and the use of FAIR and Open Source principles, providing archaeologists with access to valuable tools and datasets.\nHere is a list of those that will be most useful during this course:\n\nOrganizations:\n\nWikimedia Foundation: A non-profit organization focused on promoting the growth, development, and dissemination of free, multilingual content. It supports projects like Wikipedia, which provide free access to knowledge globally.\n\nOpenAIRE: OpenAIRE AMKE is a non-profit organization with a mission to promote open scholarship and improve discoverability, accessibility, shareability, reusability, reproducibility, and monitoring of data-driven research results, globally (Iatropoulou n.d.).\nGo FAIR Initiative: Provides guidelines on implementing FAIR principles, with resources tailored for researchers in various fields, including archaeology (“FAIR Principles” n.d.).\nARIADNE: A research infrastructure that integrates existing archaeological datasets across Europe (“Ariadne Research Infrastructure” n.d.).\n\nCIDOC-CRM: a theoretical and practical tool for information integration in the field of cultural heritage (“Home  CIDOC CRM” n.d.).\n\nThe Turing Way: collaborative writing of an online handbook (Community 2022).\nCAA: Computer Applications and Quantitative Methods in Archaeology (CAA) is an international organisation bringing together archaeologists, mathematicians, and computer scientists. CAA counts with National Chapters, including one for Germany.\nCAA Special Interest Group on Scientific Scripting Languages in Archaeology: group within the CAA that focuses on the application of Scripting Languages in archaeological research (“Home – CAA/SSLA” n.d.).\nCoalition for Archaeological Synthesis: an alliance of Partner organizations and individual Associates who are committed to fostering synthesis in archaeology to expand knowledge and benefit society (“Coalition for Archaeological Synthesis” 2024).\n\nNFDI4Objects - Research Data Infrastructure for the Material Remains of Human History: NFDI4Objects is aimed at researchers, practitioners, and students whose interests focus on the material heritage of around three million years of human and environmental history and address the challenges of modern research data infrastructures (NFDI4Objects 2024).\n\nData Repositories and Data Management Tools:\n\nORCID: ORCID, which stands for Open Researcher and Contributor ID, is a unique alphanumeric code that identifies researchers and contributors in scholarly activities.\n\nZenodo: A repository where researchers can deposit datasets, software, and publications.\n\nOpen Science Foundation: OSF is a free, open platform to support your research and enable collaboration.\nArchaeology Data Service (ADS): An archive for archaeological data from the UK, which provides access to various datasets, including excavation reports and geospatial data.\nthe Digital Archaeological Record (tDAR): An archive for archaeological data hosted in the USA, which provides access to various datasets, including excavation reports and geospatial data.\nOpen Context: A repository offering access to archaeological data from various global sources, adhering to FAIR principles [noauthor_open_nodate-1].\nHypothes.is: an open-source platform designed to facilitate collaborative annotation of web content. It is used as a browser extension or plug-in.\n\nOpen Source Tools:\n\nMarkdown: Markdown is a lightweight markup language designed for creating formatted text using a plain-text editor (“Markdown Guide” n.d.).\n\nR and Python: Programming languages with extensive libraries for data analysis, which are widely used in archaeology for statistical analysis and modeling “Welcome to Python.org” (2024). See more about these and other programming languages below.\n\nRStudio: RStudio IDE is an integrated development environment for R, a programming language for statistical computing and graphics.\nQuarto: An open-source scientific and technical publishing system.\nGit: Git is a distributed version control system designed to track changes in source code during software development.\nGitHub: GitHub is a developer platform that allows developers to create, store, manage, and share their code using Git software and a series of additional automated services.\nGitLab: GitLab is a developer platform that allows developers to create, store, manage, and share their code using Git software and a series of additional automated services.\nZotero: Zotero is a free, easy-to-use tool to help collect, organize, annotate, cite, and share metadata on references.\nZotero Style Repository: An open database for citations styles, which can be used in various reference managers and other tools for the formatting of references.\nQGIS: A free, open-source GIS tool useful for mapping and spatial analysis in archaeology (“Spatial Without Compromise · QGIS Web Site” n.d.).\n\nArches: Arches is an open-source software platform freely available for cultural heritage organizations to deploy independently to help them manage their cultural heritage data (“Arches Project Open Source Data Management Platform” 2015).\nOpenAtlas: an open-source database software developed primarily to acquire, edit, and manage research data from various fields of humanities like history, archaeology, and cultural heritage as well as related scientific data (“OpenAtlas” n.d.).\n\nopen-archaeo: An extensive list of open source archaeological software and resources (“Open-Archaeo” n.d.).\n\n\nBy utilizing these resources, archaeologists can ensure that their research aligns with Open Science, FAIR, and Open Source principles, ultimately enhancing the transparency, accessibility, and longevity of their work.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data and research data management</span>"
    ]
  },
  {
    "objectID": "session-rdm.html#introduction-to-programming-for-research",
    "href": "session-rdm.html#introduction-to-programming-for-research",
    "title": "1  Data and research data management",
    "section": "1.2 Introduction to Programming for Research",
    "text": "1.2 Introduction to Programming for Research\n\n1.2.1 What is Programming?\nProgramming is designing and implementing instructions a computer can follow to perform specific tasks. It involves writing code in various programming languages that communicate with the computer’s hardware and software to solve problems, process data, and automate repetitive tasks. At its core, programming is about creating a sequence of steps, known as algorithms, which help achieve a desired outcome (“What Is Programming? And How To Get Started” 2024).\n\n\n1.2.2 Importance of Learning Programming\nFor researchers, learning programming offers several significant advantages:\n\nEfficiency and Automation: Programming can help automate data collection, processing, and analysis, saving time and reducing human error. It also enables researchers to handle large datasets and complex calculations with ease.\nReproducibility: Writing scripts to perform analysis allows other researchers to replicate experiments, thus ensuring results can be verified and reproduced, a fundamental aspect of scientific research.\nAccess to Powerful Tools: With programming skills, researchers can access a wide range of tools for data visualization, statistical analysis, machine learning, and simulation. These tools can enhance the scope and quality of research projects.\n\n\n\n1.2.3 Overview of Common Programming Languages\nSeveral programming languages are popular in research due to their specific features and libraries(“Introduction to Programming Languages” 2018):\n\nPython: Known for its readability and versatility, Python is widely used for data analysis, machine learning, and automation. It has extensive libraries such as NumPy, pandas, and matplotlib, particularly useful for data-intensive research.\nR: A language developed explicitly for statistical computing and graphics, R is preferred in data science, bioinformatics, and fields requiring extensive statistical analysis. The Comprehensive R Archive Network (CRAN) offers thousands of packages that can handle various analytical tasks.\nMATLAB: Commonly used in engineering and scientific research, MATLAB excels at numerical computing, simulation, and algorithm development. It is prevalent in fields like physics, engineering, and finance. In contrast to R and Python, MATLAB is not open source. A MATLAB license can be obtained through the campus license of the RUB.\nJavaScript: A web development language also used in research to develop interactive data visualizations and web-based applications.\n\n\n\n1.2.4 Concept of Research Software: Tools and Scripts\nResearch software comprises tools, libraries, and scripts that aid in conducting and managing research activities. It can range from simple data cleaning and preprocessing scripts to complex software packages for statistical analysis and simulation. Although any software used in research could be considered research software, advanced training focuses on programming or scripting skills, not graphical user interface (GUI) operations (e.g., creating a plot in R or Python rather than in Microsoft Excel).\n\nTools: Research software tools like Jupyter Notebooks, RStudio, PyCharm, Visual Studio Code, etc., offer integrated development environments (IDEs) tailored to a specific range of languages, enhancing productivity and facilitating code sharing and version control.\nScripts: Scripts are text files encoding programs written to perform specific tasks. In research, scripts are often used for data cleaning, model training, and result visualization. Scripts can be used even to generate and format an entire dataset (have a peek on how the course schedule has been built with R code). Researchers can share these scripts to ensure that analyses are reproducible and consistent.\n\nLearning programming for research allows scientists to leverage these tools and scripts effectively, making research more efficient, reproducible, and insightful.\n\n\n1.2.5 Markdown: a language in between\nAs already mentioned, Markdown is a markup language with minimal syntax, designed for creating formatted text using a plain-text editor (“Markdown Guide” n.d.). Markdown is easily readable and editable for us humans, just like text. However, it is technically a programming language, as it holds machine-readable instructions (i.e., program), which, once compiled (or rendered) by the right software, produces the desired output in the form of a formatted text. An advanced text editor, such as Microsoft Word, allows you to do exactly the same (and more), but only through a sequence of interactions with the graphic user interface (GUI).\nWriting in Markdown or “writing Markdown code” is straightforward and will only require some getting used to, mainly if you are accustomed to advanced text editors. For example, instead of marking a text fragment in bold font through the usual steps, we would do it by typing ** (two asterisks) before and after the fragment.\nTo illustrate this, consider the rendering and source code of a markdown text:\n“Live as if you were to die tomorrow. Learn as if you were to live forever.” - Mahatma Gandhi, Source: BrainyQuote\n\"**Live** *as if* you were to **_die tomorrow_**. **Learn** as if you were to **_live forever_**.\"\n\nMahatma Gandhi, Source: [BrainyQuote](https://www.brainyquote.com/citation/quotes/mahatma_gandhi_133995)\nThe simplicity of this language might seem unnecessary or even annoying, but it is actually a trait that opens a wide range of potential applications. Today, markdown is indispensable for all practices related to open science and can offer a good entry point for other, more complex programming languages.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nObregon (2024)\n\n“What Is Markdown? Syntax, Examples, Usage, Best Practices” (2021)\n\n“Functional Documentation with Markdown and Version Control” (2017)\n“Basic Syntax  Markdown Guide” (n.d.)\n\n\n\n\n\n\n\n1.2.6 Where to learn (and keep learning)\n\nDatacamp (“Learn R, Python & Data Science Online” n.d.)\nUdemy (“Online Courses - Learn Anything, On Your Schedule  Udemy” n.d.)\nCoursera (“Coursera  Degrees, Certificates, & Free Online Courses” n.d.)\nCourses and workshops by higher education institutions (e.g,, RUB Research School)\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nTask 1: register yourself as a user at:\n\nORCID\n\nGitHub\n\nZenodo\n\nZotero (optional)\n\nHypothes.is (optional)\n\nTask 2: installations:\n\nGit. Follow general instructions here (de) or a video step-by-step tutorial like this or this (de), among many available online.\nGitHub Desktop\n\nR\nRStudio\nQGIS\n\nTask 3: explore datasets at Open Context\nTask 4: try writing a short document in Markdown at Markdown Live Preview.\n\n\n\n\n\n\n\n“Arches Project Open Source Data Management Platform.” 2015. https://www.archesproject.org/.\n\n\n“Ariadne Research Infrastructure.” n.d. Ariadne Research Infrastructure. Accessed October 14, 2024. https://www.ariadne-research-infrastructure.eu/.\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\n“Basic Syntax  Markdown Guide.” n.d. Accessed October 11, 2024. https://www.markdownguide.org/basic-syntax/.\n\n\nBatist, Zachary, and Joe Roe. 2024. “Open Archaeology, Open Source? Collaborative Practices in an Emerging Community of Archaeological Software Engineers.” Internet Archaeology, no. 67 (July). https://doi.org/10.11141/ia.67.13.\n\n\n“CARE Principles for Indigenous Data Governance.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=CARE_Principles_for_Indigenous_Data_Governance&oldid=1246386873.\n\n\n———. n.d. https://static1.squarespace.com/static/5d3799de845604000199cd24/t/5da9f4479ecab221ce848fb2/1571419335217/CARE+Principles_One+Pagers+FINAL_Oct_17_2019.pdf.\n\n\n“CARE Principles for Indigenous Data Governance  ARDC.” 2022. Https://Ardc.edu.au/. https://ardc.edu.au/resource/the-care-principles/.\n\n\nCarroll, Stephanie Russo, Ibrahim Garba, Oscar L. Figueroa-Rodríguez, Jarita Holbrook, Raymond Lovett, Simeon Materechera, Mark Parsons, et al. 2020. “The CARE Principles for Indigenous Data Governance” 19 (1): 43. https://doi.org/10.5334/dsj-2020-043.\n\n\n“Coalition for Archaeological Synthesis.” 2024. CfAS. https://www.archsynth.org/.\n\n\nCommunity, The Turing Way. 2022. “The Turing Way: A Handbook for Reproducible, Ethical and Collaborative Research.” Zenodo. https://doi.org/10.5281/ZENODO.3233853.\n\n\n“Coursera  Degrees, Certificates, & Free Online Courses.” n.d. Accessed October 8, 2024. https://www.coursera.org/.\n\n\nEditor, CSRC Content. n.d. “Metadata - Glossary  CSRC.” Accessed October 14, 2024. https://csrc.nist.gov/glossary/term/metadata.\n\n\n“FAIR Principles.” n.d. GO FAIR. Accessed October 7, 2024. https://www.go-fair.org/fair-principles/.\n\n\n“Functional Documentation with Markdown and Version Control.” 2017. Leon Hassan. https://blog.leonhassan.co.uk/functional-documentation-with-markdown-and-version-control/.\n\n\n“GRN · German Reproducibility Network.” n.d. Accessed October 7, 2024. https://reproducibilitynetwork.de/.\n\n\nHiebel, Gerald, Gert Goldenberg, Caroline Grutsch, Klaus Hanke, and Markus Staudt. 2021. “FAIR Data for Prehistoric Mining Archaeology.” International Journal on Digital Libraries 22 (3): 267–77. https://doi.org/10.1007/s00799-020-00282-8.\n\n\n“Home – CAA/SSLA.” n.d. Accessed October 14, 2024. https://sslarch.github.io/.\n\n\n“Home  CIDOC CRM.” n.d. Accessed October 14, 2024. https://www.cidoc-crm.org/.\n\n\nIatropoulou, Katerina. n.d. “OpenAIRE.” OpenAIRE. Accessed October 7, 2024. https://www.openaire.eu/.\n\n\n“Introduction to Programming Languages.” 2018. GeeksforGeeks. https://www.geeksforgeeks.org/introduction-to-programming-languages/.\n\n\nLamprecht, Anna-Lena, Leyla Garcia, Mateusz Kuzak, Carlos Martinez, Ricardo Arcila, Eva Martin Del Pico, Victoria Dominguez Del Angel, et al. 2020. “Towards FAIR Principles For&nbsp;research&nbsp;software.” Data Science 3 (1): 37–59. https://doi.org/10.3233/DS-190026.\n\n\n“Learn R, Python & Data Science Online.” n.d. Accessed October 8, 2024. https://www.datacamp.com.\n\n\nLien-Talks, Alphaeus. 2024. “How FAIR Is Bioarchaeological Data: With a Particular Emphasis on Making Archaeological Science Data Reusable.” Journal of Computer Applications in Archaeology 7 (1). https://doi.org/10.5334/jcaa.154.\n\n\n“Markdown Guide.” n.d. Accessed October 7, 2024. https://www.markdownguide.org/.\n\n\nMarwick, Ben. 2017. “Open Science in Archaeology,” January. https://doi.org/10.17605/OSF.IO/3D6XX.\n\n\n“Metadata.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=Metadata&oldid=1250210606.\n\n\nNational Academies of Sciences, Engineering, Policy and Global Affairs, Engineering Committee on Science, Board on Research Data Information, Division on Engineering and Physical and Sciences, Committee on Applied and Theoretical Statistics, Board on Mathematical Sciences Analytics, et al. 2019. “Understanding Reproducibility and Replicability.” In Reproducibility and Replicability in Science. National Academies Press (US). https://www.ncbi.nlm.nih.gov/books/NBK547546/.\n\n\nNFDI4Objects. 2024. “NFDI4Objects.” NFDI4Objects. http://195.37.32.58:4000/.\n\n\nNicholson, Christopher, Sarah Kansa, Neha Gupta, and Rachel Fernandez. 2023. “Will It Ever Be FAIR?: Making Archaeological Data Findable, Accessible, Interoperable, and Reusable.” Advances in Archaeological Practice 11 (1): 63–75. https://doi.org/10.1017/aap.2022.40.\n\n\nObregon, Alexander. 2024. “What Is Markdown? Uses and Benefits Explained.” Medium. https://medium.com/@AlexanderObregon/what-is-markdown-uses-and-benefits-explained-947300e1f955.\n\n\n“Online Courses - Learn Anything, On Your Schedule  Udemy.” n.d. Accessed October 8, 2024. https://www.udemy.com/.\n\n\n“Open Source.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=Open_source&oldid=1248809486.\n\n\nOpen Source Archaeology: Ethics and Practice. 2015. De Gruyter Open Poland. https://doi.org/10.1515/9783110440171.\n\n\n“Open-Archaeo.” n.d. Title. Accessed October 14, 2024. https://open-archaeo.info/.\n\n\n“OpenAtlas.” n.d. Accessed October 14, 2024. https://openatlas.eu/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\n“Spatial Without Compromise · QGIS Web Site.” n.d. Accessed October 7, 2024. https://qgis.org/.\n\n\n“The Open Source Definition.” n.d. Open Source Initiative. Accessed October 14, 2024. https://opensource.org/osd.\n\n\n“Welcome to Python.org.” 2024. Python.org. https://www.python.org/.\n\n\n“What Is FAIR?” n.d. Accessed October 14, 2024. https://www.howtofair.dk/what-is-fair/.\n\n\n“What Is Markdown? Syntax, Examples, Usage, Best Practices.” 2021. https://www.knowledgehut.com/blog/web-development/what-is-markdown.\n\n\n“What Is Metadata and How Does It Work?” n.d. WhatIs. Accessed October 14, 2024. https://www.techtarget.com/whatis/definition/metadata.\n\n\n“What Is Programming? And How To Get Started.” 2024. Coursera. https://www.coursera.org/articles/what-is-programming.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data and research data management</span>"
    ]
  },
  {
    "objectID": "session-git.html",
    "href": "session-git.html",
    "title": "2  Git and GitHub",
    "section": "",
    "text": "2.1 Version-control and Git",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session-git.html#version-control-and-git",
    "href": "session-git.html#version-control-and-git",
    "title": "2  Git and GitHub",
    "section": "",
    "text": "2.1.1 Version control: general concept and its usefulness\nVersion control is a system that helps track and manage file changes over time. It’s widely used in software development, but its applications extend to any field where file management is essential, including document editing, research, and project management. At its core, version control provides a historical record of changes. It allows users to revert to previous versions, identify when and why changes were made, and work collaboratively without the risk of overwriting each other’s work.\nThere are two main types of version control: centralized and distributed. Centralized version control systems, such as Subversion (SVN), store files in a central repository. Users check out files, make changes, and then commit them back to the central repository. While effective, centralized systems can be vulnerable if the central server fails. Distributed version control systems, like Git, address this by allowing every user to have a complete copy of the repository on their local machine. This setup enhances collaboration and provides redundancy, as users can work offline and synchronize changes with others when connected.\n\n Git icon\n\n\n\n2.1.2 Benefits of Version Control\n\nCollaboration: Version control systems make collaboration easier and more efficient by allowing multiple users to simultaneously work on the same project. With distributed systems like Git, branches can be created for different features or tasks, and changes can later be seamlessly merged into the main project. This enables teams to work independently and minimize conflicts.\nHistorical Tracking: Version control systems keep a detailed history of all changes made to the files. This allows users to see who made changes, when, and why. If an issue arises, it’s possible to revert to a previous state without losing any data, making debugging easier.\nBackup and Redundancy: In distributed systems, each user’s local copy is a backup of the entire project. This redundancy reduces the risk of data loss due to server failures or other issues and allows users to work offline and sync changes later.\nVersion Management: Version control systems assign unique identifiers to each change, per commit and file changed, usually called “Git hash” or “commit hash”. A Git hash a 40-character hexadecimal string, such as 2d3acf90f35989df8f262dc50beadc4ee3ae1560, derived from the contents of the commit, including its parent commit(s), timestamp, and author details [REF]. These identifiers allow users to switch between different versions of the project easily. It’s also possible to create branches for experimental features and merge them with the main project once they’re stable, facilitating smoother integration of new features.\nEnhanced Workflow: Many version control systems support automated processes such as Continuous Integration (CI) and Continuous Deployment (CD), which streamline development and testing. These systems can automatically test changes before they are merged, ensuring higher code quality and reducing the risk of introducing bugs.\n\nOverall, version control systems are crucial tools in modern project management and development workflows. They enable collaboration, ensure data integrity, and improve productivity by providing a structured approach to managing changes in any type of project.\nGet a short introduction to Git by watching the official Git Documentation videos here.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session-git.html#git-terminology",
    "href": "session-git.html#git-terminology",
    "title": "2  Git and GitHub",
    "section": "2.2 Git terminology",
    "text": "2.2 Git terminology\nHere are some essential Git terms to know:\n\n\n\n\n\n\n\nRepository: A storage space for your project files and their history. Repositories can be local (on your computer) or remote (on platforms like GitHub).\nInitialise: configure a specific local directory (your “working directory”) as a local repository by creating all necessary files for Git to work.\nAdd/Stage: adds a change in the working directory to the staging area, telling Git to include updates to a particular file in the next commit. However, adding or staging doesn’t really affect the repository since changes are not actually recorded until they are committed (see below).\n\n\n\n\n\n\n\n\nCommit: A snapshot of changes in the repository. Each commit has a unique ID, allowing you to track and revert changes as needed [1].\nBranch: A separate line of development. The default branch is usually called main or master. Branches allow you to work on features independently before merging them into the main project [2].\nMerge: The process of integrating changes from one branch into another. Typically, this involves merging a feature branch into the main branch.\nPull: A command that fetches changes from a remote repository and merges them into your local branch, ensuring your local work is up-to-date with the remote [2].\nPush: Uploads your commits from the local repository to the remote repository, making your changes available to others.\n\nUnderstanding these terms is crucial for effective Git usage and collaboration in any project.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Git - Gitglossary Documentation” (n.d.)\n\n“Git Definitions and Terminology Cheat Sheet” (n.d.)\n\n\n\n\n\n\n\n\n\n\nCHECK: Git software installation\n\n\n\n\n\nTo verify if Git is installed on your machine, follow these steps:\n\nOpen Command Prompt (Windows 10 or 11)\n\nPress Win + R, type cmd, and hit Enter.\nAlternatively, you can search for “Command Prompt” in the Start menu and select it.\n\nCheck for Git\n\nIn the Command Prompt window, type the following command and press Enter:\ngit --version\nIf Git is installed, you will see the installed version, e.g., git version 2.34.1.\nIf Git is not installed, you will receive an error message or see that the command is unrecognized. You can download the installer from git-scm.com and follow the installation instructions.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session-git.html#github",
    "href": "session-git.html#github",
    "title": "2  Git and GitHub",
    "section": "2.3 GitHub",
    "text": "2.3 GitHub\n\n2.3.1 What is GitHub?\nGitHub is a cloud-based platform that enables developers to store, manage, and collaborate on code repositories. It builds on Git, a version control system, by adding collaborative features like pull requests, issue tracking, and discussions, which make it easier for teams to work together on software projects.\nGitHub also offers hosting for open-source projects, allowing anyone to contribute or review code. With integrations for CI/CD, project management tools, and documentation, GitHub is a popular choice for developers worldwide to manage both personal and professional projects.\n\n GitHub icon\n\n\n\n\n\n\n\nCHECK: GitHub user and GitHub Desktop installation\n\n\n\n\n\n\nCheck GitHub Desktop Installation\nTo verify that GitHub Desktop is installed:\n\nOn Windows: Go to the Start menu, search for “GitHub Desktop,” and open the app. If it launches successfully, GitHub Desktop is installed.\nOn macOS: Use Spotlight Search (Cmd + Space), type “GitHub Desktop,” and press Enter. If the app opens, it is installed.\n\nIf you don’t have GitHub Desktop, you can download it from desktop.github.com and follow the installation instructions [1][2].\n\n\nVerify GitHub User\nTo check if you are signed in as a GitHub user:\n\nOpen GitHub Desktop.\nGo to File &gt; Options (on Windows) or GitHub Desktop &gt; Preferences (on macOS).\nUnder the Accounts tab, you should see your GitHub username and avatar if you are signed in. If not, you can sign in with your GitHub credentials here.\n\n\n\nBookmark your GitHub user profile page\nIn your Internet browser, make sure that your own GitHub user profile page is saved in Bookmarks for easy access later.\n\n\n\n\n\n\n2.3.2 GitHub terminology\nUnderstanding the core vocabulary associated with GitHub operations can help users make the most of this platform, especially for collaborative or open-source projects. Here are some key concepts to keep in mind:\n\nForking: Forking creates a personal copy of someone else’s GitHub repository in your account. It allows users to experiment with changes without affecting the original repository, and is often used to contribute to open-source projects. After forking, developers can freely modify their own versions and submit a pull request to propose these changes to the original repository if they have improvements or fixes to offer.\nCloning: Cloning involves creating a local copy of a repository on your machine. By cloning a repository, users can work offline and change files that can later be pushed back to the GitHub repository. This process is essential for local development, allowing users to commit changes and manage their workflow effectively with Git commands.\nPull Requests: A pull request (PR) is a way to propose changes in a repository. After modifying a forked or branched version of a repository, a developer can open a pull request, which initiates a review process. This feature is central to collaboration on GitHub, allowing others to review, discuss, and approve proposed changes before they are merged into the main branch.\nBranching: A branch is a parallel version of the repository within the same repository structure. By branching, developers can isolate work on different features or fixes without altering the main project files. For example, many projects have a main or master branch for the official release version, while other branches are used for development or testing. Branches are typically merged into the main branch once they are finalized.\nCommits and Push: A commit is a snapshot of changes in the repository. Every commit includes a message describing the changes, and each commit builds upon previous ones, creating a history of the repository’s development. Pushing is uploading these commits to GitHub from a local repository. After a series of commits on a local branch, a user can push these changes to the corresponding branch on GitHub to keep the remote repository up-to-date.\nGists: A gist is a simple way to share code snippets or single files. Gists can be public or secret, and they are particularly useful for sharing configuration files or code examples. Users can fork and edit Gists, making them a lightweight collaboration and code-sharing tool.\nIssues and Discussions: Issues are GitHub’s built-in tracking system for bugs, tasks, and feature requests. They allow users to report problems, suggest new features, and engage in conversations related to the project. Discussions provide a more open forum-style setting for broader conversation, enabling users to share ideas, ask questions, and contribute knowledge that might not directly relate to specific code changes.\n\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Cloning and Forking a Repository — Pythia Foundations” (n.d.)\n\n“GitHub Glossary” (n.d.)\n\n“How to Get Familiar with Forking & Cloning GitHub Repos” (2023)\n\n“Difference Between Fork and Clone in GitHub” (2021)\n\n\n\n\n\n\n2.3.3 Working with GitHub\nGitHub offers various workflows to manage repositories. Here are three common methods:\n\n\n\n\n\n\nLocal with GitHub Desktop\n\n\n\n\n\nFor those who prefer a graphical user interface (GUI):\nCloning a Repository\n\nOpen GitHub Desktop.\n\nGo to File &gt; Clone Repository.\n\nSelect the repository and click “Clone.”\n\nCreating a New Branch\n\nClick on the “Current Branch” dropdown.\n\nSelect “New Branch,” name it, and click “Create Branch.”\n\nMaking Changes\n\nEdit files in your editor.\n\nCommitting Changes\n\nReturn to GitHub Desktop.\n\nStage changed files by ticking the boxes.\n\nWrite a summary of changes and click “Commit to new-branch.”\n\nPushing Changes\n\nClick “Push origin” to upload your changes.\n\n\n\n\n\n\n\n\n\n\nRemote with Web Browser\n\n\n\n\n\nYou can also work directly on GitHub.com:\nForking a Repository\n\nGo to the repository page.\nClick on the Fork button in the top-right corner of the page.\n\nChoose an owner (user or organisation), a name and description for the new fork repository. The default will always be a exact copy of the original repository. Select whether to copy only the main branch. Click “Create fork”.\n\nCloning a Repository\n\nGo to the repository page.\n\nClick the green “Code” button and continue the cloning process locally, using console commands (copying link) or with GitHub Desktop.\n\nCreating a New Branch\n\nClick the branch dropdown on the main page.\n\nType a new branch name and click “Create branch.”\n\nMaking Changes\n\nNavigate to the file (and branch) you want to edit.\n\nClick the pencil icon to edit.\n\nMake your changes and scroll down to the “Commit changes” section.\n\nCommitting Changes\n\nEnter a commit message.\n\nChoose whether to “commit directly to main” or “Commit to a new branch…”.\n\nPushing Changes\n(No push is needed as changes are automatically saved to GitHub.)\n\n\n\n\n\n\n\n\n\nLocal with Console Commands (advanced users)\n\n\n\n\n\nTo work with Git via the command line:\nNavigate to the directory to hold the local copy\ncd path/to/local/directory\nCloning a Repository\ngit clone https://github.com/username/repository.git\nCreating a New Branch\ngit checkout -b new-branch\nMaking Changes Edit files in your favorite text editor or IDE.\nCommitting Changes\ngit add .\ngit commit -m \"Describe your changes\"\nPushing Changes\ngit push origin new-branch\n\n\n\nThese workflows enable flexibility in how you manage your projects on GitHub.\n\n\n2.3.4 Markdown (GitHub-flavoured)\nWhen Markdown files (.md) are placed in a GitHub repository, they will be automatically rendered within GitHub web interface by default, while the raw code can still be seen and edited in Markdown.\nThere are some particularities about how Markdown files will be rendered in GitHub through Internet browsers. Consult GitHub Docs for knowing more about them.\n\n\n2.3.5 How to organise repositories\nWhen structuring your repositories, following some common conventions for organizing files in subdirectories is helpful. This makes projects more readable and more accessible for others to navigate. Here are some commonly used subdirectories:\nWhen software development is a significant part:\n* source/ or src/: Contains the main source code for the project.\n* documentation/, docs/, or doc/: Stores documentation, such as guides or API references.\n* tests/: Includes test scripts to ensure code functionality.\n* bin/: Holds executable scripts or binaries.\n* config/: Contains configuration files, like YAML or JSON.\nWhen rendering a document or a graphical user interface, such as LaTeX documents, websites, web apps, or video games:\n* assets: to hold files and subdirectories with closed content and functionality files.\n* assets/images/, assets/media/, etc.: Holds all images or other media files generated externally (not by the repository’s source code).\n* assets/styles.css or assets/css/: all CSS code for formatting HTML objects.\n* assets/js/: JavaScript source code enabling interactive functionalities (it would also apply for other programming languages in similar position). Source code of this kind might also be placed inside the source code folder, if present.\nThese folder structures are conventions and not strict rules. You can adapt or modify them based on your project’s needs.\nThere are several community-based proposals for standards, including tools that can help automate the creation of a new project directory with conventional files. For example, for a typical Data Science project using Python see Cookiecutter Data Science.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\njimmy (2022)\n\nZestyclose-Low-6403 (2023)\n\ndanijar (2019)\nSuhail (2024)\n\nCioara (2018)\n\n\n\n\n\n\n2.3.6 Conventional files\n\nREADME.md: Provides an overview of the project, including what it does, how to set it up, and how to contribute. A few sections examples are:\n\nGeneral description\n\nAuthors and/or contributors\n\nAcknowledgements\n\nFunding\n\nInstallation or use instructions\n\nContributing\n\n\nLICENSE: Specifies the terms under which the content of the repository can be used, modified, and distributed. There are many licenses, varying in permissiveness and type of content. Generally, for projects involving both code and other kinds of content, we recommend CC0-1.0 or MIT. See https://choosealicense.com/ and GitHub Docs).\nCITATION.cff: human- and machine-readable citation information for software (and datasets). See example here.\n\n.gitignore: Lists files and directories that Git should ignore, such as build outputs and temporary files.\n\nCHANGELOG.md: This file logs a chronological record of all notable changes made to the project, often following conventions like Conventional Commits.\n\nreferences.bib: a file containing references in BibTex format, which can be cited within the markdown files of the repository.\n\n\n\n2.3.7 Version Tags and Releases on GitHub\nTo manage different versions of your project, GitHub allows you to create tags and releases:\n\nCreate a Tag:\n\nOpen your repository on GitHub and navigate to the Releases section.\nClick Draft a new release.\nIn the Tag version field, type a version number (e.g., v1.0.0) to create a new tag (see more in the note below).\nSpecify the target branch or commit for this tag.\n\nCreate a Release:\n\nAfter tagging, enter details such as the release title and description.\nOptionally, add release notes to summarize changes or new features [1].\nClick Publish release to make it public.\n\n\nReleases are tied to tags and provide a stable reference for each version, making it easy for users to download specific versions of your project [2].\n\n\n\n\n\n\nAbout versioning\n\n\n\n\n\nIf unfamiliar with the logic behind versioning, consult the reference to Semantic Versioning, which can also be found on the right of the “Create a new release” page in GitHub. Their summary states:\n\nGiven a version number MAJOR.MINOR.PATCH, increment the:\n1. MAJOR version when you make incompatible API changes\n2. MINOR version when you add functionality in a backward compatible manner\n3. PATCH version when you make backward compatible bug fixes\n\nHowever, if your repository is not about creating software products and services, we can do well by simply obeying a few general conventions:\n\nAdd a PATCH version discretionally when correcting bugs, typos, tuning aesthetics, etc, or refactoring code (explained in Chapter 4).\nAdd a MINOR version when expanding code functionality or adding new content (text sections, images)\nAdd a new PATCH or MINOR version every time the repository reaches a natural stable point (i.e., there are no changes planned any time soon).\nMake sure that every new MAJOR version is released (GitHub) and published (Zenodo, see below).\n\n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Creating GitHub Releases Automatically on Tags” (2024)\n“Automatically Generated Release Notes” (n.d.)\n\nSignell (2013)\n\n\n\n\n\n\n\n2.3.8 Establishing a GitHub-Zenodo Connection\nTo link your GitHub repository with Zenodo and enable citation via DOI:\n\nLogin to Zenodo: Go to Zenodo and sign in or create an account.\nAuthorize GitHub Access:\n\nClick on your profile in Zenodo and select Linked accounts.\nChoose Connect next to GitHub.\nYou will be redirected to GitHub to authorize Zenodo’s access. Approve the request to complete the connection.\n\nSelect Repository for DOI Generation:\n\nIn Zenodo, navigate to GitHub in the Linked Accounts section.\nEnable DOI generation for the desired repository. Zenodo will automatically mint DOIs for any new release you publish.\n\n\nThis connection allows you to generate and manage DOIs for GitHub repositories, enhancing your project’s citation and research accessibility.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Referencing and Citing Content” (n.d.)\n\n“Zenodo - Research. Shared.” (n.d.)\n\n“Created New Organization in GitHub and Zenodo Did Not Send a Request for Accessing It · Issue #1596 · Zenodo/Zenodo” (n.d.)\n\n“Module-5-Open-Research-Software-and-Open-Source/Content_development/Task_2.md at Master · OpenScienceMOOC/Module-5-Open-Research-Software-and-Open-Source” (n.d.)\n\n“Issue a Doi with Zenodo” (n.d.)\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nTask 1: Create a profile repository with a README file following GitHub Docs: Quickstart for writing on GitHub.\nTask 2: Making others’ repositories your own’s\n\nFork the Course Book repository.\n\nClone your fork to a local directory.\n\nModify or add a .qmd file in your local directory.\nCommit to your local repository (one or more times).\n\nPush the changed local repository to your remote repository (i.e. your fork).\n\nCreate a pull request back to the original repository.\n\nTask 3: Create a personal project repository\n\nCreate a GitHub repository under your user. Named it rub-archwiss_ followed by your surname (no special characters). Initialise with the following properties:\n\n\nPublic\nWith a README file\nwith .gitignore (template for R)\nWith a license of your choice.\n\n\nAdd all other conventional files mentioned above, even if they remain empty for now.\nEdit your README file and commit/push your changes.\n\nTask 4:\n\nSet up the GitHub-Zenodo connection.\n\nPublish your repository.\n\nUpdate README with the new Zenodo DOI (badge).\n\n\n\n\n\n\n\n\n“Automatically Generated Release Notes.” n.d. GitHub Docs. Accessed October 11, 2024. https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes.\n\n\nCioara, Andrei. 2018. “How I Organize My GitHub Repositories.” Medium. https://andreicioara.com/how-i-organize-my-github-repositories-ce877db2e8b6.\n\n\n“Cloning and Forking a Repository — Pythia Foundations.” n.d. Accessed October 28, 2024. https://foundations.projectpythia.org/foundations/github/github-cloning-forking.html.\n\n\n“Created New Organization in GitHub and Zenodo Did Not Send a Request for Accessing It · Issue #1596 · Zenodo/Zenodo.” n.d. GitHub. Accessed October 11, 2024. https://github.com/zenodo/zenodo/issues/1596.\n\n\n“Creating GitHub Releases Automatically on Tags.” 2024. https://jacobtomlinson.dev/posts/2024/creating-github-releases-automatically-on-tags/.\n\n\ndanijar. 2019. “Can I Arrange Repositories into Folders on Github?” Forum post. Stack Overflow. https://stackoverflow.com/q/11852982/6199967.\n\n\n“Difference Between Fork and Clone in GitHub.” 2021. GeeksforGeeks. https://www.geeksforgeeks.org/difference-between-fork-and-clone-in-github/.\n\n\n“Git - Gitglossary Documentation.” n.d. Accessed October 11, 2024. https://git-scm.com/docs/gitglossary.\n\n\n“Git Definitions and Terminology Cheat Sheet.” n.d. Accessed October 11, 2024. https://www.pluralsight.com/resources/blog/cloud/git-terms-explained.\n\n\n“GitHub Glossary.” n.d. GitHub Docs. Accessed October 28, 2024. https://docs.github.com/en/get-started/learning-about-github/github-glossary.\n\n\n“How to Get Familiar with Forking & Cloning GitHub Repos.” 2023. DEV Community. https://dev.to/joshhortt/how-to-get-familiar-with-forking-cloning-github-repos-46nc.\n\n\n“Issue a Doi with Zenodo.” n.d. Github for Collaborative Documentation. Accessed October 11, 2024. https://cassgvp.github.io/github-for-collaborative-documentation/docs/tut/6-Zenodo-integration.html.\n\n\njimmy. 2022. “How to Organize GitHub Repositories.” Backrightup. https://backrightup.com/blog/how-to-organize-github-repositories/.\n\n\n“Module-5-Open-Research-Software-and-Open-Source/Content_development/Task_2.md at Master · OpenScienceMOOC/Module-5-Open-Research-Software-and-Open-Source.” n.d. GitHub. Accessed October 11, 2024. https://github.com/OpenScienceMOOC/Module-5-Open-Research-Software-and-Open-Source/blob/master/content_development/Task_2.md.\n\n\n“Referencing and Citing Content.” n.d. GitHub Docs. Accessed October 11, 2024. https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content.\n\n\nSignell, Rich. 2013. “How to Handle Releases of Markdown Document on Github.” Forum post. Stack Overflow. https://stackoverflow.com/q/19727632/6199967.\n\n\nSuhail, Muhammad Ahmed. 2024. “Structuring and Organizing My Github: A Developer’s Guide.” Medium. https://medium.com/@muhammadahmedsuhail007/structuring-and-organizing-my-github-a-developers-guide-7353610f04fd.\n\n\n“Zenodo - Research. Shared.” n.d. Accessed October 11, 2024. https://help.zenodo.org/docs/profile/linking-accounts/.\n\n\nZestyclose-Low-6403. 2023. “How to Organize Repos Within an ’Organization’?” Reddit {Post}. R/Github. www.reddit.com/r/github/comments/188d324/how_to_organize_repos_within_an_organization/.",
    "crumbs": [
      "Getting started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "session-r-introduction.html",
    "href": "session-r-introduction.html",
    "title": "3  Introduction to R",
    "section": "",
    "text": "3.1 Preparation\nBefore learning more about R, make sure that everything is set up properly and that you understand the basics in RStudio GUI.",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "session-r-introduction.html#preparation",
    "href": "session-r-introduction.html#preparation",
    "title": "3  Introduction to R",
    "section": "",
    "text": "CHECK: R and RStudio installation\n\n\n\n\n\nTo ensure that both R and RStudio have been installed correctly, follow these steps:\n\nLaunch RStudio by searching for it in your applications menu.\n\nWhen it opens, you should see an interface with multiple panels, including the Console in the left or bottom left panel.\n\nIn the RStudio Console, you should see details about the R version currently running, which confirms that both R and RStudio are correctly installed and linked. You can always re-print this information by typing the command version.\n\nIf any of these steps fail, consider reinstalling R and RStudio, ensuring they are compatible with your operating system.\n\n\n\n\n3.1.1 RStudio interface (GUI)\nThe RStudio interface, or Graphical User Interface (GUI), is designed to help you work efficiently with R. By default, it consists of four main panes or panels, each potentially containing multiple tabs:\n\nConsole/Terminal/Background Jobs: This is where you can type and run R commands directly. It displays output from your code and any error messages.\n\nEnvironment/History/Connections/Build/Git: In the top-right, the Environment tab shows all active variables and data loaded into your session, while the History tab keeps a log of previously executed commands.\n\nFiles/Plots/Packages/Help/…: The bottom-right panel has several tabs for navigating your files, viewing plots and other graphical outputs, managing installed packages, and accessing R documentation.\n\nSource or Script Editor: Located in the top-left, here is where you can write, edit, and save R scripts. You can run selected code from this editor directly in the Console. This panel will be absent whenever there are no script files open in RStudio.\n\n\n\n\nRStudio GUI default structure; from “RStudio User Guide - RStudio IDE User Guide” (2024)\n\n\nMany of the various elements of RStudio GUI are self-explanatory or further explained by pop-up texts and windows. Still, beginners and occasional users can be assured that most elements can be ignored.\n\n\n3.1.2 Global Settings\nTo customize RStudio, go to Tools &gt; Global Options (or RStudio &gt; Preferences on macOS). Here, you can adjust various settings, including:\n\nGeneral: Setting the directory path where R has been installed (normally assigned automatically), your default working directory and specify start-up options.\n\nCode: Configuring code formatting, autocompletion, and syntax highlighting settings.\nAppearance: Change the editor theme, font size, and other visual preferences.\n\nPane Layout: Changing the default pane structure (not recommended).\n\n\n\n3.1.3 RStudio Projects\nRStudio Projects help organize your work by keeping all related files, scripts, data, and outputs in one place. Each project has its own working directory, which helps to manage dependencies and to maintain reproducibility. Projects are especially useful for keeping different analyses or projects separate from one another.\nTo create a new project in RStudio, follow these steps:\n\nGo to File &gt; New Project.\nYou’ll see three options:\n\nNew Directory: Create a project from scratch within a new folder. This is useful when starting a completely new analysis or project.\n\nExisting Directory: Convert an existing folder into an RStudio project. Ideal for organizing already-existing files and scripts into a RStudio project.\n\nVersion Control: Clone a project from GitHub, GitLab, or other version control systems. This option is helpful when working with collaborative projects or version-tracked repositories.\n\n\nSelect the appropriate option based on your needs. For example, if you choose New Directory, you can then select New Project, enter a project name, and specify the location to save it. Alternatively, if you are working with a GitHub repository, you could select Version Control to clone it directly into RStudio, creating a fully synchronized project environment.\n\nClick Create Project. RStudio will open a new R session within the project’s directory.\n\n\n\n3.1.4 R Scripts and Rmarkdown notebooks\nIn RStudio, both R scripts and Rmarkdown notebooks (or rendered notebooks) are used to write and execute R code, but they serve different purposes and have distinct features:\n\nScripts (.R files)\n\nPlain Text Format: Scripts are simple text files where you can write and save R code (.R file extension). They are best suited for running sequential code and writing reusable functions.\n\nExecution: You can run code line-by-line or in chunks directly in the Console. Scripts are ideal for production workflows or larger projects where maintaining clear, reproducible code is a priority.\n\nComments: You can add comments for documentation, but scripts do not natively support rich formatting like Markdown.\n\nExample of a small R script, “print_mean_value.R”, followed by its output once executed:\n\n# Calculate the mean of a numeric vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\nmean_value &lt;- mean(numbers)\nprint(mean_value)\n\n[1] 3\n\n\nIn this script, we define a vector, calculate its mean, and print the result. The focus is on the code itself, without additional formatting or documentation.\n\n\nRmarkdown documents (.Rmd/.qmd files)\n\nRich Content: RStudio allows you to create and use special files, named RMarkdown (.Rmd file extension), that combine formatted text in Markdown (e.g. headings, images, and links) and executable pieces of code or code “chunks”. When added to a Rmarkdown, the code inside a chunk can be executed on-demand by clicking on the play button on its top right corner.\nInteractive Execution: Rmarkdown supports interactive, cell-based execution, similar to Jupyter notebooks. Each code cell outputs results directly below it by default, which is useful for exploratory data analysis and iterative workflows.\nOutput Options: Notebooks can be rendered into various formats, such as HTML, PDF, or Word, allowing you to create polished, shareable reports directly from your analysis. It is possible to have code being run and rendered with output directly into a HTML file by choosing html_notebook, a type of output that approaches the one of Jupyter notebooks.\n\nExample of a small RMarkdown document, “mean_value_analysis.Rmd”:\n\n\n\n## Calculate the Mean of a Vector\n\nIn this analysis, we calculate the mean of a simple numeric vector.\n\n```{r}\n# Define the numeric vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Calculate the mean\nmean_value &lt;- mean(numbers)\nmean_value\n```\n\nThe mean of the vector is `r mean_value`.\n\n\nIn this document, Markdown is used to add a heading and text explanation, while the code chunk calculates and displays the mean. The output is shown directly below the code, creating an interactive, document-like format. Notice we can also use ` r object_name ` to print inside a markdown line the value of an R object, in this case a single number.\nHow it would look in RStudio:\n\n\n\nRMarkdown example in RStudio\n\n\nAnd this is how it would be rendered in HTML:",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "session-r-introduction.html#calculate-the-mean-of-a-vector",
    "href": "session-r-introduction.html#calculate-the-mean-of-a-vector",
    "title": "3  Introduction to R",
    "section": "Calculate the Mean of a Vector",
    "text": "Calculate the Mean of a Vector\nIn this analysis, we calculate the mean of a simple numeric vector.\n\n# Define the numeric vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Calculate the mean\nmean_value &lt;- mean(numbers)\nmean_value\n\n[1] 3\n\n\nThe mean of the vector is 3.\nAlthough not necessary by default, R Markdown files can contain YAML headers, which define the metadata for the document. Located at the top of the file, the YAML header specifies key information and rendering options, such as title, author, date, output format (e.g., HTML, PDF), and document theme. This header controls how the document is processed and rendered, allowing customization without altering the main content.\nFor example:\n---\ntitle: \"Analysis Report\"\nauthor: \"Data Scientist\"\ndate: \"2024-12-20\"\noutput: html_document\n---\nThis YAML block will render the document as an HTML report with a dynamic date and specified author and title.\nYou can learn more about Rmarkdown at the Get Started tutorial offered by its developers.\nIn summary, scripts are optimal for code-centric work with minimal formatting, while Rmarkdown documents offer a flexible, document-like interface ideal for combining narrative and code in a single file.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nG. Grolemund J. J. Allaire (n.d.)\n“R Markdown” (n.d.)\n“Introduction” (n.d.)\n“R Markdown Reporting Best Practices” (n.d.)\nH. W. and G. Grolemund (n.d.)\nMelfi (n.d.)\n“Rmarkdown :: Cheatsheet” (n.d.)\nSmith (n.d.)\nRiederer (n.d.)\nLiang (2024)\n\n\n\n\n\n3.1.5 Creating or editing other files\nIn addition to R scripts (.R) and RMarkdown notebooks (.Rmd), RStudio supports creating and editing various other file types, making it a versatile environment for different types of content and workflows. To create a new file, go to File &gt; New File and select the desired file type.\nSome examples of files you can create and edit in RStudio include:\n\nPlain Text files (.txt): Useful for notes, raw data, or configuration files.\nHTML files (.html): For creating and editing web pages, especially useful for generating custom reports.\nPython scripts (.py): RStudio supports Python, allowing you to write and execute Python code within the same environment if Python is installed on your computer.\nSQL files (.sql): You can write and run SQL queries directly in RStudio when working with databases.\n\nTo create a file with an extension not listed in RStudio, simply create a plain text file (e.g., “newFile.txt”), modify its name and add the desired extension (e.g., “newFile.json”, “newFile.css”, etc.). This flexibility allows you to manage all parts of your project, from data processing to documentation, within RStudio.\n\n\n\n\n\n\nSee also\n\n\n\n\n\nFor more information about RStudio, consult:\n\n“RStudio User Guide - RStudio IDE User Guide” (2024)\n“RStudio IDE :: Cheatsheet” (n.d.)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "session-r-introduction.html#r-syntax-and-workflow",
    "href": "session-r-introduction.html#r-syntax-and-workflow",
    "title": "3  Introduction to R",
    "section": "3.2 R syntax and workflow",
    "text": "3.2 R syntax and workflow\n\n3.2.1 Variables and data types\nIn R, variables or vectors are created by assigning values using the &lt;- operator. A variable in R corresponds to values of a specific data type, including:\n\nNumeric: Numbers, e.g., x &lt;- 10.5\nInteger: Whole numbers, declared with L, e.g., y &lt;- 3L\nCharacter: Text, surrounded by quotes, e.g., name &lt;- \"Alice\"\nLogical: Boolean values, TRUE or FALSE, e.g., is_true &lt;- TRUE\nFactor: Categorical data, useful for storing distinct categories, and keeping both category names and indexed numeric value (see Data structures bellow).\n\nVariables store data for manipulation and analysis, forming the building blocks of R programming. Variables and data structures (see bellow) can be all referred as “R objects”, and once created, they will appear listed in the Environment tab in the Environments panel (top right).\n\n\n3.2.2 Data structures\nData structures are more complex objects that are also created using the &lt;- operator.\n\nVectors (i.e. variables): The most basic data structure, a vector is a sequence of data elements of the same type (numeric, character, or logical). Created with c(), e.g., c(1, 2, 3) or c(\"a\", \"b\", \"c\").\nMatrices: Two-dimensional, homogeneous data structures (all elements are of the same type). Created using matrix(), e.g., matrix(1:9, nrow = 3, ncol = 3).\nFactor: A factor is a special case in-between a vector and a matrix, designed to facilitate operations with categorical variables. While a factor will often seem equivalent to a Character vector (e.g., c(\"Yes\", \"No\", \"Yes\")), it will be treated as a two column matrix where character values are mapped to a numeric index, assigned arbitrarily. In this nx2 matrix, where n is the number of rows or elements in each column, each row is referred as “level”. To create a factor, we must use the primitive function factor() and give it a Character vector: e.g., factor(c(\"Yes\", \"No\", \"Yes\")).\nData Frames: Tabular data structures where each column can contain different data types (numeric, character, factor, etc.). Both columns and rows can be named. Data frames are created with data.frame(): e.g., data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")).\nLists: Collections of elements that can contain different types of data structures, such as vectors, data frames, or even other lists. All list elements can be assigned a specific name. To create a list, use list(): e.g., list(a = 1, b = \"text\", c = TRUE, d = c(1, 2, 3)).\n\nEach of these data structures supports a range of operations for data manipulation and can be essential in R for organizing and analysing data effectively.\n\n\n3.2.3 Arithmetic operations, logical operations in R.\nR supports a range of arithmetic and logical operations:\n\nArithmetic Operations: Perform basic mathematical operations on numbers (type Numeric).\n\nAddition: e.g., 5 + 3\nSubtraction: e.g., 5 - 3\nMultiplication: e.g., 5 * 3\nDivision: e.g., 5 / 3\nExponentiation: 5 ^ 3\nModulus: 5 %% 3 (remainder)\n\nLogical Operations: Compare values (any type), returning Boolean values (type Logical TRUE or FALSE).\n\nEqual to: e.g., 5 == 3\nNot equal to: e.g., 5 != 3\nGreater than: e.g., 5 &gt; 3\nLess than: e.g., 5 &lt; 3\nLogical AND: e.g., TRUE & FALSE\nLogical OR: e.g., TRUE | FALSE\n\n\n\n\n3.2.4 Algorithm declaration\nAn algorithm is a step-by-step set of instructions used to solve a problem or perform a task, enabling computers to process data efficiently. Algorithms are normally fixed as re-usable code in the form of functions (or methods, depending on the programming language). These might take (or require) certain input variables, named “arguments” or “parameters”, and return certain output variables.\nIn R, a function can be declared (i.e., created for later use) by using the keyword funtion(), which is in itself a function already declared in R by default (i.e., a “primitive” function).\n\nadd_numbers &lt;- function(a, b) {\n  return(a + b)\n}\n\nA function declaration will then prescribe the input variables taken as variable names placed inside a parenthesis and separated by commas (e.g., funtion(a, b)) and use the same names inside the function’s code, enclosed by curly brackets (e.g., { return(a + b) }). A function can perform a series of operations, some of which can have external consequences, such as printing console messages or creating or modifying files. In R, a function’s output, if anything, is always a single R object, enclosed as an argument of another primitive function, return() (e.g., return(a + b)). Any R objects created or modified inside a function’s code, but not included in the output, will be erased afterwards.\nOnce declared, a function will be available or “loaded in the R session” for further use, using whatever values we assign to its input variables. For example, when a = 3 and b = 5, add_numbers() will return:\n\nresult &lt;- add_numbers(3, 5)\nprint(result)\n\n[1] 8\n\n\n\n\n3.2.5 Control flow structures\nIn programming, algorithms are executed through structured pathways known as control flow structures, determining the order in which instructions are carried out. Control flow structures include sequence (executing statements in order), selection (using conditions like “if” statements), and iteration (looping through repeated instructions). These structures direct the program’s execution path, ensuring it meets the logical requirements of the algorithm and reaches a solution effectively (“Control Flow” 2024; “Control Structures in Programming Languages” 2020).\nR provides basic control flow structures for implementing algorithms:\n\nif and else: Execute code based on a condition.\n\n\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is not greater than 5\")\n}\n\n[1] \"x is greater than 5\"\n\n\n\nfor loop: Repeat code over the elements of a vector.\n\n\nx &lt;- c(\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\")\n# iterate over numeric index, created on the fly\nfor (i in seq_along(x)) {\n  print(x[i])\n}\n\n[1] \"First\"\n[1] \"Second\"\n[1] \"Third\"\n[1] \"Fourth\"\n[1] \"Fifth\"\n\n# iterate over the values of x directly\nfor (value in x) {\n  print(value)\n}\n\n[1] \"First\"\n[1] \"Second\"\n[1] \"Third\"\n[1] \"Fourth\"\n[1] \"Fifth\"\n\n\n\nwhile loop: Repeat code while a condition is true.\n\n\ncount &lt;- 1\nwhile (count &lt;= 5) {\n  print(count)\n  count &lt;- count + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\n\n\n\nGetting help\n\n\n\nWhile learning R and its extensive vocabulary, it is useful to know where to get help. Beyond searching for specific questions online (always a good option), it can be clarifying enough to read the relevant fragment of R documentation.\nThere are mainly two ways of accessing R documentation:\n\nConsole command: In the R console, type ? followed by the name of a function (e.g., ?factor) or a canonical dataset (e.g., ?iris, see Importing data).\nRStudio Help tab: Go the Help tab in the Output area of RStudio’s interface (bottom left), and using the search field (top right in Help), type the name of a function or canonical dataset.\n\nThe R documentation entry for a function explains briefly:\n\nwhat it does (“Description”)\nwhat arguments it takes (“Arguments”)\nwhat values it outputs (“Value”)\nadditional information about its behaviour (“Details”, etc.)\nrelated bibliographic references (“References”)\nuse examples in code (“Examples”)\n\nNotice that sometimes one documentation entry might be related to an entire group of functions. For example, the entry on factor() also refers to as.factor() and is.factor(), among others.\nWarning: R will only find documentation of those functions and datasets currently loaded in the session environment. Whenever the consultation involves a package (see below), make sure that it has been installed and loaded.\n\n\n\n\n3.2.6 Writing and executing R scripts\nTo create and run an R script:\n\nGo to File &gt; New File &gt; R Script in RStudio.\nWrite your code in the editor. For example:\n\n\n# Simple R Script\nx &lt;- 5\ny &lt;- 10\nsum &lt;- x + y\nprint(sum)\n\n[1] 15\n\n\n\nHighlight the code and press Ctrl + Enter (Windows) or Cmd + Enter (Mac) to execute it in the Console.\n\n\n\n3.2.7 Using packages\nPackages in R are collections of functions, data, and documentation that extend R’s capabilities. They allow you to perform specialized tasks without having to write code from scratch. To use a package, you need to first install it and then load it into your R session.\n\nInstalling and loading packages\nTo install a package, use the install.packages() function. For example, to install the ggplot2 package:\n\ninstall.packages(\"ggplot2\")\n\nAlternatively, you may use the GUI Wizard in Tools &gt; Install Packages…, where an autocomplete feature will help selecting packages exact names.\nOnce installed, load the package with the library() function:\n\nlibrary(ggplot2)\n\nNow you can access the functions within ggplot2 and any other loaded package. You only need to install a package once, but you must load it in each new session.\n\n\nPackage collection: tidyverse\nThe tidyverse is a collection of R packages designed for data science, making data manipulation, visualization, and analysis easier and more intuitive (Wickham et al. 2019; “Tidyverse” n.d.). It includes:\n\nggplot2: For creating data visualizations using a layered approach.\ndplyr: For data manipulation, including filtering, summarizing, and arranging data.\ntidyr: For reshaping and tidying data.\nreadr: For reading data files into R quickly.\npurrr: For functional programming, allowing you to work with lists and vectors more effectively.\ntibble: A modern version of data frames with enhanced printing and subsetting.\n\nThe installation and use of the entire tidyverse works as a single package:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nThe tidyverse packages considerably change the way of working with R. Indeed, tidyverse code is now often used for introducing data science in R, since it is much easier to read and learn for beginners. However, it brings with it dependencies (i.e. other packages) and sometimes hide certain potentials that can only be explored with base R. In this course, we try to keep a balanced perspective by offering a glimpse of more than one R code solutions.\n\n\nPackage collection: tesselle\nThe tesselle collection is a suite of R packages specifically designed for teaching archaeological data analysis and modelling. These packages provide tools for handling and analysing spatial and temporal patterns in archaeological datasets, making it easier to derive insights from complex data, particularly count data, compositional data and chronological data (Frerebeau 2023; “Tesselle: R Packages & Archaeology” n.d.).\nInstall the complete suite with:\n\ninstall.packages(\"tesselle\")\n\nWe will look into more details about this collection in Chapter 7 and Chapter 6.",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "session-r-introduction.html#data-manipulation",
    "href": "session-r-introduction.html#data-manipulation",
    "title": "3  Introduction to R",
    "section": "3.3 Data Manipulation",
    "text": "3.3 Data Manipulation\n\n3.3.1 Importing data\n\nReading CSV Files\nTo read data from a CSV file, use the read.csv() or read_csv() function from the readr package. The read.csv() function is part of base R and handles typical CSV formats, while read_csv() is optimized for speed and flexibility in handling larger files.\n\n# Base R\ndata &lt;- read.csv(\"path/to/file.csv\")\n\n# Using readr package (installation required)\nlibrary(readr)\ndata &lt;- read_csv(\"path/to/file.csv\")\n\n\n\nWriting CSV Files\nTo save your data as a CSV file, use the write.csv() or write_csv() functions.\n\n# Base R\nwrite.csv(data, \"path/to/output.csv\")\n\n# Using readr package\nwrite_csv(data, \"path/to/output.csv\")\n\n\n\nUsing Canonical Datasets\nR includes several built-in or “canonical” datasets, like iris, useful for having a common benchmark for testing and giving examples. Other canonical datasets are available inside packages dedicated specialized fields, like archdata containing several archaeological datasets. To load a canonical dataset, use data():\n\n# Load the built-in iris dataset\ndata(iris)\n\n# Load the DartPoints dataset from the archdata package\nlibrary(archdata)\ndata(\"DartPoints\")\n\nAs long as the dataset is from base R, we can skip the loading step and use the dataset directly in an operation by using its name:\n\n# Find out how many columns the iris dataset have \nncol(iris)\n\n[1] 5\n\n\n\n\n\n3.3.2 Basic operations on data structures\nWe create a dummy dataset for us to test operations:\n\ndf &lt;- data.frame(x = 1:3, y = 4:6)\ndf # print it in the console\n\n  x y\n1 1 4\n2 2 5\n3 3 6\n\n\n\nAdding/Removing Elements\n\nAdding Elements: Use functions like cbind() for columns and rbind() for rows to add elements to data frames and matrices.\n\n# Adding a column\ndf$z &lt;- 7:9  # Adding column using $\ndf\n\n  x y z\n1 1 4 7\n2 2 5 8\n3 3 6 9\n\n# Adding a row\nnew_row &lt;- data.frame(x = 4, y = 7, z = 10)\ndf &lt;- rbind(df, new_row)\ndf\n\n  x y  z\n1 1 4  7\n2 2 5  8\n3 3 6  9\n4 4 7 10\n\n\nRemoving Elements: Use NULL assignment or subset() to remove columns or rows.\n\n# Removing a column\ndf$z &lt;- NULL\ndf\n\n  x y\n1 1 4\n2 2 5\n3 3 6\n4 4 7\n\n# Removing rows by index\ndf &lt;- df[-c(1, 2), ]\ndf\n\n  x y\n3 3 6\n4 4 7\n\n\n\n\n\nIndexing\nRecreate the initial dataset:\n\ndf &lt;- data.frame(x = 1:3, y = 4:6)\n\n\nNumeric Indexing: Access elements by specifying their position.\n\ndf[1, 2]  # First row, second column\n\n[1] 4\n\ndf[1:3, ]  # First three rows, all columns\n\n  x y\n1 1 4\n2 2 5\n3 3 6\n\n\nLogical Indexing: Filter based on logical conditions.\n\ndf[df$y &gt; 5, ]  # Rows where column y &gt; 5\n\n  x y\n3 3 6\n\n\nColumn Selection: Access columns by $ (column name) or numeric index.\n\ndf$x  # Access column by name\n\n[1] 1 2 3\n\ndf[, 1]  # Access first column by index\n\n[1] 1 2 3\n\n\nRow Selection: Filter rows with conditions.\n\ndf[1:2, ]  # First two rows\n\n  x y\n1 1 4\n2 2 5\n\ndf[df$x &gt; 2, ]  # Rows where column x &gt; 2\n\n  x y\n3 3 6\n\n\nCombining Indexing: Combine row and column filters for specific elements.\n\ndf[1:2, c(\"x\", \"y\")]  # First two rows, columns x and y\n\n  x y\n1 1 4\n2 2 5\n\n\n\n\n\nSubsetting (subset)\n\nThe subset() function provides an easy way to filter data by specifying conditions.\n\nsubset(df, x &gt; 2 & y &lt; 6)  # Filter rows where x &gt; 2 and y &lt; 6\n\n[1] x y\n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\nJoining or Merging (merge)\n\nUse merge() to combine two data frames based on a common key or column.\n\ndf1 &lt;- data.frame(id = 1:3, x = c(\"A\", \"B\", \"C\"))\ndf1\n\n  id x\n1  1 A\n2  2 B\n3  3 C\n\ndf2 &lt;- data.frame(id = 2:4, y = c(\"D\", \"E\", \"F\"))\ndf2\n\n  id y\n1  2 D\n2  3 E\n3  4 F\n\nmerged_df &lt;- merge(df1, df2, by = \"id\")\nmerged_df\n\n  id x y\n1  2 B D\n2  3 C E\n\n\n\n\n\nModifying Elements with Functions (apply)\n\nThe apply() function allows operations across rows or columns of a data frame or matrix.\n\n# Summing across rows (1 indicates rows)\napply(df[, 1:2], 1, sum)\n\n[1] 5 7 9\n\n# Summing across columns (2 indicates columns)\napply(df[, 1:2], 2, sum)\n\n x  y \n 6 15 \n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Indexing — R Spatial” (n.d.)\n\nPhillips (n.d.)\n\n“Indexing and Slicing Data Frames in R” (2022)\n\n“Subsetting Data  R Learning Modules” (n.d.)\n\n“Indexing into a Data Structure” (n.d.)\n\n\n\n\n\n\n\n3.3.3 Using dplyr Package\nThe dplyr package in R provides a suite of functions to manipulate data in a streamlined and readable way. Key functions include those for filtering, selecting, and mutating data, which help manage data subsets, reorganize columns, and create new variables.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndf &lt;- data.frame(x = 1:30, y = rep(c(\"A\", \"B\", \"C\"), 10), z = rep(c(TRUE, FALSE), 15))\n\n\nFiltering Data\n\nUse filter() to select rows based on conditions. Multiple conditions can be combined with & (and) or | (or).\n\n\nfiltered_data &lt;- df %&gt;% filter(x &gt; 10, y == \"A\")\nfiltered_data\n\n   x y     z\n1 13 A  TRUE\n2 16 A FALSE\n3 19 A  TRUE\n4 22 A FALSE\n5 25 A  TRUE\n6 28 A FALSE\n\nfiltered_data &lt;- df %&gt;% filter(z)\nfiltered_data\n\n    x y    z\n1   1 A TRUE\n2   3 C TRUE\n3   5 B TRUE\n4   7 A TRUE\n5   9 C TRUE\n6  11 B TRUE\n7  13 A TRUE\n8  15 C TRUE\n9  17 B TRUE\n10 19 A TRUE\n11 21 C TRUE\n12 23 B TRUE\n13 25 A TRUE\n14 27 C TRUE\n15 29 B TRUE\n\n\nSelecting Columns\n\nUse select() to choose specific columns by name. You can use : to select a range of columns or - to exclude columns.\n\n\nselected_data &lt;- df %&gt;% select(x, y)\nselected_data\n\n    x y\n1   1 A\n2   2 B\n3   3 C\n4   4 A\n5   5 B\n6   6 C\n7   7 A\n8   8 B\n9   9 C\n10 10 A\n11 11 B\n12 12 C\n13 13 A\n14 14 B\n15 15 C\n16 16 A\n17 17 B\n18 18 C\n19 19 A\n20 20 B\n21 21 C\n22 22 A\n23 23 B\n24 24 C\n25 25 A\n26 26 B\n27 27 C\n28 28 A\n29 29 B\n30 30 C\n\nselected_data &lt;- df %&gt;% select(-z)  # Exclude z (third column)\nselected_data\n\n    x y\n1   1 A\n2   2 B\n3   3 C\n4   4 A\n5   5 B\n6   6 C\n7   7 A\n8   8 B\n9   9 C\n10 10 A\n11 11 B\n12 12 C\n13 13 A\n14 14 B\n15 15 C\n16 16 A\n17 17 B\n18 18 C\n19 19 A\n20 20 B\n21 21 C\n22 22 A\n23 23 B\n24 24 C\n25 25 A\n26 26 B\n27 27 C\n28 28 A\n29 29 B\n30 30 C\n\n\nMutating Data\n\nThe mutate() function creates new columns or modifies existing ones by performing calculations or transformations on current columns.\n\n\nmutated_data &lt;- df %&gt;% mutate(new_column = x * 2)\nmutated_data\n\n    x y     z new_column\n1   1 A  TRUE          2\n2   2 B FALSE          4\n3   3 C  TRUE          6\n4   4 A FALSE          8\n5   5 B  TRUE         10\n6   6 C FALSE         12\n7   7 A  TRUE         14\n8   8 B FALSE         16\n9   9 C  TRUE         18\n10 10 A FALSE         20\n11 11 B  TRUE         22\n12 12 C FALSE         24\n13 13 A  TRUE         26\n14 14 B FALSE         28\n15 15 C  TRUE         30\n16 16 A FALSE         32\n17 17 B  TRUE         34\n18 18 C FALSE         36\n19 19 A  TRUE         38\n20 20 B FALSE         40\n21 21 C  TRUE         42\n22 22 A FALSE         44\n23 23 B  TRUE         46\n24 24 C FALSE         48\n25 25 A  TRUE         50\n26 26 B FALSE         52\n27 27 C  TRUE         54\n28 28 A FALSE         56\n29 29 B  TRUE         58\n30 30 C FALSE         60\n\n\n\nThese functions are especially powerful when combined in recursive structures or “pipelines” using %&gt;% to streamline data manipulation tasks.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“A Grammar of Data Manipulation” (n.d.)\n\n“Data: Starwars” (n.d.)\n\n“Aggregating and Analyzing Data with Dplyr” (n.d.)\n\n“Dplyr Package in R Programming” (2020)\n\nBhalla (n.d.)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "session-r-introduction.html#data-visualization",
    "href": "session-r-introduction.html#data-visualization",
    "title": "3  Introduction to R",
    "section": "3.4 Data Visualization",
    "text": "3.4 Data Visualization\nLet us load the DartPoints dataset from the archdata package and use it in the examples:\n\nlibrary(archdata)\ndata(DartPoints)\n\n\n3.4.1 Creating Plots in R with Base R Graphics\nBase R provides quick ways to create plots using functions like plot(), hist(), barplot(), and boxplot().\n\nHistograms: Display the distribution of a single continuous variable.\n\nhist(DartPoints$Length, main = \"Histogram of Dart Points Length\", xlab = \"Maximum Length (mm)\")\n\n\n\n\n\n\n\n\nBar plots: Used for categorical data to show the frequency or count for each category.\n\nbarplot(table(DartPoints$Blade.Sh), main = \"Bar Plot of Blade Shape\", xlab = \"Blade Shape categories\", names.arg = c(\"Excurvate\", \"Incurvate\", \"Recurvate\", \"Straight\"))\n\n\n\n\n\n\n\n\nBox plots: Visualize the relationship between one continuous variable and one categorical variable.\n\nboxplot(Length ~ Blade.Sh, DartPoints, main = \"Box Plot of Length vs Blade Shape\", xlab = \"Blade Shape categories\", ylab = \"Maximum Length (mm)\", names = c(\"Excurvate\", \"Incurvate\", \"Recurvate\", \"Straight\"))\n\n\n\n\n\n\n\n\nScatter plots: Visualize the relationship between two continuous variables.\n\nplot(DartPoints$Width, DartPoints$Length, main = \"Scatter Plot of Length vs Width\", xlab = \"Maximum Width (mm)\", ylab = \"Maximum Length (mm)\")\n\n\n\n\n\n\n\n\n\nIt is possible to customise the graphics much further, for example, by colouring (col argument) or using different point shapes (pch argument) in scatter plots:\n\nplot(DartPoints$Width, DartPoints$Length, \n     col = DartPoints$Blade.Sh, # use factor as it is, plot() will convert it into colours\n     pch = as.numeric(DartPoints$Blade.Sh), # use factor index as code for symbols\n     main = \"Scatter Plot of Length vs Width\", \n     xlab = \"Maximum Width (mm)\", ylab = \"Maximum Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n3.4.2 Creating Multiple Plot Figures with layout\n\nUse layout() to organize multiple plots in a single window.\n\nlayout(matrix(1:2, nrow = 1))\nhist(DartPoints$Length, main = \"Plot 1: histogram\", xlab = \"Maximum Length (mm)\")\nboxplot(DartPoints$Length, main = \"Plot 2: boxplot\")\n\n\n\n\n\n\n\n\nWith the right combination of additional arguments, we are able to produce customised layouts for a combined plot:\n\nlayout(matrix(1:2, nrow = 2))\npar(mar = c(0.1, 5, 1, 1))\nhist(DartPoints$Length, xaxt='n', main = \"Distribution of Maximum Length in the DartPoints dataset\", xlab = \"\", ylab = \"Frequency\")\npar(mar = c(5, 5, 0.1, 1))\nboxplot(DartPoints$Length, horizontal = TRUE, xlab = \"Maximum Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 Creating Plots in ggplot2\nggplot2 is an extensive package for creating complex and aesthetically pleasing graphics.\n\nlibrary(ggplot2)\n\nHere are a few examples using ggplot2:\n\nHistograms:\n\nggplot(DartPoints, aes(x = Length)) + \n  geom_histogram() +\n  labs(title = \"Histogram of Dart Points Length\")\n\n\n\n\n\n\n\n\nBar plots:\n\nggplot(DartPoints, aes(x = Blade.Sh)) + \n  geom_bar() +\n  scale_x_discrete(labels = c(\"Excurvate\", \"Incurvate\", \"Recurvate\", \"Straight\")) + \n  labs(title = \"Bar plot of Blade Shape\", x = \"Blade Shape categories\")\n\n\n\n\n\n\n\n\nBox plots:\n\nggplot(DartPoints, aes(x = Blade.Sh, y = Length)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = c(\"Excurvate\", \"Incurvate\", \"Recurvate\", \"Straight\")) + \n  labs(title = \"Box Plot of Length vs Blade Shape\", x = \"Blade Shape categories\", y = \"Maximum Length (mm)\")\n\n\n\n\n\n\n\n\nScatter plots:\n\nggplot(DartPoints, aes(x = Width, y = Length)) +\n  geom_point() +\n  labs(title = \"Length vs Width\")\n\n\n\n\n\n\n\n\n\nAs base R, ggplot2 also allow customising graphics. Replicating the same example, we add colors and shape matching a categorical variable:\n\nggplot(DartPoints, aes(x = Width, y = Length, \n                       color = Blade.Sh, # use factor as it is, ggplot() will convert it into colours\n                       shape = Blade.Sh)) + # use factor as it is, ggplot() will convert it into shapes\n  geom_point() +\n  labs(title = \"Length vs Width by Blade Shape\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n3.4.4 Creating Multiple Plot Figures with gridExtra::grid.arrange\ngridExtra::grid.arrange() allows for multiple ggplot2 plots in one layout.\n\nlibrary(gridExtra)\np1 &lt;- ggplot(DartPoints, aes(x = Length)) + geom_histogram()\np2 &lt;- ggplot(DartPoints, aes(x = factor(Blade.Sh))) + geom_bar() + xlab(\"Blade Shape categories\")\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n3.4.5 Base R Graphics and ggplot2: Comparative\nBase R is quick and requires less setup, ideal for simple exploratory analysis. It offers a more robust and flexible framework for building complex custom plots, containing multiple plot types and separated R objects.\nggplot2 provides a grammar of graphics, which is more powerful and customizable for detailed visualizations. In general, it produces more attractive plots for public display and it is the best choice whenever the goal to produce a standard plot from a single dataset.\n\n\n3.4.6 Saving Plots: Opening and Closing Graphic Devices in R\nUse png(), pdf(), etc., to open graphic devices, giving at least the path and file name as argument, and dev.off() to close the device. After execution, the corresponding file will be created at the specified directory.\n\npng(\"myPlots/plot.png\")\nplot(mtcars$mpg, mtcars$wt)\ndev.off()\n\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nEngel (n.d.b)\n\n“Function Reference” (n.d.)\n\nHoltz (n.d.a)\n\nKabacoff (n.d.)\n\nHoltz (n.d.b)\n\nQin (n.d.)\n\nHealy (n.d.)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "session-r-introduction.html#interactive-visualizations",
    "href": "session-r-introduction.html#interactive-visualizations",
    "title": "3  Introduction to R",
    "section": "3.5 Interactive Visualizations",
    "text": "3.5 Interactive Visualizations\n\n3.5.1 Introduction to Creating Interactive Visualizations\nInteractive visualizations enhance data exploration by allowing users to zoom, pan, hover, and filter elements in a plot. In R, libraries like plotly integrate with ggplot2 and base graphics to turn static plots into interactive ones. The plotly package is particularly useful for creating dashboards and presentations, as it enables seamless transitions between static and interactive graphs.\n\n\n3.5.2 Example: Building an Interactive Plot using plotly and knitr\nWith plotly and knitr in R Markdown, you can embed interactive plots directly in a report.\n\n# Load libraries\nlibrary(plotly)\nlibrary(knitr)\n\n# Change Blade.Sh levels to a full text form\nDartPoints$Blade.Sh &lt;- factor(DartPoints$Blade.Sh,\n                              levels = c(\"E\", \"I\", \"R\", \"S\"),\n                              labels = c(\"Excurvate\", \"Incurvate\", \"Recurvate\", \"Straight\"))\n\n# Create a basic ggplot2 scatter plot\np &lt;- ggplot(DartPoints, aes(x = Width, y = Length, color = Blade.Sh)) +\n  geom_point(size = 2) +\n  labs(title = \"Length vs Width by Blade Shape\", color = \"Blade Shape\")\n\n# Convert ggplot2 object to plotly\ninteractive_plot &lt;- ggplotly(p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\nThis example takes a ggplot2 scatter plot and transforms it into an interactive visualization with plotly. You can embed this code in an R Markdown document to generate a shareable, interactive HTML report.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nEngel (n.d.a)\n\nMéo (2023)\nCollins (2020)\n\nMajumder (2021)\n\nNHS-R Community (2021)\n\n\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nCreate a new project in RStudio, placing it at the root directory of your own repository (cloned local branch). Inside, create a “data” and a “plots” subdirectories (i.e. folders).\nCreate a Rmarkdown document and give it a title, e.g. “Introduction to R: an exercise”.\nIn this document, create a R code chunk to start working and create new chunks and markdown commentaries at every computation step, as you see fit.\nCreate a data frame named “stone_tools_data” directly in R (i.e. constructed by typing vectors and joining them by code) with the following characteristics (based on Carlson 2017, p. 26):\n\nSet of six stone tools with inventory number\n\nVariables or columns: recording of dimensions (length, breadth, thickness), material type, and whether the material is local or non-local.\n\nData per object:\n\nLN15:\n\nLength: 18\n\nBreadth: 9\n\nThickness: 3\n\nMaterial type: chert\n\nMaterial provenance: local\n\n\nLN17:\n\nLength: 14\n\nBreadth: 7\n\nThickness: 2\n\nMaterial type: chert\n\nMaterial provenance: local\n\n\nLN18:\n\nLength: 21\n\nBreadth: 10\n\nThickness: 3\n\nMaterial type: obsidian\n\nMaterial provenance: local\n\n\nLN21:\n\nLength: 14\n\nBreadth: 7\n\nThickness: 3\n\nMaterial type: chert\n\nMaterial provenance: non-local\n\n\nLN23:\n\nLength: 17\n\nBreadth: 8\n\nThickness: 3\n\nMaterial type: obsidian\n\nMaterial provenance: local\n\n\nLN24:\n\nLength: 16\n\nBreadth: 8\n\nThickness: 2\n\nMaterial type: obsidian\n\nMaterial provenance: non-local\n\n\n\nCheck that the data and data types are coherent with the specifications. Save it as a CSV file (stored in a “data” subdirectory) and load it back as a new R object (e.g. “stone_tools_data2”). Compare.\nCreate a plot showing the counts of objects made of chert and obsidian. Save it as a PNG file in “plots” subdirectory.\nCreate a new variable (“type_and_provenance”) that combines type and provenance and create a plot showing the counts in each category. Save it as a PNG file in “plots” subdirectory.\nCreate a single figure displaying the variable distribution of the three dimensions measured. Save it as both a PNG and a SVG file in “plots” subdirectory.\nCreate a plot displaying the relationship between length and breadth. Save it as a PNG file in “plots” subdirectory.\nCreate a plot displaying the relationship between length and breadth, this time marking (point type, colour) objects by their “type_and_provenance”. Save it as both a PNG and a EPS file in “plots” subdirectory.\n(EXTRA) Create a figure to help explore the question: Do stone tools of different material and provenance tend to be of different size?\n(EXTRA) Duplicate the Rmarkdown document and add “_tidyverse” to the file name. Inside, replicate all steps and using tidyverse and ggplot functions.\nCommit all changes and push to the remote using RStudio (use the buttons in the Git tab in the “Environments” panel on the top right).\nQ&A and troubleshooting.\n\n\n\n\n\n\n\n“A Grammar of Data Manipulation.” n.d. Accessed October 29, 2024. https://dplyr.tidyverse.org/.\n\n\n“Aggregating and Analyzing Data with Dplyr.” n.d. Accessed October 29, 2024. https://datacarpentry.org/R-genomics/04-dplyr.html.\n\n\nBhalla, Deepanshu. n.d. “Dplyr Tutorial : Data Manipulation (50 Examples).” ListenData. Accessed October 29, 2024. https://www.listendata.com/2016/08/dplyr-tutorial.html.\n\n\nCollins, Neil. 2020. “How to Create Interactive Reports in R Markdown Part II: Data Visualisation.” Medium. https://medium.com/@SportSciData/how-to-create-interactive-reports-in-r-markdown-part-ii-data-visualisation-6b8061136370.\n\n\n“Control Flow.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=Control_flow&oldid=1253332211.\n\n\n“Control Structures in Programming Languages.” 2020. GeeksforGeeks. https://www.geeksforgeeks.org/control-structures-in-programming-languages/.\n\n\n“Data: Starwars.” n.d. Accessed October 29, 2024. https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html.\n\n\n“Dplyr Package in R Programming.” 2020. GeeksforGeeks. https://www.geeksforgeeks.org/dplyr-package-in-r-programming/.\n\n\nEngel, Claudia A. n.d.a. Chapter 2 Interactive Graphs  Data Visualization with R. Accessed October 29, 2024. https://cengel.github.io/R-data-viz/interactive-graphs.html.\n\n\n———. n.d.b. Data Visualization with R. Accessed October 29, 2024. https://cengel.github.io/R-data-viz/.\n\n\nFrerebeau, Nicolas. 2023. Tesselle: Easily Install and Load ’Tesselle’ Packages. Pessac, France: Université Bordeaux Montaigne. https://doi.org/10.5281/zenodo.6500491.\n\n\n“Function Reference.” n.d. Accessed October 29, 2024. https://ggplot2.tidyverse.org/reference/.\n\n\nGrolemund, Garrett, J. J. Allaire. n.d. R Markdown: The Definitive Guide. Accessed October 29, 2024. https://bookdown.org/yihui/rmarkdown/.\n\n\nGrolemund, Hadley Wickham and Garrett. n.d. 27 R Markdown  R for Data Science. Accessed November 12, 2024. https://r4ds.had.co.nz/r-markdown.html.\n\n\nHealy, Yan Holtz and Conor. n.d. “From Data to Viz  Find the Graphic You Need.” Accessed October 29, 2024. https://www.data-to-viz.com/data-to-viz.com.\n\n\nHoltz, Yan. n.d.a. “Data Visualization with R and Ggplot2  the R Graph Gallery.” Accessed October 29, 2024. https://r-graph-gallery.com/ggplot2-package.html.\n\n\n———. n.d.b. “The R Graph Gallery – Help and Inspiration for R Charts.” The R Graph Gallery. Accessed October 29, 2024. https://r-graph-gallery.com/.\n\n\n“Indexing — R Spatial.” n.d. Accessed October 29, 2024. https://rspatial.org/intr/4-indexing.html.\n\n\n“Indexing and Slicing Data Frames in R.” 2022. GeeksforGeeks. https://www.geeksforgeeks.org/indexing-and-slicing-data-frames-in-r/.\n\n\n“Indexing into a Data Structure.” n.d. Accessed October 29, 2024. http://www.cookbook-r.com/Basics/Indexing_into_a_data_structure/.\n\n\n“Introduction.” n.d. Accessed October 29, 2024. https://rmarkdown.rstudio.com/lesson-1.html.\n\n\nKabacoff, Robert. n.d. Modern Data Visualization with R. Accessed October 29, 2024. https://rkabacoff.github.io/datavis/.\n\n\nLiang, Hao. 2024. “Hao203/Rmarkdown-YAML.” https://github.com/hao203/rmarkdown-YAML.\n\n\nMajumder, Prateek. 2021. “Guide to Create Interactive Plots with Plotly Python.” Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/10/interactive-plots-in-python-with-plotly-a-complete-guide/.\n\n\nMelfi, Vince, Jeffrey W. Doser. n.d. Chapter 3 Scripts and R Markdown  R Programming for Data Sciences. Accessed November 12, 2024. https://doserlab.com/files/for875/_book/scripts.\n\n\nMéo, Grace Di. 2023. “Creating Interactive Visualizations with Plotly.” Programming Historian, December. https://programminghistorian.org/en/lessons/interactive-visualization-with-plotly.\n\n\nNHS-R Community. 2021. “Plotting Interactive Visualizations with Plotly in R.” https://www.youtube.com/watch?v=WmofiOklux8.\n\n\nPhillips, Nathaniel D. n.d. YaRrr! The Pirate’s Guide to R. Accessed October 29, 2024. https://bookdown.org/ndphillips/YaRrr/.\n\n\nQin, Yufei. n.d. “Research Guides: Data Visualization in R: Introduction.” Accessed October 29, 2024. https://libguides.princeton.edu/c.php?g=1350384&p=9965533.\n\n\n“R Markdown.” n.d. Accessed October 29, 2024. https://rmarkdown.rstudio.com/index.html.\n\n\n“R Markdown Reporting Best Practices.” n.d. Accessed November 12, 2024. https://www.appsilon.com/post/r-markdown-reporting-best-practices.\n\n\nRiederer, Emily, Christophe Dervieux. n.d. 2.2 R Markdown Anatomy  R Markdown Cookbook. Accessed November 12, 2024. https://bookdown.org/yihui/rmarkdown-cookbook/rmarkdown-anatomy.html.\n\n\n“Rmarkdown :: Cheatsheet.” n.d. Accessed October 14, 2024. https://rstudio.github.io/cheatsheets/html/rmarkdown.html?_gl=1*1k59g6x*_ga*MjAyMzI2NjEwMC4xNzIzODI3Mjk2*_ga_2C0WZ1JHG0*MTcyODkxNzc2NS4xMy4xLjE3Mjg5MTkwNTIuMC4wLjA.\n\n\n“RStudio IDE :: Cheatsheet.” n.d. Accessed October 14, 2024. https://rstudio.github.io/cheatsheets/html/rstudio-ide.html?_gl=1*11i4y2y*_ga*MjAyMzI2NjEwMC4xNzIzODI3Mjk2*_ga_2C0WZ1JHG0*MTcyODkxNzc2NS4xMy4xLjE3Mjg5MTkwNTIuMC4wLjA.\n\n\n“RStudio User Guide - RStudio IDE User Guide.” 2024. RStudio User Guide. https://docs.posit.co/ide/user/.\n\n\nSmith, Zachary M. n.d. 9 Lesson 4: YAML Headers  R Markdown Crash Course. Accessed November 12, 2024. https://zsmith27.github.io/rmarkdown_crash-course/lesson-4-yaml-headers.html.\n\n\n“Subsetting Data  R Learning Modules.” n.d. Accessed October 29, 2024. https://stats.oarc.ucla.edu/r/modules/subsetting-data/.\n\n\n“Tesselle: R Packages & Archaeology.” n.d. Tesselle. Accessed October 14, 2024. https://www.tesselle.org/.\n\n\n“Tidyverse.” n.d. Accessed October 14, 2024. https://www.tidyverse.org/.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "session-r-programming.html",
    "href": "session-r-programming.html",
    "title": "4  Best practices in programming",
    "section": "",
    "text": "4.1 Code organisation",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Best practices in programming</span>"
    ]
  },
  {
    "objectID": "session-r-programming.html#code-organisation",
    "href": "session-r-programming.html#code-organisation",
    "title": "4  Best practices in programming",
    "section": "",
    "text": "4.1.1 Modular programming\nImportance of modularity\nBreaking down your code into functions and modules enhances readability, maintainability, and reusability. This approach helps isolate specific tasks and allows for easier debugging and testing.\n\n\n\n\n\n\nSingle-responsability Principle\n\n\n\nThe Single-responsibility Principle (SRP) is a key concept in software design that ensures each function or module in your code has one, and only one, reason to change. For students learning R, this means breaking down your code into smaller, well-defined functions where each function does one specific task. This makes your code easier to understand, test, and maintain (“Single-Responsibility Principle - Wikipedia” n.d.).\nFor example, instead of writing one long script that loads data, cleans it, and plots it, you can write separate functions for each task: - load_data() handles loading the data. - clean_data() takes care of data cleaning. - plot_data() is responsible for plotting.\nBy adhering to SRP, you reduce the chance of introducing bugs when modifying or extending your code. If the way data is loaded changes, you only need to adjust load_data() without worrying about unintended side effects on clean_data() or plot_data().\n\n\n\nExample: Creating in-script custom functions\n# Custom function to calculate mean\ncalculate_mean &lt;- function(data) {\n  mean(data, na.rm = TRUE)\n}\n\n# Usage\nnumbers &lt;- c(1, 2, 3, NA, 5)\ncalculate_mean(numbers)\n\n\nExample: Wrapping messy code into clear functions\nMessy Code Example:\n\n# Messy and hard to follow\ndata &lt;- read.csv(\"data.csv\")\ndata$cleaned &lt;- na.omit(data$column)\ndata$scaled &lt;- (data$cleaned - mean(data$cleaned)) / sd(data$cleaned)\nhist(data$scaled)\n\nWrapped in functions clearly defined:\n\n# Function to load data\nload_data &lt;- function(filepath) {\n  read.csv(filepath)\n}\n\n# Function to clean data\nclean_data &lt;- function(data, column) {\n  na.omit(data[[column]])\n}\n\n# Function to scale data\nscale_data &lt;- function(data) {\n  (data - mean(data)) / sd(data)\n}\n\n# Function to plot histogram\nplot_histogram &lt;- function(data) {\n  hist(data)\n}\n\n# Main execution sequence\nmain &lt;- function() {\n  raw_data &lt;- load_data(\"data.csv\")\n  cleaned_data &lt;- clean_data(raw_data, \"column\")\n  scaled_data &lt;- scale_data(cleaned_data)\n  plot_histogram(scaled_data)\n}\n\n# Run the main function\nmain()\n\nBy organizing the operations into clear, single-responsibility functions, the code becomes more readable, maintainable, and easier to debug.\n\n\nExample: Creating and importing custom R scripts\n\nCreating a script: Save the following function in a file named utility_functions.R.\n\n# utility_functions.R\ncalculate_sum &lt;- function(data) {\n  sum(data, na.rm = TRUE)\n}\n\nImporting the script:\n\nsource(\"utility_functions.R\") # loads all functions defined inside the script\nnumbers &lt;- c(1, 2, 3, NA, 5)\ncalculate_sum(numbers)\n\n\n\n\n\n4.1.2 Code structuring\nStructuring a data science project\nA well-organized project structure separates code, data, and outputs, which facilitates efficient project management.\n\nExample: Setting up a basic project structure in R and RStudio\n\nFolder Structure:\nmy_project/  \n├── data/  \n│   └── raw_data.csv  \n├── scripts/  \n│   ├── 01_load_data.R  \n│   └── 02_analyse_data.R  \n│   └── 03_visualise_data.R  \n├── outputs/  \n│   └── analysis_results.csv  \n└── workflow.R  \n└── my_project.Rproj  \nScript Example:\n\n01_load_data.R\n\nload_data &lt;- function(file_name, dir = \"data\", save_rds = FALSE) {\n  file_path &lt;- paste(dir, file_name, sep = \"/\")\n  # load raw data\n  raw_data &lt;- read.csv(paste(file_path, \"csv\", sep = \".\"))\n  if (save_rds) {\n    # save a copy as a R dataset (.rds)\n    saveRDS(raw_data, file = paste(file_path, \"rds\", sep = \".\"))\n  }\n}\n\n02_analyse_data.R\n\nanalyse_data &lt;- function() {\n  summary_stats &lt;- summary(raw_data)\n  print(summary_stats)\n  # Save the results to an output file\n  write.csv(summary_stats, \"outputs/analysis_results.csv\")\n}\n\n03_visualise_data.R\n\nvisualise_data &lt;- function(dataset, plot_name, dir = \"outputs\", width = 480, height = 480) {\n  file_path &lt;- paste(dir, plot_name, sep = \"/\")\n  file_name &lt;- paste(file_path, \"png\", sep = \".\")\n  png(file_name, width = width, height = height)\n  pairs(dataset) # matrix of scatterplots\n  dev.off()\n}\n\nworkflow.R\n\nsource(\"01_load_data.R\")\nsource(\"02_analyse_data.R\")\nsource(\"03_visualise_data.R\")\n\ndt &lt;- load_data(\"raw_data\")\n\nanalyze_data(dt)\n\nvisualise_data(dt, \"Variables overview\", width = 560, height = 560)\n\n\n\nThis structure ensures clarity, with each component of the project clearly demarcated, promoting better workflow and collaboration.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nGrolemund (n.d.)\n\nGillespie and Lovelace (n.d.)\n\n“Welcome  Advanced R” (n.d.)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Best practices in programming</span>"
    ]
  },
  {
    "objectID": "session-r-programming.html#writing-clean-and-readable-code",
    "href": "session-r-programming.html#writing-clean-and-readable-code",
    "title": "4  Best practices in programming",
    "section": "4.2 Writing clean and readable code",
    "text": "4.2 Writing clean and readable code\nCode is effectively written for a computer to “understand” and execute (i.e., machine readable). However, humans revise, learn, and expand code (geek humans, but humans all the same!). Therefore, when writing code, you should remember a few practices that will make it easier to read and understand by humans (i.e., more human-readable).\n\n4.2.1 Naming conventions\n\nBad practices:\n\nNon-descriptive variable names\nUsing vague names makes it hard to understand the purpose of a variable.\n\n# Bad naming\na &lt;- 22.5\nb &lt;- function(x) {\n  mean(x)\n}\n\nInconsistent naming styles\nMixing different styles (e.g., camelCase, snake_case, and inconsistent capitalization) leads to confusion.\n\n# Inconsistent naming\ncalcMean &lt;- function(X) {\n  mean(X)\n}\ncalculate_mean &lt;- function(x) {\n  mean(x)\n}\n\n\n\n\nBest practices:\n\nUsing descriptive and consistent names\nChoose clear and consistent names to convey purpose.\n\n# Good naming conventions\naverage_temperature &lt;- 22.5\ncalculate_mean &lt;- function(values) {\n  mean(values)\n}\n\nFor more guidance, refer to the tidyverse style guide (“Tidyverse Style Guide” n.d.).\n\n\n\n\n4.2.2 Commenting and documentation\n\nBad practices:\n\nLack of comments\nSkipping comments leads to difficulty in understanding the purpose or logic.\n\n# No comments\ndata &lt;- read.csv(\"data.csv\")\nresult &lt;- data[data$score &gt; 10, ]\n\nOver-commenting obvious code\nCommenting on trivial lines unnecessarily clutters the code.\n\n# Adding 1 to x\nx &lt;- x + 1\n\n\n\n\nBest practices:\n\nMeaningful comments and using roxygen2\nDocument non-obvious logic and function purpose clearly.\n\n#' Filter data by score\n#'\n#' This function filters data for scores greater than 10.\n#' @param data A dataframe with a score column\n#' @return Filtered dataframe\nfilter_high_scores &lt;- function(data) {\n  data[data$score &gt; 10, ]\n}\n\nFor more about the roxygen2 package, see its “vignette” (“Learn More” n.d.).\n\n\n\n\n4.2.3 Avoiding magic numbers and hardcoding\n\n\n\n\n\n\nMagic numbers in programming\n\n\n\nMagic numbers refer to unique numeric values embedded directly in the code without context or explanation. These numbers often appear arbitrary and can make the code difficult to understand and maintain. They are problematic because:\n1. Lack of clarity: Without meaningful names, the purpose of the number is unclear to others (or to you in the future).\n2. Hard to update: If the same number appears in multiple places, updating it becomes error-prone and tedious.\n3. Reduced readability: Magic numbers obscure the intent of the code, making it harder to follow and debug.\n\n\n\nBad practices:\n\nUsing magic numbers\nHardcoding values without explanation can confuse users about their significance.\n\n# Hardcoded magic number\nfor (i in 1:7) {\n  print(i)\n}\n\n\n\n\nBest practices:\n\nDefining constants\nClearly define constants for better clarity and easy updates.\n\n# Using constants\nDAYS_IN_WEEK &lt;- 7\nfor (i in 1:DAYS_IN_WEEK) {\n  print(i)\n}\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Tidyverse Style Guide” (n.d.)\n\nAlboukadel (2020)\n\n“R Variables and Constants” (n.d.)\n\n“Style Guide · Advanced R.” (n.d.)\n\n“R Variables and Constants (With Examples)” (n.d.)\n\n“Google’s R Style Guide” (n.d.)\n\n\n\n\n\n\n\n4.2.4 Writing efficient and scalable code\nEfficient and scalable R code is crucial when dealing with large datasets or computationally intensive tasks. Scalability refers to the code’s ability to handle increasing amounts of data or complexity without significant performance degradation.\n\nVectorization\nVectorization involves replacing explicit loops with vectorized operations, which are more efficient and concise. This approach leverages R’s optimized internal functions to operate on entire vectors or matrices in one go, reducing execution time.\nExamples:\n\nBase R:\n\n# Loop approach\nresult &lt;- numeric(1000)\nfor (i in 1:1000) {\n  result[i] &lt;- i^2\n}\n\n# Vectorized approach\nresult &lt;- (1:1000)^2\n\nUsing dplyr:\n\nlibrary(dplyr)\ndata &lt;- tibble(x = 1:1000)\n\n# Vectorized operation with dplyr\ndata &lt;- data %&gt;%\n  mutate(square = x^2)\n\n\n\n\nMemory Management\nEfficient memory management is essential when working with large datasets to prevent memory exhaustion and improve performance. data.table is a package specifically designed for handling large datasets efficiently by minimizing memory usage and speeding up data operations.\nExample:\n\nUsing data.table:\n\nlibrary(data.table)\n\n# Creating a large data.table\ndt &lt;- data.table(x = rnorm(1e7), y = rnorm(1e7))\n\n# Efficiently computing on large datasets\ndt[, mean_x := mean(x)]\n\n\ndata.table optimizes memory usage by modifying data in place and avoids unnecessary copies, making it ideal for large-scale data processing.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nAnderson (n.d.)\n\n“R Dtplyr: How to Efficiently Process Huge Datasets with a Data.table Backend” (n.d.)\n\nLipkin (2022)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Best practices in programming</span>"
    ]
  },
  {
    "objectID": "session-r-programming.html#refactoring",
    "href": "session-r-programming.html#refactoring",
    "title": "4  Best practices in programming",
    "section": "4.3 Refactoring",
    "text": "4.3 Refactoring\nRefactoring refers to the process of improving existing code without changing its external behaviour. The goal is to make the code cleaner, more efficient, and easier to maintain. Refactoring enhances readability, reduces complexity, and simplifies future modifications.\n\n4.3.1 Why Refactor?\n\nImproves code readability: Well-structured code is easier to understand.\nEnhances maintainability: Simplified codebase reduces the time required for future updates or debugging.\nPromotes reusability: Modularized and clean code is more reusable in different contexts.\n\n\n\n4.3.2 Examples of Refactoring in R\n\n1. Removing Redundant Code\nBefore:\n\ndata &lt;- read.csv(\"data.csv\")\ndata_cleaned &lt;- na.omit(data)\ndata_cleaned &lt;- data_cleaned[data_cleaned$score &gt; 10, ]\n\nAfter:\n\ndata &lt;- read.csv(\"data.csv\") |&gt;\n  na.omit() |&gt;\n  subset(score &gt; 10)\n# or using dplyr\ndata &lt;- read.csv(\"data.csv\") %&gt;%\n  na.omit() %&gt;%\n  filter(score &gt; 10)\n\nUsing dplyr pipelines makes the code concise and easier to read.\n\n\n2. Breaking Down Long Functions\nBefore:\n\ncalculate_metrics &lt;- function(data) {\n  mean_value &lt;- mean(data$score)\n  sd_value &lt;- sd(data$score)\n  return(list(mean = mean_value, sd = sd_value))\n}\n\nAfter:\n\ncalculate_mean &lt;- function(data) {\n  mean(data$score)\n}\n\ncalculate_sd &lt;- function(data) {\n  sd(data$score)\n}\n\ncalculate_metrics &lt;- function(data) {\n  list(mean = calculate_mean(data), sd = calculate_sd(data))\n}\n\nBreaking down long functions into smaller ones improves readability and reusability.\n\n\n3. Replacing Hardcoded Values with Constants\nBefore:\n\nif (length(data) &gt; 1000) {\n  print(\"Large dataset\")\n}\n\nAfter:\n\nTHRESHOLD &lt;- 1000\nif (length(data) &gt; THRESHOLD) {\n  print(\"Large dataset\")\n}\n\nUsing constants improves clarity and ease of updating values.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Introduction to R - Refactoring the Workshop” (n.d.)\n\n“Programming with R: Best Practices for Writing R Code” (n.d.)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Best practices in programming</span>"
    ]
  },
  {
    "objectID": "session-r-programming.html#extra-testing-and-validation",
    "href": "session-r-programming.html#extra-testing-and-validation",
    "title": "4  Best practices in programming",
    "section": "4.4 EXTRA: Testing and validation",
    "text": "4.4 EXTRA: Testing and validation\n\n4.4.1 Writing Unit Tests\nUnit tests are essential for ensuring code correctness by verifying that each function behaves as expected. They help catch errors early and make code more robust by facilitating easy modifications and refactoring.\n\nExample: Writing basic unit tests in R (testthat)\n\nInstall testthat package:\n\ninstall.packages(\"testthat\")\n\nCreate a test script (test_calculate_mean.R):\n\nlibrary(testthat)\n\ncalculate_mean &lt;- function(x) {\n  if (!is.numeric(x)) stop(\"Input must be numeric\")\n  mean(x, na.rm = TRUE)\n}\n\ntest_that(\"calculate_mean works correctly\", {\n  expect_equal(calculate_mean(c(1, 2, 3)), 2)\n  expect_equal(calculate_mean(c(NA, 2, 3)), 2.5)\n  expect_error(calculate_mean(\"string\"))\n})\n\nRun the tests:\n\ntest_file(\"test_calculate_mean.R\")\n\n\n\n\n\n4.4.2 Data Validation\nValidating data inputs and outputs is crucial for maintaining data integrity, especially in data processing pipelines. It ensures that the data conforms to expected formats and values, avoiding downstream errors.\n\nExample: Implementing data validation checks in data processing scripts\nGiven a dataset:\n\ndata &lt;- data.frame(\n  age = c(25, -5, 30),\n  salary = c(50000, 0, 60000),\n  name = c(\"Alice\", NA, \"Bob\")\n)\n\nCheck if each variable comply with certain rules:\n* age is non-negative,\n* salary is positive,\n* name is not missing.\n\nstopifnot(data$age &gt;= 0, data$salary &gt; 0, !is.na(data$name))\n\n# or with custom error messages: \nstopifnot(data$age &gt;= 0, \"Age values must be non-negative\")\nstopifnot(data$salary &gt; 0, \"Salary values must be positive\")\nstopifnot(!is.na(data$name), \"Names must not be NA\")\n\nFor handling larger rule sets and getting a more structured feedback, consider using validate package:\n\nlibrary(validate)\n\nrules &lt;- validator(\nage &gt;= 0,\nsalary &gt; 0,\n!is.na(name)\n)\n\n# Validate the data\ncheck &lt;- confront(data, rules)\nsummary(check)\n\n  name items passes fails nNA error warning        expression\n1   V1     3      2     1   0 FALSE   FALSE age - 0 &gt;= -1e-08\n2   V2     3      2     1   0 FALSE   FALSE        salary &gt; 0\n3   V3     3      2     1   0 FALSE   FALSE      !is.na(name)\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Prerequisites” (n.d.)\n\n“Best Practices for Durable R Code” (n.d.)\n\n“The Validation Set Approach in R Programming - GeeksforGeeks” (n.d.)\n\nKennedy (2019)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Best practices in programming</span>"
    ]
  },
  {
    "objectID": "session-r-programming.html#extra-error-handling-and-debugging",
    "href": "session-r-programming.html#extra-error-handling-and-debugging",
    "title": "4  Best practices in programming",
    "section": "4.5 EXTRA: Error handling and debugging",
    "text": "4.5 EXTRA: Error handling and debugging\nError handling and debugging are essential for building robust R code that gracefully manages unexpected issues. Effective error handling prevents crashes and improves user feedback, while debugging tools help identify and resolve problems during code development.\n\n4.5.1 Error Handling Techniques\nUsing tryCatch in R\ntryCatch is a versatile function for handling errors, warnings, and messages in R. It allows you to specify actions for different types of errors, ensuring that code can handle unexpected conditions gracefully.\nWriting Meaningful Error Messages\nMeaningful error messages help both developers and users understand what went wrong and guide them toward solutions. Avoid generic messages like \"Error occurred\" and instead specify the exact problem.\nExample: Implementing error handling in a data processing script\n\nprocess_data &lt;- function(data) {\n  tryCatch({\n    # check\n    if (!\"score\" %in% names(data)) stop(\"Missing 'score' column in data\")\n    # filter\n    data &lt;- data[data$score &gt; 10, ]  # Process data\n    return(data)\n  },\n  error = function(e) {\n    cat(\"Error in processing data:\", e$message, \"\\n\")\n    return(NULL)\n  })\n}\n\n# Usage\nresult &lt;- process_data(iris)\n\nError in processing data: Missing 'score' column in data \n\n\nIn this example, tryCatch captures errors from read.csv() or the missing column and provides a custom message.\n\n\n4.5.2 Debugging tools\nIntroduction to Debugging Tools: browser in R\nThe browser function in R allows you to pause code execution at a specific point, enabling step-by-step inspection of variables and function behaviour. This is especially useful for identifying unexpected values or conditions.\nExample: Walkthrough of a debugging session in R\n\ncalculate_mean &lt;- function(data) {\n  browser()  # Debugging breakpoint\n  mean(data$score, na.rm = TRUE)\n}\n\n# Sample data\ndata &lt;- data.frame(score = c(10, NA, 20, 15))\ncalculate_mean(data)\n\nWhen browser() is called, R will pause execution and open an interactive environment. Here, you can inspect data$score, check conditions, and proceed line by line, allowing you to diagnose issues effectively.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Exceptions and Debugging · Advanced R.” (n.d.)\n\n“22 Debugging  Advanced R” (n.d.)\n\n“License (GPL-3)” (n.d.)\n\n“4  Debugging Pipelines – The {Targets} R Package User Manual” (n.d.)\n\nAlbert Rapp (2024)",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Best practices in programming</span>"
    ]
  },
  {
    "objectID": "session-r-programming.html#extra-creating-and-releasing-packages",
    "href": "session-r-programming.html#extra-creating-and-releasing-packages",
    "title": "4  Best practices in programming",
    "section": "4.6 EXTRA: Creating and releasing packages",
    "text": "4.6 EXTRA: Creating and releasing packages\nReusable and shareable code helps in building efficient workflows, saving time, and reducing redundancy across projects. Reusable code can be encapsulated in functions, libraries, or R packages to simplify future use and sharing.\n\n4.6.1 Creating packages\nWriting functions and libraries that can be reused in multiple projects helps standardize tasks and reduces the risk of introducing errors by duplicating code. R packages are an ideal way to organize reusable functions, data, and documentation, making it easy to use them across different projects.\nExample: Creating a Simple R Package 1. Set up the package\nUse the usethis package to initiate a new package:\n::: {.cell}\ninstall.packages(\"usethis\")\nusethis::create_package(\"path/to/myPackage\")\n:::\n\nAdd a function\nCreate a function file in the R/ folder, for example R/hello.R:\n\nhello &lt;- function(name = \"world\") {\n  paste(\"Hello,\", name)\n}\n\nDocument the function\nUse roxygen2 to add documentation:\n\n#' Say Hello\n#'\n#' @param name A character string. Default is \"world\".\n#' @return A greeting message.\n#' @export\nhello &lt;- function(name = \"world\") {\n  paste(\"Hello,\", name)\n}\n\nBuild and test\nLoad and test your package:\n\ndevtools::document()\ndevtools::load_all()\nhello(\"R user\")\n\n\n\n\n4.6.2 Releasing/sharing packages\nOnce your code is reusable as package, share it to help others benefit from your work. Publishing packages on CRAN or GitHub makes them accessible to other users and developers, while sharing R Markdown notebooks helps others reproduce and understand your analyses.\nGitHub: Initialize a Git repository in the package directory or a new project in RStudio as a R package, set up a GitHub connection, and add the package files. Push changes to GitHub and release with a new tag.\nCRAN: Follow CRAN submission guidelines.\nSharing packages or code through these platforms allows the broader R community to contribute, use, and improve your work.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\n“Making Your First R Package” (n.d.)\n\n (n.d.)\n\n“Writing R Packages in Rstudio” (n.d.)\n\nStatistikinDD (2021)\n\n\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nRefactoring Code (30min)\n\nFirst attempt: Refactoring a sample script to follow best practices (clean code, modularity, documentation).\n\nSecond attempt: try using a Large Language Model (LLM) to refactor.\n\n\nCollaborative Exercise (40min)\n\nSimulating a collaborative workflow with Git: making and reviewing pull requests.\n\nGroups of two or three\n\nRe-use one of the repositories created in GitHub (Session 2) and populated by R code and output files (Session 3).\n\nMutual reviews and change suggestions.\n\nDiscussion, accepting/rejecting changes, and merge decision\n\n\nOpen discussion (10min)\n\nAddressing common challenges in applying best practices to real-world projects.\n\n\n\n\n\n\n\n\nn.d. Accessed November 12, 2024. https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf.\n\n\n“22 Debugging  Advanced R.” n.d. Accessed November 12, 2024. https://adv-r.hadley.nz/debugging.html.\n\n\n“4  Debugging Pipelines – The {Targets} R Package User Manual.” n.d. Accessed November 12, 2024. https://books.ropensci.org/targets/debugging.html.\n\n\nAlbert Rapp. 2024. “Stop Code Errors From Crashing Your Whole R Script.” https://www.youtube.com/watch?v=4Az6pyWrUG4.\n\n\nAlboukadel. 2020. “R Coding Style Best Practices.” Datanovia. https://www.datanovia.com/en/blog/r-coding-style-best-practices/.\n\n\nAnderson, and Brooke, Sean Kross. n.d. 1.9 Working with Large Datasets  Mastering Software Development in R. Accessed November 5, 2024. https://github.com/rdpeng/RProgDA.\n\n\n“Best Practices for Durable R Code.” n.d. Accessed November 5, 2024. https://www.appsilon.com/post/best-practices-for-durable-r-code.\n\n\n“Exceptions and Debugging · Advanced R.” n.d. Accessed November 12, 2024. http://adv-r.had.co.nz/Exceptions-Debugging.html.\n\n\nGillespie, Colin, and Robin Lovelace. n.d. Efficient R Programming. Accessed November 5, 2024. https://csgillespie.github.io/efficientR/.\n\n\n“Google’s R Style Guide.” n.d. Accessed November 5, 2024. https://web.stanford.edu/class/cs109l/unrestricted/resources/google-style.html.\n\n\nGrolemund, Hadley Wickham and Garrett. n.d. Welcome  R for Data Science. Accessed November 5, 2024. https://r4ds.had.co.nz/.\n\n\n“Introduction to R - Refactoring the Workshop.” n.d. HackMD. Accessed November 5, 2024. https://hackmd.io/@libjohn/Bkpg0zp3I.\n\n\nKennedy, Hannah. 2019. “Best Practices for Unit Testing and Linting with R.” Medium. https://medium.com/@mindlessroman/best-practices-for-unit-testing-and-linting-with-r-6d40be95391c.\n\n\n“Learn More.” n.d. Accessed November 5, 2024. https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html.\n\n\n“License (GPL-3).” n.d. Accessed November 12, 2024. https://cran.r-project.org/web/packages/tryCatchLog/vignettes/tryCatchLog-intro.html.\n\n\nLipkin, Gus. 2022. “Writing Faster R with Vectorization and the {Apply} Family.” Medium. https://guslipkin.medium.com/writing-faster-r-with-vectorization-and-the-apply-family-ff6078a2583a.\n\n\n“Making Your First R Package.” n.d. Accessed November 12, 2024. https://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html.\n\n\n“Prerequisites.” n.d. Accessed November 5, 2024. https://cran.r-project.org/web/packages/validate/vignettes/cookbook.html.\n\n\n“Programming with R: Best Practices for Writing R Code.” n.d. Accessed November 5, 2024. https://swcarpentry.github.io/r-novice-inflammation/06-best-practices-R.html.\n\n\n“R Dtplyr: How to Efficiently Process Huge Datasets with a Data.table Backend.” n.d. Accessed November 5, 2024. https://www.appsilon.com/post/r-dtplyr.\n\n\n“R Variables and Constants.” n.d. Accessed November 5, 2024. https://www.programiz.com/r/variables-and-constants.\n\n\n“R Variables and Constants (With Examples).” n.d. Accessed November 5, 2024. https://www.datamentor.io/r-programming/variable-constant.\n\n\n“Single-Responsibility Principle - Wikipedia.” n.d. Accessed November 5, 2024. https://en.wikipedia.org/wiki/Single-responsibility_principle.\n\n\nStatistikinDD. 2021. “Creating Your First R Package in 2 Minutes in RStudio!” https://www.youtube.com/watch?v=47PN2VG3RmI.\n\n\n“Style Guide · Advanced R.” n.d. Accessed November 5, 2024. http://adv-r.had.co.nz/Style.html.\n\n\n“The Validation Set Approach in R Programming - GeeksforGeeks.” n.d. Accessed November 5, 2024. https://www.geeksforgeeks.org/the-validation-set-approach-in-r-programming/.\n\n\n“Tidyverse Style Guide.” n.d. Accessed November 5, 2024. https://style.tidyverse.org/index.html.\n\n\n“Welcome  Advanced R.” n.d. Accessed November 5, 2024. https://adv-r.hadley.nz/.\n\n\n“Writing R Packages in Rstudio.” n.d. Accessed November 12, 2024. https://ourcodingclub.github.io/tutorials/writing-r-package/.",
    "crumbs": [
      "Programming in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Best practices in programming</span>"
    ]
  },
  {
    "objectID": "session-data-science-workflow.html",
    "href": "session-data-science-workflow.html",
    "title": "5  Data Science Workflow",
    "section": "",
    "text": "5.1 Introduction to Data Science Workflow\nIn a typical data science workflow, we move through five primary stages: data collection, data cleaning, exploration, modeling, and communication. These steps collectively transform raw data into actionable insights driven by research questions while also enabling efficient and reproducible data science practices.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Workflow</span>"
    ]
  },
  {
    "objectID": "session-data-science-workflow.html#introduction-to-data-science-workflow",
    "href": "session-data-science-workflow.html#introduction-to-data-science-workflow",
    "title": "5  Data Science Workflow",
    "section": "",
    "text": "Data Collection: This is the first step, where data is gathered from various sources, such as databases, APIs, or web scraping. In R, packages like readr and httr facilitate efficient data import from structured files (e.g., CSV, Excel) and web sources.\nData Cleaning: Data cleaning involves preparing raw data for analysis, handling missing values, correcting data types, and dealing with outliers. Tools like dplyr and tidyr are often used in R to perform these operations, enabling tasks like removing duplicates, imputing missing data, and restructuring data into a “tidy” format suitable for analysis.\nData Exploration: Exploratory Data Analysis (EDA) is the phase where we examine the data’s characteristics, uncover patterns, and form hypotheses. Visualizations (using pairs() or a wide range of plot types available in Base R and ggplot2) and summary statistics (summary(), skimr) help in understanding the data distribution, relationships between variables, and identifying any anomalies.\nModeling: At this stage, statistical or machine learning models are developed to predict or explain outcomes. In R, packages like caret and tidymodels streamline the modelling process, from splitting data and selecting models to tuning hyperparameters and evaluating performance.\nCommunication: The final stage focuses on presenting findings clearly, often through reports, dashboards, or interactive applications. Using R Markdown for reports or Shiny for interactive applications enables data scientists to effectively communicate insights to stakeholders.\n\n\nQuick example in R: hypothetical local dataset “house_prices.csv”\nLet’s consider a simple example of a data science project where we predict house prices based on available features:\n\nData Collection: Load a dataset such as house_prices.csv using read_csv().\nData Cleaning: Use dplyr to handle missing values (e.g., mutate_if(is.na, median) to replace with median values) and convert categorical variables to factors.\nData Exploration: Visualize relationships between features like square footage and price using ggplot(data = houses) + geom_point(aes(x = sqft, y = price)).\nModeling: Create a linear model with lm(price ~ sqft + num_bedrooms, data = houses).\nCommunication: Report results in an R Markdown document, showing model coefficients, predictions, and visualizations to explain findings.\n\n\n\nQuick example in R: canonical dataset USArrests\nLet’s consider a simple example of a data science project. Suppose we aim to determine main factors in criminality using the canonical dataset USArrests.\n\nData Collection: Load the dataset directly from R’s built-in datasets.\n\n\ndata(\"USArrests\")\n\n\nData Cleaning: Check for any missing values or potential outliers that might represent data entry errors.\n\n\n# Adding a missing value at row 1, column 1\nUSArrests[1, 1] &lt;- NA\n\n# Check for missing values\nany(is.na(USArrests))\n\n[1] TRUE\n\n# remove missing value\nUSArrests_clean &lt;- na.omit(USArrests)\n\n\nData Exploration: Visualize and summarize data to understand patterns. For instance, we might be interested in the value distribution of murder arrests.\n\n\n# Quick summary of the data distribution\nsummary(USArrests_clean)\n\n     Murder          Assault         UrbanPop          Rape      \n Min.   : 0.800   Min.   : 45.0   Min.   :32.00   Min.   : 7.30  \n 1st Qu.: 4.000   1st Qu.:109.0   1st Qu.:54.00   1st Qu.:14.90  \n Median : 7.200   Median :159.0   Median :66.00   Median :20.00  \n Mean   : 7.678   Mean   :169.4   Mean   :65.69   Mean   :21.23  \n 3rd Qu.:11.100   3rd Qu.:249.0   3rd Qu.:78.00   3rd Qu.:26.20  \n Max.   :17.400   Max.   :337.0   Max.   :91.00   Max.   :46.00  \n\n# Visualize the distribution of artefact \"types\" (represented by variables like 'Murder' here)\nlayout(matrix(1:2, nrow = 2), heights = c(2, 1.5))\npar(mar = c(0, 4, 4, 2) + 0.1)\nhist(USArrests_clean$Murder, main=\"Distribution of counts of murder arrests in 'USArrests'\", xaxt = \"n\")\npar(mar = c(2, 4, 0, 2) + 0.1)\nboxplot(USArrests_clean$Murder, horizontal = TRUE)\n\n\n\n\n\n\n\n\nThis combined histogram and box plot would give a quick view of the central tendencies and spread of murder arrests across the dataset. For example, we can already observe that the mean and median of the distribution (7.677551 and 7.2, respectively) are on the lower half of the range (0.8, 17.4), which also imply a longer right tail of the distribution (i.e., the difference between maximum and mean is greater than the one between mean and minimum).\n\nModelling: Create a simple model to analyse relationships between variables. For instance, let’s assume we’re exploring how UrbanPop (population percentage in urban areas) might relate to Murder.\n\n\nmodel &lt;- lm(Murder ~ UrbanPop, data = USArrests_clean)\nsummary(model)\n\n\nCall:\nlm(formula = Murder ~ UrbanPop, data = USArrests_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3323 -3.7628 -0.7344  2.9121  9.8656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  6.02650    2.90210   2.077   0.0433 *\nUrbanPop     0.02513    0.04315   0.582   0.5630  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.359 on 47 degrees of freedom\nMultiple R-squared:  0.007167,  Adjusted R-squared:  -0.01396 \nF-statistic: 0.3393 on 1 and 47 DF,  p-value: 0.563\n\n\nThe results of a simple linear regression analysis provides a basic understanding of how one variable might be related on another, interpreted also as dependent, presumably causal relationship. In this case, there is no statistically significant evidence that urban population percentage is a relevant factor in murder arrests in our dataset.\n\nCommunication: Present findings in an R Markdown document, incorporating both visualizations and model summaries. You can interpret results to suggest whether murder arrests are more common in highly populated regions.\n\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nHazzan and Mike (2023)\n\ntjohannsen (2023)\n\nSaltz (2020)",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Workflow</span>"
    ]
  },
  {
    "objectID": "session-data-science-workflow.html#data-science-workflow-in-r-data-import-and-preparation",
    "href": "session-data-science-workflow.html#data-science-workflow-in-r-data-import-and-preparation",
    "title": "5  Data Science Workflow",
    "section": "5.2 Data Science Workflow in R: Data Import and Preparation",
    "text": "5.2 Data Science Workflow in R: Data Import and Preparation\n\n5.2.1 Importing Data\nA critical step in any data science workflow is importing data from external sources. R provides robust tools for importing data from a variety of formats:\n\nCSV files: Use the read.csv() function from base R.\n\nExcel files: Leverage the readxl package with read_excel().\n\nDatabases: Employ the DBI package to connect to relational databases and fetch data using SQL queries.\n\n\nExamples using base R, readxl and DBI+RSQLite\n\n# Load necessary libraries\nlibrary(readxl)\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Read a CSV file\ncsv_data &lt;- read.csv(\"data.csv\")\n\n# Read an Excel file\nexcel_data &lt;- readxl::read_excel(\"data.xlsx\")\n\n# Connect to a SQLite database and fetch data\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \"data.db\")\ndb_data &lt;- DBI::dbGetQuery(con, \"SELECT * FROM table_name\")\nDBI::dbDisconnect(con)\n\n\n\n\n5.2.2 Data Cleaning\nBefore analysis, raw data often requires cleaning to address issues like missing values, duplicates, and inconsistencies.\n\nExample using Base R\n\nRemoving Missing Values\nUse na.omit() to remove rows with missing values or is.na() to identify them.\n\n\n# Example data\ndata &lt;- data.frame(A = c(1, 2, NA, 4), B = c(\"x\", NA, \"y\", \"z\"))\nprint(data)\n\n   A    B\n1  1    x\n2  2 &lt;NA&gt;\n3 NA    y\n4  4    z\n\n# Remove rows with NA\nclean_data &lt;- na.omit(data)\nprint(clean_data)\n\n  A B\n1 1 x\n4 4 z\n\n\n\nHandling Duplicates\nUse duplicated() to identify duplicate rows or unique() to retain only unique rows.\n\n\ndata &lt;- data.frame(A = c(1, 2, 2, 4), B = c(\"x\", \"y\", \"y\", \"z\"))\nprint(data)\n\n  A B\n1 1 x\n2 2 y\n3 2 y\n4 4 z\n\n# Remove duplicate rows\ndata_unique &lt;- data[!duplicated(data), ]\nprint(data_unique)\n\n  A B\n1 1 x\n2 2 y\n4 4 z\n\n\n\nReplacing Values\nReplace specific values with ifelse() or direct indexing.\n\n\ndata &lt;- data.frame(A = c(1, 2, 999, 4), B = c(\"x\", \"y\", \"z\", \"999\"))\nprint(data)\n\n    A   B\n1   1   x\n2   2   y\n3 999   z\n4   4 999\n\n# Replace 999 with NA\ndata[data == 999] &lt;- NA\nprint(data)\n\n   A    B\n1  1    x\n2  2    y\n3 NA    z\n4  4 &lt;NA&gt;\n\n\n\n\nExample using tidyverse\nKey functions include:\n- tidyr: Tools like fill() (fill missing values) and drop_na() (remove rows with NAs).\n- dplyr: Functions like distinct() to remove duplicates and mutate() to fix inconsistencies.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\n# Example dataset\ndata &lt;- data.frame(id = c(1, 2, 2, 3, NA), value = c(NA, \"A\", \"A\", \"B\", \"C\"))\nprint(data)\n\n  id value\n1  1  &lt;NA&gt;\n2  2     A\n3  2     A\n4  3     B\n5 NA     C\n\n# Clean the data\ncleaned_data &lt;- data %&gt;%\n  drop_na(id) %&gt;% # Remove rows with missing IDs\n  distinct() %&gt;%  # Remove duplicates\n  fill(value, .direction = \"down\") # Fill missing values downward\nprint(cleaned_data)\n\n  id value\n1  1  &lt;NA&gt;\n2  2     A\n3  3     B\n\n\n\n\n\n5.2.3 Data Transformation\nTransforming data is essential for reshaping and preparing it for analysis.\n\nExample using Base R\nBase R provides versatile and efficient tools for cleaning and transforming data.\nA few example operations common in data science workflows are:\n\nFiltering Rows\nSubset data using logical conditions.\n\n\ndata &lt;- data.frame(A = 1:5, B = letters[1:5])\nprint(data)\n\n  A B\n1 1 a\n2 2 b\n3 3 c\n4 4 d\n5 5 e\n\n# Filter rows where A &gt; 3\nfiltered_data &lt;- data[data$A &gt; 3, ]\nprint(filtered_data)\n\n  A B\n4 4 d\n5 5 e\n\n\n\nSelecting Columns\nUse indexing to select specific columns.\n\n\n# Select column A\nselected_columns &lt;- data[, \"A\", drop = FALSE]\nprint(selected_columns)\n\n  A\n1 1\n2 2\n3 3\n4 4\n5 5\n\n\nNOTE: the argument drop = FALSE ensures that the original data frame structure is not lost in the process (run data[, \"A\"] to compare).\n\nAdding or Modifying Columns\nUse the $ operator or indexing to create or modify columns.\n\n\ndata$new_col &lt;- data$A * 2\nprint(data)\n\n  A B new_col\n1 1 a       2\n2 2 b       4\n3 3 c       6\n4 4 d       8\n5 5 e      10\n\n\n\nReshaping Data\nUse reshape() to pivot data between wide and long formats. The wide format is where each variable corresponds to a column (table format) while the long format assign variable-value pairs to different rows, retaining one or more variables in the wide format.\n\n\n# Example wide format\ndata &lt;- data.frame(id = 1:2, Q1 = c(10, 20), Q2 = c(30, 40))\n\n# Convert to long format\nlong_data &lt;- reshape(data, direction = \"long\", varying = list(c(\"Q1\", \"Q2\")), v.names = \"value\", timevar = \"quarter\")\nprint(long_data)\n\n    id quarter value\n1.1  1       1    10\n2.1  2       1    20\n1.2  1       2    30\n2.2  2       2    40\n\n\n\n\nExample using tidyverse\nUse dplyr for:\n* Filtering and selecting rows/columns: filter(), select().\n* Creating new variables: mutate().\n* Reshaping: pivot_longer() and pivot_wider().\n\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Example dataset\ndata &lt;- data.frame(\n  id = 1:3,\n  Q1 = c(10, 20, 30),\n  Q2 = c(15, 25, 35)\n)\nprint(data)\n\n  id Q1 Q2\n1  1 10 15\n2  2 20 25\n3  3 30 35\n\n# Perform all operations sequentially in a single step \ntransformed_data &lt;- data %&gt;%\n  pivot_longer(cols = starts_with(\"Q\"), names_to = \"quarter\", values_to = \"value\") %&gt;%\n  filter(value &gt; 15) %&gt;%  # Filter rows where value &gt; 15\n  mutate(value_scaled = value / max(value))  # Add a new scaled column\nprint(transformed_data)\n\n# A tibble: 4 × 4\n     id quarter value value_scaled\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1     2 Q1         20        0.571\n2     2 Q2         25        0.714\n3     3 Q1         30        0.857\n4     3 Q2         35        1    \n\n\nBy mastering these data preparation steps, you ensure a clean and well-structured dataset, setting the stage for effective analysis and visualization.\n\n\n\n\n\n\nSee also\n\n\n\n\n\n\nKnoxville (n.d.)\n\nRapp (2024)\n\nKopra et al. (2024)",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Workflow</span>"
    ]
  },
  {
    "objectID": "session-data-science-workflow.html#exploratory-data-analysis",
    "href": "session-data-science-workflow.html#exploratory-data-analysis",
    "title": "5  Data Science Workflow",
    "section": "5.3 Exploratory Data Analysis",
    "text": "5.3 Exploratory Data Analysis\n\n5.3.1 Univariate Statistics\n\nNumeric variables\nThis section equips you to explore univariate distributions of numeric variables, uncovering insights from centrality to variability with both statistical and visual techniques.\n\nHistograms: Exploring a single variable involves visualizing its distribution to identify patterns such as central tendency, spread, and outliers. Histograms are one of the most effective tools for this.\n\n\n\nExample: Using ggplot2 for histograms\n\nlibrary(ggplot2)\n\n# Example dataset - variable with normal distribution\ndata &lt;- data.frame(value = rnorm(1000, mean = 50, sd = 10))\n\n# Create a histogram\nggplot(data, aes(x = value)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Values\", x = \"Value\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nRange: The range of a variable provides a simple measure of the spread of the data. It is calculated as the difference between the maximum and minimum values. Base R already has a function named range() to handle this calculation:\n\n\ncat(\"Min:\", min(data$value), \"Max:\", max(data$value))\n\nMin: 18.25599 Max: 82.13318\n\nrange_val &lt;- range(data$value)\n\ncat(\"Range (Max - Min):\", range_val)\n\nRange (Max - Min): 18.25599 82.13318\n\n\n\nCentral tendency measures: the mean, median, and mode describe the centre of the distribution.\n\nDispersion measures: variance and standard deviation describe the spread of the data.\n\n\nmean_val &lt;- mean(data$value)\nmedian_val &lt;- median(data$value)\nvariance &lt;- var(data$value)\nstd_dev &lt;- sd(data$value)\n\n# Print results\ncat(\"Mean:\", mean_val, \"Median:\", median_val, \"Variance:\", variance, \"SD:\", std_dev)\n\nMean: 49.47998 Median: 49.14122 Variance: 94.70789 SD: 9.731798\n\n\n\n\nExample: Plotting a Histogram and Marking the Mean\nOverlay the mean on a histogram to visualize its position relative to the distribution.\n\nggplot(data, aes(x = value)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"black\") +\n  geom_vline(aes(xintercept = mean_val), color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Histogram with Mean Marked\", x = \"Value\", y = \"Frequency\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nBox plots: A box plot of a single variable can be useful to visualise central tendency and dispersion measures at the same time. It offers the same information of an histogram, though in a more analytical form, by default plotting the median (strong line), the 1st and 3rd or interquartile range (box), the (pseudo)range or 1.5-interquartile range (lines), and outliers (points outside the 1.5x interquartile range).\n\n\n\nExample: Using ggplot2 for an univariate box plot\n\nlibrary(ggplot2)\n\n# Example dataset - variable with normal distribution\ndata &lt;- data.frame(value = rnorm(1000, mean = 50, sd = 20))\n\n# Create a histogram\nggplot(data, aes(x = value)) +\n  geom_boxplot(fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Values\", x = \"Value\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\nNon-numeric variables\nThese approaches allow for comprehensive exploration of univariate statistics for categorical variables, emphasizing both numeric summaries and visual insights.\nUnivariate statistics for non-numeric (categorical) variables focus on summarizing and visualizing the distribution of categories. Here’s a breakdown with examples:\n\nFrequency Tables: A frequency table lists the counts of each category, helping to understand the distribution.\n\n\n# Example Dataset\ndata &lt;- data.frame(category = sample(c(\"A\", \"B\", \"C\"), size = 100, replace = TRUE))\n\n# Frequency Table\ntable(data$category)\n\n\n A  B  C \n38 36 26 \n\n\n\nBar Plots: Bar plots visually represent the frequency distribution of categories.\n\n\nlibrary(ggplot2)\n\n# Bar Plot\nggplot(data, aes(x = category)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Category Distribution\", x = \"Category\", y = \"Count\")\n\n\n\n\n\n\n\n\n\nProportion Visualization: Proportions provide relative frequencies, useful for comparing categorical data.\n\n\n# Proportion Table\nprop_table &lt;- prop.table(table(data$category))\n\n# Pie Chart\nggplot(data, aes(x = \"\", fill = category)) +\n  geom_bar(width = 1) +\n  coord_polar(\"y\") +\n  labs(title = \"Category Proportions\")\n\n\n\n\n\n\n\n\n\nMode: The mode is the most frequently occurring category.\n\n\n# Mode Calculation\nmode_category &lt;- names(which.max(table(data$category)))\ncat(\"Mode:\", mode_category)\n\nMode: A\n\n\n\n\n\n5.3.2 Bivariate statistics\nLoading DartPoints dataset from archdata:\n\nlibrary(archdata)\ndata(DartPoints)\n\n\nScatter Plots: Visualize relationships between two numerical variables.\n\n\nggplot(DartPoints, aes(x = H.Length, y = Weight)) +\n   geom_point() +\n   labs(x = \"Haft element length (mm)\", y = \"Weight (gm)\")\n\n\n\n\n\n\n\n\n\nBox Plots: Compare a numerical variable across categories of a categorical variable. Ideal for comparing central tendency and dispersal measurements between groups or categories.\n\n\nggplot(DartPoints, aes(x = Haft.Sh, y = H.Length)) +\n   geom_boxplot() +\n   scale_x_discrete(labels = c(\"Angular\", \"Excurvate\", \"Incurvate\", \"Recurvate\", \"Straight\")) +\n   labs(x = \"Shape lateral haft element\", y = \"Haft element length (mm)\")\n\n\n\n\n\n\n\n\n\nBar Plots with two variables (stacked): Compare counts or proportions of categorical variables.\n\n\nggplot(DartPoints, aes(x = Haft.Sh, fill = Should.Sh)) +\n   geom_bar() +\n   scale_x_discrete(labels = c(\"Angular\", \"Excurvate\", \"Incurvate\", \"Recurvate\", \"Straight\")) +\n   scale_fill_discrete(labels = c(\"Excurvate\", \"Incurvate\", \"Straight\", \"None\")) +\n   labs(x = \"Shape lateral haft element\", y = \"Count\", fill = \"Shoulder shape\")\n\n\n\n\n\n\n\n\n\nContingency Tables: Quick assessment of the distribution of counts among two categorical variables.\n\n\ntable(DartPoints$Haft.Sh, DartPoints$Should.Sh)\n\n   \n     E  I  S  X\n  A  0  2  0  0\n  E  0  3  6  0\n  I  0 16  4  2\n  R  0  0  1  0\n  S  3 16 35  1\n\n\n\nMosaic Plots: A hybrid between contingency tables and stacked bar plots, mosaic plots are useful especially when counts in cross-category groups (i.e. cells in contingency table) are greater than 0. A mosaic plot represents the conditional relative frequency for a cell in the contingency table as the area of rectangular tiles. Adding a shaded color, it can also be used to visualise the deviation from the expected frequency (residual) from a Pearson Chi-square or Likelihood Ratio G^2 test.\n\n\nmosaicplot(table(DartPoints$Haft.Sh, DartPoints$Should.Sh),\n           xlab = \"Shape lateral haft element\",\n           ylab = \"Shoulder shape\",\n           main = \"\",\n           shade = TRUE)\n\n\n\n\n\n\n\n\n\nCorrelation: Measure the direction and strength of linear relationships between numerical variables. In base R, the function cor() will return the Pearson correlation coefficient by default.\n\n\ncor(DartPoints$H.Length, DartPoints$Weight)\n\n[1] 0.486397\n\n\n\nSimple linear Regression: Calculate the parameters (intercept, slope) for a linear model with the minimum distance towards data points in two numerical variables. Geometrically, such model is equivalent to a line in a two-dimensional plane.\n\nWith Base R:\n\nmodel &lt;- lm(Weight ~ H.Length, data = DartPoints,)\nsummary(model)\n\n\nCall:\nlm(formula = Weight ~ H.Length, data = DartPoints)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9356 -2.3685 -0.7377  2.1173 19.6316 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.80129    1.35913   0.590    0.557    \nH.Length     0.51019    0.09715   5.252 1.02e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.696 on 89 degrees of freedom\nMultiple R-squared:  0.2366,    Adjusted R-squared:  0.228 \nF-statistic: 27.58 on 1 and 89 DF,  p-value: 1.018e-06\n\nplot(DartPoints$H.Length, DartPoints$Weight,\n     xlab = \"Haft element length (mm)\", \n     ylab = \"Weight (gm)\")\nabline(model, col = \"red\", lwd = 5)\n# or\nabline(a = model$coefficients[\"(Intercept)\"], \n       b = model$coefficients[\"H.Length\"],\n       col = \"blue\", lty = 3, lwd = 5)\n\n\n\n\n\n\n\n\nWith ggplot2, a linear model can be added directly to a plot with geom_smooth(method = \"lm\"):\n\nggplot(DartPoints, aes(x = H.Length, y = Weight)) +\n   geom_point() +\n   geom_smooth(method = \"lm\", color = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe function geom_smooth() will add by default a shaded area around the line, representing the confidence interval (see argument se and level in ?geom_smooth()).\n\nVisualizing multiple correlation pairs\n\nQuick visualisation of a correlation matrix using cor()\n\ncor(DartPoints[, c(\"Length\", \"Width\", \"Thickness\")])\n\n             Length     Width Thickness\nLength    1.0000000 0.7689932 0.5890989\nWidth     0.7689932 1.0000000 0.5459291\nThickness 0.5890989 0.5459291 1.0000000\n\n\nBuild a larger correlation matrix (only numerical variables and excluding cases with missing values) and plot it using corrplot() from the corrplot package:\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nselected_variables &lt;- c(\"Length\", \"Width\", \"Thickness\", \"B.Width\", \"J.Width\", \"H.Length\", \"Weight\")\n\ncorr_matrix &lt;- cor(DartPoints[, selected_variables],\n                   use = \"complete.obs\")\n\ncorrplot(corr_matrix, method = \"circle\")\n\n\n\n\n\n\n\n\n\nHypothesis Testing:\n\nt-Test: Compare means of numerical variables across two categories. Conventionally, P-value &lt; 0.05 means that the null hypothesis (there are no differences between the means of these variables) is very unlikely.\n\n# consider only cases in blade shape categories \"E\" (excurvate) and \"S\" (straight)\nDartPoints_IandS &lt;- subset(DartPoints, Blade.Sh == \"E\" | Blade.Sh == \"S\")\n\n# apply test for Weight between the blade shape two categories\nt.test(Weight ~ Blade.Sh, data = DartPoints_IandS)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Blade.Sh\nt = 1.6009, df = 72.799, p-value = 0.1137\nalternative hypothesis: true difference in means between group E and group S is not equal to 0\n95 percent confidence interval:\n -0.3652556  3.3471603\nsample estimates:\nmean in group E mean in group S \n       8.530952        7.040000 \n\n\nIn this case, the evidence is insufficient for demonstrating that there is a consistent difference in weight between dart points with excurvate and straight blades.\nChi-Square Test: Test independence between categorical variables. Conventionally, P-value &lt; 0.05 means that the null hypothesis (the variables are independent) is very unlikely.\n\nchisq.test(table(DartPoints$Haft.Sh, DartPoints$Haft.Or))\n\nWarning in chisq.test(table(DartPoints$Haft.Sh, DartPoints$Haft.Or)):\nChi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DartPoints$Haft.Sh, DartPoints$Haft.Or)\nX-squared = 101.85, df = 16, p-value = 1.556e-14\n\n\nIn this case, the evidence supports, with 95% confidence, that haft shape and orientation are not independent.\n\nQuasi-multivariate approaches:\n\nVisualise multiple subsets of a bivariate relationship by splitting plots by a categorical variable.\n“Faceting” scatter plots with ggplot2:\n\nggplot(DartPoints, aes(x = Thickness, y = Weight)) +\n   geom_point() +\n   facet_wrap(~ Blade.Sh)\n\n\n\n\n\n\n\n\nVisualise multiple pairwise bivariate relationships with pairs() (only numeric variables):\n\nselected_variables &lt;- c(\"Length\", \"Width\", \"Thickness\", \"B.Width\", \"J.Width\", \"H.Length\", \"Weight\")\n\npairs(DartPoints[, selected_variables])\n\n\n\n\n\n\n\n\nExample of further customisation:\n\nreg &lt;- function(x, y, ...) {\n  points(x,y, ...)\n  abline(lm(y~x)) \n }\n\npanel.cor &lt;- function(x, y, digits = 2, prefix = \"\", cex.cor, ...) {\n usr &lt;- par(\"usr\"); on.exit(par(usr))\n par(usr = c(0, 1, 0, 1))\n r &lt;- abs(cor(x, y, use = \"complete.obs\"))\n txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n txt &lt;- paste0(prefix, txt)\n text(0.5, 0.5, txt, cex = 1.1, font = 4)\n}\n\npairs(DartPoints[, selected_variables], \n      upper.panel = reg, \n      lower.panel = panel.cor,\n      cex = 1.5, pch = 19, col = adjustcolor(4, .4))\n\n\n\n\n\n\n\n\n\nLogistic Regression: a statistical method used for binary classification problems. It estimates the probability of an observation being in one or another category (binary variable or categorical variable with two possible values) based on one or more independent (explanatory) variables. As the linear regression, the analysis involves calculating the parameters of a equation corresponding to a geometric object, in this case a sigmoid or logistic curve.\n\n\n# consider only cases in blade shape categories \"E\" (excurvate) and \"S\" (straight)\nDartPoints_IandS &lt;- subset(DartPoints, Blade.Sh == \"E\" | Blade.Sh == \"S\")\n\nmodel &lt;- glm(Blade.Sh ~ Length + Width + J.Width, data = DartPoints_IandS, family = \"binomial\")\nsummary(model)\n\n\nCall:\nglm(formula = Blade.Sh ~ Length + Width + J.Width, family = \"binomial\", \n    data = DartPoints_IandS)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  2.71823    1.49160   1.822   0.0684 .\nLength      -0.08122    0.03421  -2.374   0.0176 *\nWidth        0.23553    0.09825   2.397   0.0165 *\nJ.Width     -0.25868    0.13064  -1.980   0.0477 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 113.63  on 81  degrees of freedom\nResidual deviance: 104.02  on 78  degrees of freedom\nAIC: 112.02\n\nNumber of Fisher Scoring iterations: 4\n\n\nA logistic regression model with significant coefficients (p-values &lt; 0.05) can be considered good classifiers, and could help us predict or infer the binary classification from additional combinations of explanatory variables. However, a truly good predictor will normally require a higher number of cases when building up the model.\nTo visualize a logistic regression model in R, you can use ggplot2 to create a curve showing the predicted probabilities alongside the observed binary outcomes in relation to one explanatory variable (e.g., Length).\n\n# Prepare data for visualization\n# Generate a sequence of Length values and keep Width and J.Width fixed at their mean\nnew_data &lt;- data.frame(\n  Length = seq(min(DartPoints_IandS$Length), max(DartPoints_IandS$Length), length.out = 100),\n  Width = mean(DartPoints_IandS$Width, na.rm = TRUE),\n  J.Width = mean(DartPoints_IandS$J.Width, na.rm = TRUE)\n)\n\n# Add predicted probabilities\nnew_data$predicted_prob &lt;- predict(model, newdata = new_data, type = \"response\")\n\n# Plot the predicted probabilities\nggplot(new_data, aes(x = Length, y = predicted_prob)) +\n   geom_line(color = \"blue\") +\n   labs(\n      title = \"Predicted Probability of Blade Shape 'E' by Length\",\n      x = \"Length\",\n      y = \"Predicted Probability\"\n   ) +\n   theme_minimal()",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Workflow</span>"
    ]
  },
  {
    "objectID": "session-data-science-workflow.html#extrabasic-machine-learning-concepts",
    "href": "session-data-science-workflow.html#extrabasic-machine-learning-concepts",
    "title": "5  Data Science Workflow",
    "section": "5.4 (EXTRA)Basic Machine Learning Concepts",
    "text": "5.4 (EXTRA)Basic Machine Learning Concepts\n\nIntroduction to Machine Learning\n\nOverview of supervised and unsupervised learning.\n\nExample: Differentiating between regression and classification tasks.\n\n\nK-Nearest Neighbors (KNN)\n\nUnderstanding KNN for classification.\n\nExample: Implementing KNN using class package.\n\n\nClustering\n\nIntroduction to clustering techniques (e.g., k-means clustering).\n\nExample: Performing k-means clustering with kmeans and visualizing clusters.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Workflow</span>"
    ]
  },
  {
    "objectID": "session-data-science-workflow.html#extramodel-evaluation",
    "href": "session-data-science-workflow.html#extramodel-evaluation",
    "title": "5  Data Science Workflow",
    "section": "5.5 (EXTRA)Model Evaluation",
    "text": "5.5 (EXTRA)Model Evaluation\n\nTrain/Test Split\n\nSplitting data into training and testing sets.\n\nExample: Using caret package to split data and train models.\n\n\nModel Performance Metrics\n\nEvaluating model performance: accuracy, confusion matrix, ROC curve.\n\nExample: Calculating and interpreting metrics using caret and pROC.\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nBuilding a Simple Data Science Workflow: data import, cleaning, exploration, and basic regression and hypothesis testing.\n\nImport a dataset of your choosing. Consider the ones included in the archdata package (install and load the package, then consult ?archdata).\nCheck if the given dataset has any missing values, normally appearing as NA in R. Notice that some databases might use specific conventions, such as coding missing values as an odd extreme number (e.g., -999) or a specific text (e.g., indeterminate).\n\nGet a general overview of the data using one or more options shown above, depending on the dataset structure and variable types. For example, using pairs() with numeric variables or applying table() to categorical variables.\nConsider visualising and testing the relationships between the most interesting variables. For example, does the distribution of the numerical variable A varies along the categories of the categorical variable B? Use a combination of plots, pair-wise simple linear regression models, and T tests or Chi-Square tests.\n\nQ&A and Troubleshooting\n\nAddressing challenges in implementing basic data science methods in R.\n\n\n\n\n\n\n\n\nHazzan, Orit, and Koby Mike. 2023. “The Data Science Workflow.” In Guide to Teaching Data Science: An Interdisciplinary Approach, edited by Orit Hazzan and Koby Mike, 151–63. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-24758-3_10.\n\n\nKnoxville, and Methodology-University of Tennessee, Statistics. n.d. 2 Data Preparation and Cleaning in R  R Software Handbook. Accessed November 29, 2024. https://bookdown.org/aschmi11/RESMHandbook/data-preparation-and-cleaning-in-r.html.\n\n\nKopra, Juho, Santtu Tikka, Merja Heinäniemi, Sonsoles López-Pernas, and Mohammed Saqr. 2024. “An R Approach to Data Cleaning and Wrangling for Education Research.” In Learning Analytics Methods and Tutorials: A Practical Guide Using R, edited by Mohammed Saqr and Sonsoles López-Pernas, 95–119. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-54464-4_4.\n\n\nRapp, Albert. 2024. “The 6 Most Fundamental Functions for Data Cleaning with R  R-Bloggers.” https://www.r-bloggers.com/2024/06/the-6-most-fundamental-functions-for-data-cleaning-with-r/.\n\n\nSaltz, Jeff. 2020. “What Is a Data Science Workflow?” Data Science Process Alliance. https://www.datascience-pm.com/data-science-workflow/.\n\n\ntjohannsen. 2023. “Data Science Mit R.” R-Coding. https://r-coding.de/blog/data-science-mit-r/.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Workflow</span>"
    ]
  },
  {
    "objectID": "session-count-and-seriation.html",
    "href": "session-count-and-seriation.html",
    "title": "6  Count data and seriation",
    "section": "",
    "text": "6.1 Introduction to Count Data in Archaeology",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Count data and seriation</span>"
    ]
  },
  {
    "objectID": "session-count-and-seriation.html#introduction-to-count-data-in-archaeology",
    "href": "session-count-and-seriation.html#introduction-to-count-data-in-archaeology",
    "title": "6  Count data and seriation",
    "section": "",
    "text": "Understanding Count Data\n\nDefinition and significance of count data in archaeological contexts (e.g., artifact counts, feature frequencies).\n\nExample: Overview of typical archaeological datasets involving count data.\n\n\nChallenges in Analysing Count Data\n\nIssues with skewness, overdispersion, and zero inflation.\n\nExample: Common problems encountered in archaeological count data analysis.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Count data and seriation</span>"
    ]
  },
  {
    "objectID": "session-count-and-seriation.html#basic-statistical-methods-for-count-data",
    "href": "session-count-and-seriation.html#basic-statistical-methods-for-count-data",
    "title": "6  Count data and seriation",
    "section": "6.2 Basic Statistical Methods for Count Data",
    "text": "6.2 Basic Statistical Methods for Count Data\n\nPoisson and Negative Binomial Distributions\n\nIntroduction to Poisson distribution and its application to count data.\n\nExample: Fitting a Poisson model using glm in R.\n\nIntroduction to the Negative Binomial distribution for overdispersed data.\n\nExample: Fitting a Negative Binomial model using MASS::glm.nb.\n\n\nGoodness-of-Fit Testing\n\nAssessing the fit of count data models.\n\nExample: Performing a chi-square goodness-of-fit test in R.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Count data and seriation</span>"
    ]
  },
  {
    "objectID": "session-count-and-seriation.html#introduction-to-seriation",
    "href": "session-count-and-seriation.html#introduction-to-seriation",
    "title": "6  Count data and seriation",
    "section": "6.3 Introduction to Seriation",
    "text": "6.3 Introduction to Seriation\n\nWhat is Seriation?\n\nOverview of seriation techniques and their importance in archaeology for ordering artefacts or sites chronologically.\n\nExample: Historical applications of seriation in archaeology.\n\n\nSeriation Techniques\n\nIntroduction to different seriation methods (e.g., frequency seriation, contextual seriation).\n\nExample: Basic seriation using traditional methods.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Count data and seriation</span>"
    ]
  },
  {
    "objectID": "session-count-and-seriation.html#using-the-tesselle-package-for-seriation",
    "href": "session-count-and-seriation.html#using-the-tesselle-package-for-seriation",
    "title": "6  Count data and seriation",
    "section": "6.4 Using the tesselle Package for Seriation",
    "text": "6.4 Using the tesselle Package for Seriation\n\nIntroduction to tesselle\n\nOverview of the tesselle package and its tools for seriation.\n\nExample: Installation and loading of tesselle.\n\n\nPractical Seriation in R\n\nPerforming seriation.\n\nExample: Applying seriation to an archaeological dataset (e.g., pottery styles, stratigraphic data).\n\n\nVisualizing Seriation Results\n\nCreating visual representations of seriation results.\n\nExample: Plotting seriation outputs.\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nCase Study: Seriation of Archaeological Artefacts\n\nStep-by-step walkthrough of a seriation analysis using count data.\n\nExample: Seriation of pottery fragments or lithic tools using tesselle.\n\n\nQ&A and Troubleshooting\n\nAddressing common issues in count data analysis and seriation in archaeological contexts.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Count data and seriation</span>"
    ]
  },
  {
    "objectID": "session-compositional-data.html",
    "href": "session-compositional-data.html",
    "title": "7  Compositional data",
    "section": "",
    "text": "7.1 Introduction to Compositional Data in Archaeology",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compositional data</span>"
    ]
  },
  {
    "objectID": "session-compositional-data.html#introduction-to-compositional-data-in-archaeology",
    "href": "session-compositional-data.html#introduction-to-compositional-data-in-archaeology",
    "title": "7  Compositional data",
    "section": "",
    "text": "Understanding Compositional Data\n\nDefinition and examples of compositional data in archaeology (e.g., proportions of different materials, chemical compositions).\n\nExample: Typical archaeological datasets that include compositional data.\n\n\nChallenges in Analysing Compositional Data\n\nIssues with relative proportions and the “closed” nature of compositional data.\n\nExample: Limitations of traditional statistical methods on compositional data.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compositional data</span>"
    ]
  },
  {
    "objectID": "session-compositional-data.html#basic-concepts-in-compositional-data-analysis",
    "href": "session-compositional-data.html#basic-concepts-in-compositional-data-analysis",
    "title": "7  Compositional data",
    "section": "7.2 Basic Concepts in Compositional Data Analysis",
    "text": "7.2 Basic Concepts in Compositional Data Analysis\n\nOverview of methods in Multivariate statistics\n\nLog-Ratio Transformations\n\nIntroduction to log-ratio transformations (CLR, ILR, ALR) for compositional data.\n\nExample: Applying a centred log-ratio (CLR) transformation in R (e.g. nexus::transform_clr).\n\n\nVisualizing Compositional Data\n\nTechniques for visualizing compositional data (ternary plots, bar charts).\n\nExample: Creating a ternary plot using the isopleuros::ternary_plot and ggtern.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compositional data</span>"
    ]
  },
  {
    "objectID": "session-compositional-data.html#exploratory-data-analysis",
    "href": "session-compositional-data.html#exploratory-data-analysis",
    "title": "7  Compositional data",
    "section": "7.3 Exploratory Data Analysis",
    "text": "7.3 Exploratory Data Analysis\n\nPrincipal Component Analysis (PCA)\n\nIntroduction to PCA tailored for compositional data.\n\nExample: Performing PCA on compositional data using tesselle::nexus::pca.\n\nBiplots and screeplots\n\nExample: Plotting PCA results as a biplot and add visualisation aids using tesselle::dimensio functions.\n\n\nCluster Analysis\n\nOverview of clustering techniques for exploring groupings in compositional data.\n\nExample: Applying hierarchical clustering on transformed compositional data using tesselle.\n\nggplot2: dendrograms with ggraph\n\n\nCorrespondence Analysis\n\nCorrespondence Analysis for compositional data.\n\nExample: Performing Correspondence Analysis using tesselle::ca.\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nCase Study: Analysis of Compositional Data from Archaeological Sites\n\nStep-by-step walkthrough of an exploratory analysis of compositional data.\n\nExample: Analysing chemical compositions of ceramics or soils using tesselle.\n\n\nQ&A and Troubleshooting\n\nAddressing challenges in visualizing and analyzing compositional data in archaeological research.",
    "crumbs": [
      "Data Science in R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compositional data</span>"
    ]
  },
  {
    "objectID": "session-database-1.html",
    "href": "session-database-1.html",
    "title": "8  Databases (I)",
    "section": "",
    "text": "8.1 Introduction to Databases in Archaeology\nIn summary, databases are essential tools in archaeology for managing and analyzing data. Relational databases, in particular, offer significant advantages for handling structured archaeological data, ensuring data integrity, and supporting complex queries. Non-relational databases provide flexibility and scalability for handling diverse and large volumes of data, making them suitable for specific use cases in archaeology.",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Databases (I)</span>"
    ]
  },
  {
    "objectID": "session-database-1.html#introduction-to-databases-in-archaeology",
    "href": "session-database-1.html#introduction-to-databases-in-archaeology",
    "title": "8  Databases (I)",
    "section": "",
    "text": "What is a Database?\n\nDefinition and importance of databases in archaeology.\n\nA database is an organized collection of data that is stored and accessed electronically. In archaeology, databases are crucial for storing, retrieving, and managing vast amounts of data collected during excavations and research. They help archaeologists efficiently organize and analyze information, ensuring that valuable data is preserved and easily accessible for future studies.\nImportance of Databases in Archaeology:\n\nData Organization: Databases allow archaeologists to systematically organize data, making it easier to manage and retrieve information. This is essential for maintaining detailed records of archaeological findings.\nData Preservation: Databases ensure that archaeological data is preserved in a digital format, protecting it from physical degradation and loss. This is particularly important for long-term research projects\nData Sharing: Databases facilitate data sharing among researchers, enabling collaboration and the exchange of information. This enhances the overall quality of archaeological research.\nData Analysis: Databases support complex data analysis, allowing archaeologists to identify patterns, trends, and relationships within the data. This aids in the interpretation of archaeological findings.\n\nExample: Common Uses of Databases for Managing Archaeological Data:\n\nArtifacts: Databases can store detailed information about artifacts, including descriptions, photographs, dimensions, materials, and provenance. This allows archaeologists to catalog and track artifacts systematically. For example, an artifact database might include fields for the artifact’s type, material, date of discovery, and associated excavation site.\nExcavation Records: Databases can manage excavation records, such as site plans, stratigraphic layers, context sheets, and field notes. This helps in documenting the excavation process and maintaining a comprehensive record of findings. For example, an excavation database might include fields for the site name, excavation date, stratigraphic unit, and associated artifacts.\nResearch Data: Databases can store research data, including radiocarbon dates, chemical analyses, and spatial data from Geographic Information Systems (GIS). This facilitates data analysis and interpretation. For example, a research database might include fields for sample ID, analysis type, date, and results.\nBibliographic References: Databases can manage bibliographic references and literature reviews, helping archaeologists keep track of relevant publications and sources. For example, a bibliographic database might include fields for the author, title, publication year, and abstract.\n\n\n\nTypes of Databases\n\nOverview of relational vs. non-relational databases.\n\nRelational Databases: Relational databases use tables to store data and relationships between data. They follow the relational model, where data is organized into tables with rows and columns. Each table represents a specific entity, and relationships between tables are established using keys (primary and foreign keys). Relational databases use SQL (Structured Query Language) for querying and managing data. Examples include MySQL, PostgreSQL, Oracle, and Microsoft SQL Server.\n\nAdvantages:\n\nData Integrity: Relational databases enforce data integrity through constraints and relationships between tables. This ensures that data is consistent and accurate.\nStructured Data: Relational databases are well-suited for structured data, such as artifact catalogs and excavation records. Complex Queries: Relational databases support complex queries using SQL, enabling detailed searches and analyses.\nScalability: Relational databases can handle large datasets and support concurrent access by multiple users.\nData Security: Relational databases provide robust security features, including user authentication, access controls, and encryption.\n\n\nNon-Relational Databases: Non-relational databases, also known as NoSQL databases, are designed for unstructured or semi-structured data. They can handle large volumes of diverse data types and are often used for big data and real-time web applications. NoSQL databases include various types, such as document stores (e.g., MongoDB), key-value stores (e.g., Redis), column-family stores (e.g., Cassandra), and graph databases (e.g., Neo4j).\n\nAdvantages:\n\nFlexibility: Non-relational databases can store unstructured and semi-structured data, making them suitable for diverse data types.\nScalability: Non-relational databases are designed to scale horizontally, handling large volumes of data across distributed systems.\nPerformance: Non-relational databases can provide high performance for specific use cases, such as real-time data processing and big data analytics.\nSchema-less Design: Non-relational databases do not require a fixed schema, allowing for more flexible data modeling.\n\n\n\nExample: Advantages of Relational Databases for Structured Archaeological Data:\n\nData Integrity: Relational databases enforce data integrity through constraints and relationships between tables. This ensures that data is consistent and accurate, which is essential for maintaining reliable archaeological records. For example, a relational database can enforce referential integrity by ensuring that every artifact record references a valid excavation site.\nStructured Data: Relational databases are well-suited for structured data, such as artifact catalogs and excavation records. The tabular format allows for easy organization and retrieval of data. For example, an artifact table can have columns for artifact ID, type, material, and date of discovery.\nComplex Queries: Relational databases support complex queries using SQL, enabling archaeologists to perform detailed searches and analyses. For example, an archaeologist can query the database to find all artifacts made of a specific material from a particular excavation layer or to calculate the average age of artifacts from a specific site.\nScalability: Relational databases can handle large datasets and support concurrent access by multiple users. This is important for collaborative archaeological projects where multiple researchers need to access and update the database simultaneously. For example, a relational database can support multiple archaeologists entering data from different excavation sites in real-time.\nData Security: Relational databases provide robust security features, including user authentication, access controls, and encryption. This ensures that sensitive archaeological data is protected from unauthorized access. For example, a relational database can restrict access to certain tables or fields based on user roles, ensuring that only authorized personnel can view or modify sensitive data.",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Databases (I)</span>"
    ]
  },
  {
    "objectID": "session-database-1.html#relational-database-concepts",
    "href": "session-database-1.html#relational-database-concepts",
    "title": "8  Databases (I)",
    "section": "8.2 Relational Database Concepts",
    "text": "8.2 Relational Database Concepts\n\nBasic Concepts\n\nIntroduction to Tables, Records, Fields, Primary and Foreign Keys:\n\nTables: In a relational database, data is organized into tables. Each table represents a specific entity, such as artifacts, excavation sites, or researchers. Tables consist of rows and columns.\nRecords: A record (or row) in a table represents a single instance of the entity. For example, a record in an artifacts table might represent a single artifact.\nFields: A field (or column) in a table represents a specific attribute of the entity. For example, fields in an artifacts table might include artifact ID, type, material, and date of discovery.\nPrimary Keys: A primary key is a unique identifier for each record in a table. It ensures that each record can be uniquely identified. For example, an artifact ID can serve as the primary key in an artifacts table.\nForeign Keys: A foreign key is a field in one table that links to the primary key in another table. It establishes a relationship between the two tables. For example, a site ID in an artifacts table can serve as a foreign key linking to the primary key in a sites table.\n\nExample: Organizing Excavation Data in Relational Tables:\n\nSite Locations Table:\n\nFields: Site ID (Primary Key), Site Name, Location, Excavation Date\nExample Record: (1, “Site A”, “Location A”, “2023-01-01”)\n\nStratigraphy Table:\n\nFields: Stratigraphy ID (Primary Key), Site ID (Foreign Key), Layer, Description\nExample Record: (1, 1, “Layer 1”, “Topsoil layer”)\n\nFinds Table:\n\nFields: Find ID (Primary Key), Site ID (Foreign Key), Stratigraphy ID (Foreign Key), Artifact Type, Material, Date of Discovery\nExample Record: (1, 1, 1, “Pottery”, “Clay”, “2023-01-02”)\n\n\n\nBy organizing excavation data into relational tables, archaeologists can efficiently manage and retrieve information about sites, stratigraphy, and finds.\n\nNormalization\n\nIntroduction to Database Normalization: Database normalization is the process of organizing data in a database to avoid redundancy and ensure data integrity. The goal is to structure the data in such a way that each piece of information is stored only once, reducing the risk of inconsistencies and improving data accuracy.\n\nNormalization Principles:\n\nFirst Normal Form (1NF): Ensures that each field contains only atomic (indivisible) values and that each record is unique.\nSecond Normal Form (2NF): Ensures that all non-key fields are fully dependent on the primary key.\nThird Normal Form (3NF): Ensures that all non-key fields are not only dependent on the primary key but also independent of each other.\n\n\n\nExample: Structuring Artifact Data into Normalized Tables:\n\nArtifacts Table:\n\nFields: Artifact ID (Primary Key), Artifact Type ID (Foreign Key), Material ID (Foreign Key), Condition ID (Foreign Key), Date of Discovery\nExample Record: (1, 1, 1, 1, “2023-01-02”)\n\nArtifact Types Table:\n\nFields: Artifact Type ID (Primary Key), Artifact Type\nExample Record: (1, “Pottery”)\n\nMaterials Table:\n\nFields: Material ID (Primary Key), Material\nExample Record: (1, “Clay”)\n\nConditions Table:\n\nFields: Condition ID (Primary Key), Condition\nExample Record: (1, “Intact”)\n\n\nBy normalizing artifact data into separate tables for artifact types, materials, and conditions, archaeologists can avoid redundancy and ensure data integrity.\nEntity-Relationship (ER) Models\nAn Entity-Relationship (ER) model is a visual representation of the relationships between different entities in a database. It helps in designing the database structure by illustrating how entities are connected and how data flows between them.\nComponents of an ER Model:\n\nEntities: Represented by rectangles, entities are objects or concepts that have data stored about them. For example, “Site,” “Stratigraphy,” and “Finds” are entities.\nAttributes: Represented by ovals, attributes are properties or characteristics of entities. For example, “Site Name” and “Location” are attributes of the “Site” entity.\nRelationships: Represented by diamonds, relationships show how entities are connected. For example, a “Finds” entity might have a relationship with both the “Site” and “Stratigraphy” entities.\n\nExample: Designing an ER Diagram for an Archaeological Database:\n\nEntities:\n\nSite (Attributes: Site ID, Site Name, Location, Excavation Date)\nStratigraphy (Attributes: Stratigraphy ID, Site ID, Layer, Description)\nFinds (Attributes: Find ID, Site ID, Stratigraphy ID, Artifact Type, Material, Date of Discovery)\n\nRelationships:\n\nA “Site” can have multiple “Stratigraphy” layers (One-to-Many relationship).\nA “Stratigraphy” layer can have multiple “Finds” (One-to-Many relationship).\nA “Find” is associated with one “Site” and one “Stratigraphy” layer (Many-to-One relationship).\n\n\nER Diagram:\n+—————-+ +—————-+ +—————-+\n| Site | | Stratigraphy | | Finds |\n|—————-| |—————-| |—————-|\n| Site ID (PK) |&lt;——–&gt;| Site ID (FK) |&lt;——–&gt;| Site ID (FK) |\n| Site Name | | Stratigraphy ID| | Stratigraphy ID|\n| Location | | Layer | | Find ID (PK) |\n| Excavation Date| | Description | | Artifact Type |\n+—————-+ +—————-+ | Material |\n| Date of Discovery|\n+—————-+\nIn this ER diagram, the “Site” entity is related to the “Stratigraphy” entity through the “Site ID” field, and the “Stratigraphy” entity is related to the “Finds” entity through the “Stratigraphy ID” field. This visual representation helps in understanding the relationships between different archaeological datasets and designing a well-structured database.",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Databases (I)</span>"
    ]
  },
  {
    "objectID": "session-database-1.html#introduction-to-sql-structured-query-language",
    "href": "session-database-1.html#introduction-to-sql-structured-query-language",
    "title": "8  Databases (I)",
    "section": "8.3 Introduction to SQL (Structured Query Language)",
    "text": "8.3 Introduction to SQL (Structured Query Language)\n\nBasic SQL Commands\n\n\nOverview of SQL Syntax and Common Commands:\n\n\nCREATE: Used to create a new table or database. Syntax: “CREATE TABLE table_name (column1 datatype, column2 datatype, …);”\nINSERT: Used to insert new records into a table. Syntax: “INSERT INTO table_name (column1, column2, …) VALUES (value1, value2, …);”\nSELECT: Used to retrieve data from a table. Syntax: “SELECT column1, column2, … FROM table_name;”\nUPDATE: Used to modify existing records in a table. Syntax: “UPDATE table_name SET column1 = value1, column2 = value2, … WHERE condition;”\nDELETE: Used to delete records from a table. Syntax: “DELETE FROM table_name WHERE condition;”\n\nExample: Creating tables for archaeological data and inserting records.\n\nCreating a Table for Sites:\nCREATE TABLE Sites ( SiteID INT PRIMARY KEY, SiteName VARCHAR(100), Location VARCHAR(100), ExcavationDate DATE );\nInserting Records into the Sites Table: INSERT INTO Sites (SiteID, SiteName, Location, ExcavationDate) VALUES (1, ‘Site A’, ‘Location A’, ‘2023-01-01’); .\n\n\n\nCreating a Database with SQL\n\nCreate the Database:\n\nCREATE DATABASE ArchaeologyDB;\n\nUse the Database:\n\nUSE ArchaeologyDB;\n\nCreate different Tables:\n\n\nArtifact Table\nCREATE TABLE Artifacts ( ArtifactID INT PRIMARY KEY, ArtifactType VARCHAR(50), Material VARCHAR(50), Context VARCHAR(100) );\nSites Table\nCREATE TABLE Sites ( SiteID INT PRIMARY KEY, SiteName VARCHAR(100), Location VARCHAR(100), ExcavationDate DATE );\n\n\nInsert Records:\n\n\nInsert into Artifacts Table: INSERT INTO Artifacts ( ArtifactID, ArtifactType, Material, Context) VALUES (1, ‘Pottery’, ‘Clay’, ‘Layer 1’);\nInsert in Sites Table:\nINSERT INTO Sites (SiteID, SiteName, Location, ExcavationDate) VALUES (1, ‘Site A’, ‘Location A’, ‘2023-01-01’);\n\nIndexing and Keys\n\nExplanation of Primary Keys, Foreign Keys, and Indexing for Optimizing Database Performance:\n\nPrimary Keys: A primary key is a unique identifier for each record in a table. It ensures that each record can be uniquely identified.\n\nExample: ArtifactID in the Artifacts table.\n\nForeign Keys: A foreign key is a field in one table that links to the primary key in another table. It establishes a relationship between the two tables.\n\nExample: SiteID in the Artifacts table linking to SiteID in the Sites table.\n\nIndexing: Indexes are used to speed up the retrieval of data. They create a data structure that allows for faster searches.\n\nExample: Creating an index on the ArtifactType field in the Artifacts table.\n\n\nDefining keys to link excavation sites to finds in different tables:\n\nCreating Foreign Key Relationship: ALTER TABLE Artifacts ADD SiteID INTm ADD CONSTRAINT FK_Site, FOREIGN Key (SiteID) REFERENCES Sites(SiteID);\nCreating an Index: CREATE Index idx_ArtifactType ON Artifacts (ArtifactType);\n\n\nBasic Querying with SQL\nWriting Basic Queries to Retrieve Data from a Database:\n\nSelect All Records:\nSELECT * FROM Artifacts\nSelect Specific Columns:\nSELECT ArtifactID, ArtifactType, Material FROM Artifacts\n\nExample: Using SELECT Statements to Extract Information About Finds or Excavation Layers:\n\nRetrieve All Artifacts:\nSELECT * FROM Artifacts\nRetrieve Artifacts of a Specific Type:\nSELECT ArtifactID, ArtifactType, Material\nFROM Artifacts\nWHERE ArtifactType = 'Pottery'\n\nFiltering and Sorting Data\nUsing WHERE, ORDER BY, and GROUP BY Clauses to Filter and Sort Data:\n\nWHERE Clause: Used to filter records based on a condition.\n\nExample:\nSELECT * FROM Artifacts WHERE Material = 'Clay'\n\nORDER BY Clause: Used to sort the result set by one or more columns.\n\nExample:\nSELECT * FROM Artifacts ORDER BY DateOfDiscovery DESC\n\nGROUP BY Clause: Used to group records that have the same values in specified columns.\n\nExample:\n\nSELECT Material, COUNT(*) FROM Artifacts GROUP BY Material\n\nQuerying Artifacts by Material Type or Sorting Excavation Records by Date:\n\nFilter Artifacts by Material Type:\nSELECT * FROM Artifacts\nWHERE Material = 'Clay'\nSort Excavation Records by Date:\nSELECT * FROM Sites\nORDER BY ExcavationDate DESC\n\nJoining Tables\nUsing JOIN Statements to Combine Related Tables:\n\nINNER JOIN: Combines records from two tables where there is a match in both tables.\n\nSyntax:\nSELECT columns FROM table1 INNER JOIN table2 ON table1.column = table2.column\n\nLEFT JOIN: Returns all records from the left table and matched records from the right table.\n\nSyntax:\nSELECT columns FROM table1 LEFT JOIN table2 ON table1.column = table2.column\n\nRIGHT JOIN: Returns all records from the right table and matched records from the left table.\n\nSyntax:\nSELECT columns FROM table1 RIGHT JOIN table2 ON table1.column = table2.column\n\n\nWriting Queries to Retrieve Artifacts Based on Their Excavation Context:\n\nJoin Artifacts with Sites:\nSELECT Artifacts.ArtifactID, Artifacts.ArtifactType, Sites.SiteName, Sites.Location\nFROM Artifacts\nINNER JOIN Sites ON Artifacts.SiteID = Sites.SiteID\nJoin Artifacts with Stratigraphy:\nSELECT Artifacts.ArtifactID, Artifacts.ArtifactType, Stratigraphy.Layer, Stratigraphy.Description\nFROM Artifacts\nINNER JOIN Stratigraphy ON Artifacts.StratigraphyID = Stratigraphy.StratigraphyID",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Databases (I)</span>"
    ]
  },
  {
    "objectID": "session-database-1.html#database-tools-for-archaeology",
    "href": "session-database-1.html#database-tools-for-archaeology",
    "title": "8  Databases (I)",
    "section": "8.4 Database Tools for Archaeology",
    "text": "8.4 Database Tools for Archaeology\n\nAccessing Databases from R\n\nIntroduction to R packages (DBI, RSQLite, RPostgres) for interacting with databases.\n\nExample: Connecting to an SQLite or PostgreSQL database from R for archaeological data analysis.\n\n\nSQLite for small projects\n\nIntroduction to SQLite as a lightweight database solution for archaeological projects.\n\nExample: Creating a simple SQLite database for site data using R (RSQLite, e.g. https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html) or Python.\n\n\nPostgreSQL for larger projects\n\nOverview of PostgreSQL for larger archaeological datasets.\n\nExample: Setting up a PostgreSQL database in R for managing excavation records.\n\n\nQuerying Databases in R\n\nWriting SQL queries in R and retrieving data for further analysis.\n\nExample: Querying an excavation database from R and visualizing the results with ggplot2.\n\n\nData Wrangling and Cleaning\n\nUsing R to clean and manipulate database data for analysis.\n\nExample: Using dplyr in R to filter and transform queried data.\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nBuilding a Small Archaeological Database\n\nStep-by-step walkthrough of creating a database schema for a hypothetical excavation project (using RSQLite).\n\nExample: Creating tables for stratigraphy, finds, and context.\n\n\nInserting and Managing Data\n\nPractical examples of adding and managing archaeological data.\n\nExample: Inserting excavation records into the database using SQL.\n\n\nQ&A and Troubleshooting\n\nAddressing challenges in database design and SQL queries.",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Databases (I)</span>"
    ]
  },
  {
    "objectID": "session-database-2.html",
    "href": "session-database-2.html",
    "title": "9  Databases (II)",
    "section": "",
    "text": "9.1 Using GIS Databases for Archaeology",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Databases (II)</span>"
    ]
  },
  {
    "objectID": "session-database-2.html#using-gis-databases-for-archaeology",
    "href": "session-database-2.html#using-gis-databases-for-archaeology",
    "title": "9  Databases (II)",
    "section": "",
    "text": "Introduction to Spatial Databases\n\nOverview of spatial databases and their use in archaeology (e.g., storing geospatial excavation data).\n\nExample: Introduction to PostGIS for spatial data management in PostgreSQL.\n\n\nLinking GIS Data to Databases\n\nUsing databases to manage spatial data for archaeological analysis.\n\nExample: Storing site coordinates and linking them to excavation records in a spatial database.\n\n\nQuerying Spatial Data\n\nWriting spatial queries to retrieve geospatial data from a database.\n\nExample: Querying sites within a specific radius or extracting artifact distributions across layers.\n\n\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\nQuerying and Analyzing Archaeological Data\n\nWalkthrough of writing SQL queries to extract meaningful insights from archaeological data.\n\nExample: Retrieving and analyzing artifact distributions based on context and stratigraphy.\n\n\nCombining Database and Spatial Data\n\nPractical examples of integrating database queries with GIS data for archaeological site analysis.\n\nExample: Using a database to visualize the spatial distribution of artifacts across an excavation site.\n\n\nQ&A and Troubleshooting\n\nAddressing challenges in querying and analyzing archaeological databases.",
    "crumbs": [
      "Databases",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Databases (II)</span>"
    ]
  },
  {
    "objectID": "session-gis-1.html",
    "href": "session-gis-1.html",
    "title": "10  GIS (I)",
    "section": "",
    "text": "(Basic concepts, installation and setting up, loading files)\n\nCool archive video about ARCInfo (1988): https://youtu.be/7xqNyUOIRCs?si=FulmlUVzaThGE9BU\n\n(Dooley n.d.)\n\n\n\n\n\n\nHands-on Practice\n\n\n\n\n\n\n\n\n\n\nDooley, Andy MacLachlan, Adam Dennett and Claire. n.d. CASA0005 Geographic Information Systems and Science. Accessed October 9, 2024. https://andrewmaclachlan.github.io/CASA0005repo/index.html.",
    "crumbs": [
      "GIS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GIS (I)</span>"
    ]
  },
  {
    "objectID": "session-gis-2.html",
    "href": "session-gis-2.html",
    "title": "11  GIS (II)",
    "section": "",
    "text": "(get a map image displaying data from shape and raster files)\n\n\n\n\n\n\nHands-on Practice",
    "crumbs": [
      "GIS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GIS (II)</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "n.d. Accessed November 12, 2024. https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf.\n\n\n“22 Debugging  Advanced\nR.” n.d. Accessed November 12, 2024. https://adv-r.hadley.nz/debugging.html.\n\n\n“4  Debugging Pipelines – The {Targets}\nR Package User Manual.” n.d. Accessed November 12,\n2024. https://books.ropensci.org/targets/debugging.html.\n\n\n“A Grammar of Data\nManipulation.” n.d. Accessed October 29, 2024. https://dplyr.tidyverse.org/.\n\n\n“Aggregating and Analyzing Data with Dplyr.” n.d. Accessed\nOctober 29, 2024. https://datacarpentry.org/R-genomics/04-dplyr.html.\n\n\nAlbert Rapp. 2024. “Stop Code Errors\nFrom Crashing Your\nWhole R Script.” https://www.youtube.com/watch?v=4Az6pyWrUG4.\n\n\nAlboukadel. 2020. “R Coding Style\nBest Practices.” Datanovia. https://www.datanovia.com/en/blog/r-coding-style-best-practices/.\n\n\nAnderson, and Brooke, Sean Kross. n.d. 1.9 Working with\nLarge Datasets \nMastering Software Development in\nR. Accessed November 5, 2024. https://github.com/rdpeng/RProgDA.\n\n\n“Arches Project Open Source\nData Management Platform.”\n2015. https://www.archesproject.org/.\n\n\n“Ariadne Research\nInfrastructure.” n.d. Ariadne Research\nInfrastructure. Accessed October 14, 2024. https://www.ariadne-research-infrastructure.eu/.\n\n\n“Automatically Generated Release Notes.” n.d. GitHub\nDocs. Accessed October 11, 2024. https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes.\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on\nReproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\n“Basic Syntax  Markdown\nGuide.” n.d. Accessed October 11, 2024. https://www.markdownguide.org/basic-syntax/.\n\n\nBatist, Zachary, and Joe Roe. 2024. “Open\nArchaeology, Open Source?\nCollaborative Practices in an Emerging Community of\nArchaeological Software Engineers.” Internet\nArchaeology, no. 67 (July). https://doi.org/10.11141/ia.67.13.\n\n\n“Best Practices for Durable\nR Code.” n.d. Accessed November 5,\n2024. https://www.appsilon.com/post/best-practices-for-durable-r-code.\n\n\nBhalla, Deepanshu. n.d. “Dplyr Tutorial :\nData Manipulation (50\nExamples).” ListenData. Accessed October\n29, 2024. https://www.listendata.com/2016/08/dplyr-tutorial.html.\n\n\n“CARE Principles for\nIndigenous Data\nGovernance.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=CARE_Principles_for_Indigenous_Data_Governance&oldid=1246386873.\n\n\n———. n.d. https://static1.squarespace.com/static/5d3799de845604000199cd24/t/5da9f4479ecab221ce848fb2/1571419335217/CARE+Principles_One+Pagers+FINAL_Oct_17_2019.pdf.\n\n\n“CARE Principles for\nIndigenous Data Governance\n ARDC.” 2022.\nHttps://Ardc.edu.au/. https://ardc.edu.au/resource/the-care-principles/.\n\n\nCarroll, Stephanie Russo, Ibrahim Garba, Oscar L. Figueroa-Rodríguez,\nJarita Holbrook, Raymond Lovett, Simeon Materechera, Mark Parsons, et\nal. 2020. “The CARE Principles for\nIndigenous Data\nGovernance” 19 (1): 43. https://doi.org/10.5334/dsj-2020-043.\n\n\nCioara, Andrei. 2018. “How I Organize My\nGitHub Repositories.” Medium.\nhttps://andreicioara.com/how-i-organize-my-github-repositories-ce877db2e8b6.\n\n\n“Cloning and Forking a Repository —\nPythia Foundations.” n.d. Accessed\nOctober 28, 2024. https://foundations.projectpythia.org/foundations/github/github-cloning-forking.html.\n\n\n“Coalition for Archaeological\nSynthesis.” 2024. CfAS. https://www.archsynth.org/.\n\n\nCollins, Neil. 2020. “How to Create\nInteractive Reports in R\nMarkdown Part II:\nData Visualisation.” Medium.\nhttps://medium.com/@SportSciData/how-to-create-interactive-reports-in-r-markdown-part-ii-data-visualisation-6b8061136370.\n\n\nCommunity, The Turing Way. 2022. “The Turing\nWay: A Handbook for Reproducible, Ethical and\nCollaborative Research.” Zenodo. https://doi.org/10.5281/ZENODO.3233853.\n\n\n“Control Flow.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=Control_flow&oldid=1253332211.\n\n\n“Control Structures in Programming\nLanguages.” 2020. GeeksforGeeks. https://www.geeksforgeeks.org/control-structures-in-programming-languages/.\n\n\n“Coursera  Degrees,\nCertificates, & Free Online\nCourses.” n.d. Accessed October 8, 2024. https://www.coursera.org/.\n\n\n“Created New Organization in GitHub and\nZenodo Did Not Send a Request for Accessing It ·\nIssue #1596 · Zenodo/Zenodo.” n.d. GitHub.\nAccessed October 11, 2024. https://github.com/zenodo/zenodo/issues/1596.\n\n\n“Creating GitHub Releases Automatically\non Tags.” 2024. https://jacobtomlinson.dev/posts/2024/creating-github-releases-automatically-on-tags/.\n\n\ndanijar. 2019. “Can I Arrange Repositories into\nFolders on Github?” Forum post. Stack\nOverflow. https://stackoverflow.com/q/11852982/6199967.\n\n\n“Data: Starwars.” n.d. Accessed October 29, 2024. https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html.\n\n\n“Difference Between Fork and Clone in\nGitHub.” 2021. GeeksforGeeks. https://www.geeksforgeeks.org/difference-between-fork-and-clone-in-github/.\n\n\nDooley, Andy MacLachlan, Adam Dennett and Claire. n.d.\nCASA0005 Geographic\nInformation Systems and\nScience. Accessed October 9, 2024. https://andrewmaclachlan.github.io/CASA0005repo/index.html.\n\n\n“Dplyr Package in R\nProgramming.” 2020. GeeksforGeeks. https://www.geeksforgeeks.org/dplyr-package-in-r-programming/.\n\n\nEditor, CSRC Content. n.d. “Metadata - Glossary\n CSRC.” Accessed October 14, 2024. https://csrc.nist.gov/glossary/term/metadata.\n\n\nEngel, Claudia A. n.d.a. Chapter 2 Interactive Graphs\n Data Visualization with\nR. Accessed October 29, 2024. https://cengel.github.io/R-data-viz/interactive-graphs.html.\n\n\n———. n.d.b. Data Visualization with\nR. Accessed October 29, 2024. https://cengel.github.io/R-data-viz/.\n\n\n“Exceptions and Debugging · Advanced\nR.” n.d. Accessed November 12, 2024. http://adv-r.had.co.nz/Exceptions-Debugging.html.\n\n\n“FAIR Principles.” n.d. GO\nFAIR. Accessed October 7, 2024. https://www.go-fair.org/fair-principles/.\n\n\nFrerebeau, Nicolas. 2023. Tesselle: Easily\nInstall and Load ’Tesselle’\nPackages. Pessac, France: Université Bordeaux\nMontaigne. https://doi.org/10.5281/zenodo.6500491.\n\n\n“Function Reference.” n.d. Accessed October 29, 2024. https://ggplot2.tidyverse.org/reference/.\n\n\n“Functional Documentation with Markdown and Version\nControl.” 2017. Leon Hassan. https://blog.leonhassan.co.uk/functional-documentation-with-markdown-and-version-control/.\n\n\nGillespie, Colin, and Robin Lovelace. n.d. Efficient R\nProgramming. Accessed November 5, 2024. https://csgillespie.github.io/efficientR/.\n\n\n“Git - Gitglossary Documentation.” n.d.\nAccessed October 11, 2024. https://git-scm.com/docs/gitglossary.\n\n\n“Git Definitions and Terminology\nCheat Sheet.” n.d. Accessed October 11,\n2024. https://www.pluralsight.com/resources/blog/cloud/git-terms-explained.\n\n\n“GitHub Glossary.” n.d. GitHub Docs.\nAccessed October 28, 2024. https://docs.github.com/en/get-started/learning-about-github/github-glossary.\n\n\n“Google’s R Style\nGuide.” n.d. Accessed November 5, 2024. https://web.stanford.edu/class/cs109l/unrestricted/resources/google-style.html.\n\n\nGrolemund, Garrett, J. J. Allaire. n.d. R Markdown:\nThe Definitive Guide.\nAccessed October 29, 2024. https://bookdown.org/yihui/rmarkdown/.\n\n\nGrolemund, Hadley Wickham and Garrett. n.d.a. 27 R\nMarkdown  R for Data\nScience. Accessed November 12, 2024. https://r4ds.had.co.nz/r-markdown.html.\n\n\n———. n.d.b. Welcome  R for\nData Science. Accessed November 5, 2024.\nhttps://r4ds.had.co.nz/.\n\n\nHazzan, Orit, and Koby Mike. 2023. “The Data\nScience Workflow.” In Guide to\nTeaching Data Science:\nAn Interdisciplinary\nApproach, edited by Orit Hazzan and Koby Mike, 151–63.\nCham: Springer International Publishing. https://doi.org/10.1007/978-3-031-24758-3_10.\n\n\nHealy, Yan Holtz and Conor. n.d. “From Data to Viz\n Find the Graphic You Need.” Accessed\nOctober 29, 2024. https://www.data-to-viz.com/data-to-viz.com.\n\n\nHiebel, Gerald, Gert Goldenberg, Caroline Grutsch, Klaus Hanke, and\nMarkus Staudt. 2021. “FAIR Data for Prehistoric\nMining Archaeology.” International Journal on Digital\nLibraries 22 (3): 267–77. https://doi.org/10.1007/s00799-020-00282-8.\n\n\nHoltz, Yan. n.d.a. “Data Visualization with R and\nGgplot2  the R Graph\nGallery.” Accessed October 29, 2024. https://r-graph-gallery.com/ggplot2-package.html.\n\n\n———. n.d.b. “The R Graph\nGallery – Help and Inspiration for\nR Charts.” The R Graph Gallery. Accessed\nOctober 29, 2024. https://r-graph-gallery.com/.\n\n\n“Home – CAA/SSLA.” n.d. Accessed\nOctober 14, 2024. https://sslarch.github.io/.\n\n\n“Home  CIDOC CRM.”\nn.d. Accessed October 14, 2024. https://www.cidoc-crm.org/.\n\n\n“How to Get Familiar with Forking &\nCloning GitHub Repos.” 2023. DEV\nCommunity. https://dev.to/joshhortt/how-to-get-familiar-with-forking-cloning-github-repos-46nc.\n\n\nIatropoulou, Katerina. n.d. “OpenAIRE.”\nOpenAIRE. Accessed October 7, 2024. https://www.openaire.eu/.\n\n\n“Indexing — R Spatial.” n.d.\nAccessed October 29, 2024. https://rspatial.org/intr/4-indexing.html.\n\n\n“Indexing and Slicing Data\nFrames in R.” 2022.\nGeeksforGeeks. https://www.geeksforgeeks.org/indexing-and-slicing-data-frames-in-r/.\n\n\n“Indexing into a Data Structure.” n.d. Accessed October 29,\n2024. http://www.cookbook-r.com/Basics/Indexing_into_a_data_structure/.\n\n\n“Introduction.” n.d. Accessed October 29, 2024. https://rmarkdown.rstudio.com/lesson-1.html.\n\n\n“Introduction to Programming\nLanguages.” 2018. GeeksforGeeks. https://www.geeksforgeeks.org/introduction-to-programming-languages/.\n\n\n“Introduction to R - Refactoring the\nWorkshop.” n.d. HackMD. Accessed November\n5, 2024. https://hackmd.io/@libjohn/Bkpg0zp3I.\n\n\n“Issue a Doi with Zenodo.” n.d. Github for\nCollaborative Documentation. Accessed October 11, 2024. https://cassgvp.github.io/github-for-collaborative-documentation/docs/tut/6-Zenodo-integration.html.\n\n\njimmy. 2022. “How to Organize GitHub\nRepositories.” Backrightup. https://backrightup.com/blog/how-to-organize-github-repositories/.\n\n\nKabacoff, Robert. n.d. Modern Data\nVisualization with R. Accessed October\n29, 2024. https://rkabacoff.github.io/datavis/.\n\n\nKennedy, Hannah. 2019. “Best Practices for\nUnit Testing and Linting with\nR.” Medium. https://medium.com/@mindlessroman/best-practices-for-unit-testing-and-linting-with-r-6d40be95391c.\n\n\nKnoxville, and Methodology-University of Tennessee, Statistics. n.d.\n2 Data Preparation and\nCleaning in R  R\nSoftware Handbook. Accessed November 29,\n2024. https://bookdown.org/aschmi11/RESMHandbook/data-preparation-and-cleaning-in-r.html.\n\n\nKopra, Juho, Santtu Tikka, Merja Heinäniemi, Sonsoles López-Pernas, and\nMohammed Saqr. 2024. “An R Approach to\nData Cleaning and Wrangling for\nEducation Research.” In Learning\nAnalytics Methods and Tutorials:\nA Practical Guide\nUsing R, edited by Mohammed Saqr and\nSonsoles López-Pernas, 95–119. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-54464-4_4.\n\n\nLamprecht, Anna-Lena, Leyla Garcia, Mateusz Kuzak, Carlos Martinez,\nRicardo Arcila, Eva Martin Del Pico, Victoria Dominguez Del Angel, et\nal. 2020. “Towards FAIR Principles\nFor&nbsp;research&nbsp;software.” Data Science 3\n(1): 37–59. https://doi.org/10.3233/DS-190026.\n\n\n“Learn More.” n.d. Accessed November 5, 2024. https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html.\n\n\n“Learn R, Python & Data\nScience Online.” n.d. Accessed October\n8, 2024. https://www.datacamp.com.\n\n\nLiang, Hao. 2024. “Hao203/Rmarkdown-YAML.” https://github.com/hao203/rmarkdown-YAML.\n\n\n“License (GPL-3).” n.d. Accessed November 12,\n2024. https://cran.r-project.org/web/packages/tryCatchLog/vignettes/tryCatchLog-intro.html.\n\n\nLien-Talks, Alphaeus. 2024. “How FAIR Is\nBioarchaeological Data: With a\nParticular Emphasis on Making\nArchaeological Science Data\nReusable.” Journal of Computer Applications in\nArchaeology 7 (1). https://doi.org/10.5334/jcaa.154.\n\n\nLipkin, Gus. 2022. “Writing Faster R\nwith Vectorization and the {Apply} Family.”\nMedium. https://guslipkin.medium.com/writing-faster-r-with-vectorization-and-the-apply-family-ff6078a2583a.\n\n\nMajumder, Prateek. 2021. “Guide to Create\nInteractive Plots with Plotly\nPython.” Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/10/interactive-plots-in-python-with-plotly-a-complete-guide/.\n\n\n“Making Your First R\nPackage.” n.d. Accessed November 12, 2024. https://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html.\n\n\n“Markdown Guide.” n.d. Accessed October 7,\n2024. https://www.markdownguide.org/.\n\n\nMarwick, Ben. 2017. “Open Science in\nArchaeology,” January. https://doi.org/10.17605/OSF.IO/3D6XX.\n\n\nMelfi, Vince, Jeffrey W. Doser. n.d. Chapter 3 Scripts\nand R Markdown  R\nProgramming for Data\nSciences. Accessed November 12, 2024. https://doserlab.com/files/for875/_book/scripts.\n\n\nMéo, Grace Di. 2023. “Creating Interactive\nVisualizations with Plotly.”\nProgramming Historian, December. https://programminghistorian.org/en/lessons/interactive-visualization-with-plotly.\n\n\n“Metadata.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=Metadata&oldid=1250210606.\n\n\n“Module-5-Open-Research-Software-and-Open-Source/Content_development/Task_2.md\nat Master ·\nOpenScienceMOOC/Module-5-Open-Research-Software-and-Open-Source.”\nn.d. GitHub. Accessed October 11, 2024. https://github.com/OpenScienceMOOC/Module-5-Open-Research-Software-and-Open-Source/blob/master/content_development/Task_2.md.\n\n\nNational Academies of Sciences, Engineering, Policy and Global Affairs,\nEngineering Committee on Science, Board on Research Data Information,\nDivision on Engineering and Physical and Sciences, Committee on Applied\nand Theoretical Statistics, Board on Mathematical Sciences Analytics, et\nal. 2019. “Understanding Reproducibility and\nReplicability.” In Reproducibility and\nReplicability in Science. National\nAcademies Press (US). https://www.ncbi.nlm.nih.gov/books/NBK547546/.\n\n\nNFDI4Objects. 2024. “NFDI4Objects.”\nNFDI4Objects. http://195.37.32.58:4000/.\n\n\nNHS-R Community. 2021. “Plotting Interactive Visualizations with\nPlotly in R.” https://www.youtube.com/watch?v=WmofiOklux8.\n\n\nNicholson, Christopher, Sarah Kansa, Neha Gupta, and Rachel Fernandez.\n2023. “Will It Ever Be\nFAIR?: Making Archaeological\nData Findable, Accessible,\nInteroperable, and Reusable.”\nAdvances in Archaeological Practice 11 (1): 63–75. https://doi.org/10.1017/aap.2022.40.\n\n\nObregon, Alexander. 2024. “What Is\nMarkdown? Uses and Benefits\nExplained.” Medium. https://medium.com/@AlexanderObregon/what-is-markdown-uses-and-benefits-explained-947300e1f955.\n\n\n“Online Courses - Learn\nAnything, On Your\nSchedule  Udemy.” n.d.\nAccessed October 8, 2024. https://www.udemy.com/.\n\n\n“Open Source.” 2024. Wikipedia. https://en.wikipedia.org/w/index.php?title=Open_source&oldid=1248809486.\n\n\nOpen Source Archaeology:\nEthics and Practice. 2015. De Gruyter\nOpen Poland. https://doi.org/10.1515/9783110440171.\n\n\n“Open-Archaeo.” n.d. Title. Accessed October 14,\n2024. https://open-archaeo.info/.\n\n\n“OpenAtlas.” n.d. Accessed October 14, 2024.\nhttps://openatlas.eu/.\n\n\nPhillips, Nathaniel D. n.d. YaRrr! The\nPirate’s Guide to R.\nAccessed October 29, 2024. https://bookdown.org/ndphillips/YaRrr/.\n\n\n“Prerequisites.” n.d. Accessed November 5, 2024. https://cran.r-project.org/web/packages/validate/vignettes/cookbook.html.\n\n\n“Programming with R: Best\nPractices for Writing R\nCode.” n.d. Accessed November 5, 2024. https://swcarpentry.github.io/r-novice-inflammation/06-best-practices-R.html.\n\n\nQin, Yufei. n.d. “Research Guides: Data\nVisualization in R:\nIntroduction.” Accessed October 29, 2024. https://libguides.princeton.edu/c.php?g=1350384&p=9965533.\n\n\nR Core Team. 2024. R: A Language and\nEnvironment for Statistical\nComputing. Vienna, Austria: R Foundation for\nStatistical Computing. https://www.R-project.org/.\n\n\n“R Dtplyr: How to Efficiently\nProcess Huge Datasets with a\nData.table Backend.” n.d. Accessed November 5, 2024.\nhttps://www.appsilon.com/post/r-dtplyr.\n\n\n“R Markdown.” n.d. Accessed October 29, 2024.\nhttps://rmarkdown.rstudio.com/index.html.\n\n\n“R Markdown Reporting Best\nPractices.” n.d. Accessed November 12, 2024. https://www.appsilon.com/post/r-markdown-reporting-best-practices.\n\n\n“R Variables and Constants.” n.d.\nAccessed November 5, 2024. https://www.programiz.com/r/variables-and-constants.\n\n\n“R Variables and Constants\n(With Examples).” n.d. Accessed\nNovember 5, 2024. https://www.datamentor.io/r-programming/variable-constant.\n\n\nRapp, Albert. 2024. “The 6 Most\nFundamental Functions for Data\nCleaning with R \nR-Bloggers.” https://www.r-bloggers.com/2024/06/the-6-most-fundamental-functions-for-data-cleaning-with-r/.\n\n\n“Referencing and Citing Content.” n.d. GitHub\nDocs. Accessed October 11, 2024. https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content.\n\n\nRiederer, Emily, Christophe Dervieux. n.d. 2.2 R\nMarkdown Anatomy  R\nMarkdown Cookbook. Accessed November 12,\n2024. https://bookdown.org/yihui/rmarkdown-cookbook/rmarkdown-anatomy.html.\n\n\n“Rmarkdown :: Cheatsheet.” n.d. Accessed\nOctober 14, 2024. https://rstudio.github.io/cheatsheets/html/rmarkdown.html?_gl=1*1k59g6x*_ga*MjAyMzI2NjEwMC4xNzIzODI3Mjk2*_ga_2C0WZ1JHG0*MTcyODkxNzc2NS4xMy4xLjE3Mjg5MTkwNTIuMC4wLjA.\n\n\n“RStudio IDE ::\nCheatsheet.” n.d. Accessed October 14, 2024. https://rstudio.github.io/cheatsheets/html/rstudio-ide.html?_gl=1*11i4y2y*_ga*MjAyMzI2NjEwMC4xNzIzODI3Mjk2*_ga_2C0WZ1JHG0*MTcyODkxNzc2NS4xMy4xLjE3Mjg5MTkwNTIuMC4wLjA.\n\n\n“RStudio User Guide -\nRStudio IDE User\nGuide.” 2024. RStudio User Guide. https://docs.posit.co/ide/user/.\n\n\nSaltz, Jeff. 2020. “What Is a Data\nScience Workflow?” Data Science\nProcess Alliance. https://www.datascience-pm.com/data-science-workflow/.\n\n\nSignell, Rich. 2013. “How to Handle Releases of Markdown Document\non Github.” Forum post. Stack Overflow. https://stackoverflow.com/q/19727632/6199967.\n\n\n“Single-Responsibility Principle - Wikipedia.”\nn.d. Accessed November 5, 2024. https://en.wikipedia.org/wiki/Single-responsibility_principle.\n\n\nSmith, Zachary M. n.d. 9 Lesson 4: YAML\nHeaders  R Markdown\nCrash Course. Accessed November 12, 2024.\nhttps://zsmith27.github.io/rmarkdown_crash-course/lesson-4-yaml-headers.html.\n\n\n“Spatial Without Compromise · QGIS\nWeb Site.” n.d. Accessed October 7,\n2024. https://qgis.org/.\n\n\nStatistikinDD. 2021. “Creating Your\nFirst R Package in 2\nMinutes in RStudio!” https://www.youtube.com/watch?v=47PN2VG3RmI.\n\n\n“Style Guide · Advanced R.” n.d.\nAccessed November 5, 2024. http://adv-r.had.co.nz/Style.html.\n\n\n“Subsetting Data  R\nLearning Modules.” n.d. Accessed\nOctober 29, 2024. https://stats.oarc.ucla.edu/r/modules/subsetting-data/.\n\n\nSuhail, Muhammad Ahmed. 2024. “Structuring and\nOrganizing My Github: A\nDeveloper’s Guide.” Medium. https://medium.com/@muhammadahmedsuhail007/structuring-and-organizing-my-github-a-developers-guide-7353610f04fd.\n\n\n“Tesselle: R Packages &\nArchaeology.” n.d. Tesselle. Accessed\nOctober 14, 2024. https://www.tesselle.org/.\n\n\n“The Open Source\nDefinition.” n.d. Open Source Initiative.\nAccessed October 14, 2024. https://opensource.org/osd.\n\n\n“The Validation Set\nApproach in R Programming -\nGeeksforGeeks.” n.d. Accessed November 5, 2024. https://www.geeksforgeeks.org/the-validation-set-approach-in-r-programming/.\n\n\n“Tidyverse.” n.d. Accessed October 14, 2024. https://www.tidyverse.org/.\n\n\n“Tidyverse Style Guide.” n.d. Accessed November 5, 2024. https://style.tidyverse.org/index.html.\n\n\ntjohannsen. 2023. “Data Science Mit\nR.” R-Coding. https://r-coding.de/blog/data-science-mit-r/.\n\n\n“Welcome  Advanced\nR.” n.d. Accessed November 5, 2024. https://adv-r.hadley.nz/.\n\n\n“What Is FAIR?” n.d. Accessed October 14,\n2024. https://www.howtofair.dk/what-is-fair/.\n\n\n“What Is Markdown? Syntax,\nExamples, Usage, Best\nPractices.” 2021. https://www.knowledgehut.com/blog/web-development/what-is-markdown.\n\n\n“What Is Metadata and How Does It Work?” n.d.\nWhatIs. Accessed October 14, 2024. https://www.techtarget.com/whatis/definition/metadata.\n\n\n“What Is Programming? And\nHow To Get\nStarted.” 2024. Coursera. https://www.coursera.org/articles/what-is-programming.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding\nPrinciples for Scientific Data Management and\nStewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.\n\n\n“Writing R Packages in Rstudio.”\nn.d. Accessed November 12, 2024. https://ourcodingclub.github.io/tutorials/writing-r-package/.\n\n\n“Zenodo - Research. Shared.” n.d.\nAccessed October 11, 2024. https://help.zenodo.org/docs/profile/linking-accounts/.\n\n\nZestyclose-Low-6403. 2023. “How to Organize Repos Within an\n’Organization’?” Reddit {Post}. R/Github. www.reddit.com/r/github/comments/188d324/how_to_organize_repos_within_an_organization/.",
    "crumbs": [
      "References"
    ]
  }
]