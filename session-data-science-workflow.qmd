# Data Science Workflow {#sec-data-science-workflow}

## Introduction to Data Science Workflow

In a typical data science workflow, we move through five primary stages: **data collection**, **data cleaning**, **exploration**, **modeling**, and **communication**. These steps collectively transform raw data into actionable insights driven by research questions while also enabling efficient and reproducible data science practices.

1. **Data Collection**: This is the first step, where data is gathered from various sources, such as databases, APIs, or web scraping. In R, packages like `readr` and `httr` facilitate efficient data import from structured files (e.g., CSV, Excel) and web sources.

2. **Data Cleaning**: Data cleaning involves preparing raw data for analysis, handling missing values, correcting data types, and dealing with outliers. Tools like `dplyr` and `tidyr` are often used in R to perform these operations, enabling tasks like removing duplicates, imputing missing data, and restructuring data into a “tidy” format suitable for analysis.

3. **Data Exploration**: Exploratory Data Analysis (EDA) is the phase where we examine the data's characteristics, uncover patterns, and form hypotheses. Visualizations (using `pairs()` or a wide range of plot types available in Base R and `ggplot2`) and summary statistics (`summary()`, `skimr`) help in understanding the data distribution, relationships between variables, and identifying any anomalies.

4. **Modeling**: At this stage, statistical or machine learning models are developed to predict or explain outcomes. In R, packages like `caret` and `tidymodels` streamline the modelling process, from splitting data and selecting models to tuning hyperparameters and evaluating performance.

5. **Communication**: The final stage focuses on presenting findings clearly, often through reports, dashboards, or interactive applications. Using R Markdown for reports or Shiny for interactive applications enables data scientists to effectively communicate insights to stakeholders.

### Quick example in R: hypothetical local dataset "house_prices.csv" {.unnumbered}

Let’s consider a simple example of a data science project where we predict house prices based on available features:

- **Data Collection**: Load a dataset such as `house_prices.csv` using `read_csv()`.
- **Data Cleaning**: Use `dplyr` to handle missing values (e.g., `mutate_if(is.na, median)` to replace with median values) and convert categorical variables to factors.
- **Data Exploration**: Visualize relationships between features like square footage and price using `ggplot(data = houses) + geom_point(aes(x = sqft, y = price))`.
- **Modeling**: Create a linear model with `lm(price ~ sqft + num_bedrooms, data = houses)`.
- **Communication**: Report results in an R Markdown document, showing model coefficients, predictions, and visualizations to explain findings.

### Quick example in R: canonical dataset `USArrests` {.unnumbered}

Let’s consider a simple example of a data science project. Suppose we aim to determine main factors in criminality using the canonical dataset `USArrests`.

1. **Data Collection**: Load the dataset directly from R’s built-in datasets.
```{r}
data("USArrests")
```

2. **Data Cleaning**: Check for any missing values or potential outliers that might represent data entry errors.
```{r}
# Adding a missing value at row 1, column 1
USArrests[1, 1] <- NA

# Check for missing values
any(is.na(USArrests))

# remove missing value
USArrests_clean <- na.omit(USArrests)
```
   
3. **Data Exploration**: Visualize and summarize data to understand patterns. For instance, we might be interested in the value distribution of murder arrests.
```{r}
# Quick summary of the data distribution
summary(USArrests_clean)

# Visualize the distribution of artefact "types" (represented by variables like 'Murder' here)
layout(matrix(1:2, nrow = 2), heights = c(2, 1.5))
par(mar = c(0, 4, 4, 2) + 0.1)
hist(USArrests_clean$Murder, main="Distribution of counts of murder arrests in 'USArrests'", xaxt = "n")
par(mar = c(2, 4, 0, 2) + 0.1)
boxplot(USArrests_clean$Murder, horizontal = TRUE)
```
   This combined histogram and box plot would give a quick view of the central tendencies and spread of murder arrests across the dataset. For example, we can already observe that the mean and median of the distribution (`r mean(USArrests_clean$Murder)` and `r median(USArrests_clean$Murder)`, respectively) are on the lower half of the range (`r range(USArrests_clean$Murder)`), which also imply a longer right tail of the distribution (i.e., the difference between maximum and mean is greater than the one between mean and minimum).

4. **Modelling**: Create a simple model to analyse relationships between variables. For instance, let’s assume we’re exploring how `UrbanPop` (population percentage in urban areas) might relate to `Murder`.
```{r}
model <- lm(Murder ~ UrbanPop, data = USArrests_clean)
summary(model)
```
The results of a simple linear regression analysis provides a basic understanding of how one variable might be related on another, interpreted also as *dependent*, presumably causal relationship. In this case, there is **no statistically significant evidence** that urban population percentage is a relevant factor in murder arrests in our dataset. 

5. **Communication**: Present findings in an R Markdown document, incorporating both visualizations and model summaries. You can interpret results to suggest whether murder arrests are more common in highly populated regions.

::: {.callout-note collapse="true"}
#### See also {.unnumbered}

- @hazzan_data_2023  
- @tjohannsen_data_2023  
- @saltz_what_2020  

:::

## Data Science Workflow in R: Data Import and Preparation

### Importing Data

A critical step in any data science workflow is importing data from external sources. R provides robust tools for importing data from a variety of formats:

- **CSV files:** Use the `read.csv()` function from base R.  
- **Excel files:** Leverage the `readxl` package with `read_excel()`.  
- **Databases:** Employ the `DBI` package to connect to relational databases and fetch data using SQL queries.  

#### Examples using base R, `readxl` and `DBI`+`RSQLite` {.unnumbered}

```{r}
#| eval: false
# Load necessary libraries
library(readxl)
library(DBI)
library(RSQLite)

# Read a CSV file
csv_data <- read.csv("data.csv")

# Read an Excel file
excel_data <- readxl::read_excel("data.xlsx")

# Connect to a SQLite database and fetch data
con <- DBI::dbConnect(RSQLite::SQLite(), "data.db")
db_data <- DBI::dbGetQuery(con, "SELECT * FROM table_name")
DBI::dbDisconnect(con)
```

### Data Cleaning

Before analysis, raw data often requires cleaning to address issues like missing values, duplicates, and inconsistencies.

#### Example using Base R {.unnumbered}

* **Removing Missing Values**  
Use `na.omit()` to remove rows with missing values or `is.na()` to identify them.

```{r}
# Example data
data <- data.frame(A = c(1, 2, NA, 4), B = c("x", NA, "y", "z"))
print(data)

# Remove rows with NA
clean_data <- na.omit(data)
print(clean_data)
```

* **Handling Duplicates**  
Use `duplicated()` to identify duplicate rows or `unique()` to retain only unique rows.

```{r}
data <- data.frame(A = c(1, 2, 2, 4), B = c("x", "y", "y", "z"))
print(data)

# Remove duplicate rows
data_unique <- data[!duplicated(data), ]
print(data_unique)
```

* **Replacing Values**  
Replace specific values with `ifelse()` or direct indexing.

```{r}
data <- data.frame(A = c(1, 2, 999, 4), B = c("x", "y", "z", "999"))
print(data)

# Replace 999 with NA
data[data == 999] <- NA
print(data)
```

#### Example using `tidyverse` {.unnumbered}

Key functions include:  
- **`tidyr`:** Tools like `fill()` (fill missing values) and `drop_na()` (remove rows with NAs).  
- **`dplyr`:** Functions like `distinct()` to remove duplicates and `mutate()` to fix inconsistencies. 

```{r, warning=FALSE}
library(dplyr)
library(tidyr)

# Example dataset
data <- data.frame(id = c(1, 2, 2, 3, NA), value = c(NA, "A", "A", "B", "C"))
print(data)

# Clean the data
cleaned_data <- data %>%
  drop_na(id) %>% # Remove rows with missing IDs
  distinct() %>%  # Remove duplicates
  fill(value, .direction = "down") # Fill missing values downward
print(cleaned_data)
```

### Data Transformation

Transforming data is essential for reshaping and preparing it for analysis. 

#### Example using Base R {.unnumbered}

Base R provides versatile and efficient tools for cleaning and transforming data.

A few example operations common in data science workflows are:

* **Filtering Rows**  
Subset data using logical conditions.

```{r}
data <- data.frame(A = 1:5, B = letters[1:5])
print(data)

# Filter rows where A > 3
filtered_data <- data[data$A > 3, ]
print(filtered_data)
```

* **Selecting Columns**  
Use indexing to select specific columns.

```{r}
# Select column A
selected_columns <- data[, "A", drop = FALSE]
print(selected_columns)
```

NOTE: the argument `drop = FALSE` ensures that the original data frame structure is not lost in the process (run `data[, "A"]` to compare).

* **Adding or Modifying Columns**  
Use the `$` operator or indexing to create or modify columns.

```{r}
data$new_col <- data$A * 2
print(data)
```

* **Reshaping Data**  
Use `reshape()` to pivot data between wide and long formats. The wide format is where each variable corresponds to a column (table format) while the long format assign variable-value pairs to different rows, retaining one or more variables in the wide format. 

```{r}
# Example wide format
data <- data.frame(id = 1:2, Q1 = c(10, 20), Q2 = c(30, 40))

# Convert to long format
long_data <- reshape(data, direction = "long", varying = list(c("Q1", "Q2")), v.names = "value", timevar = "quarter")
print(long_data)
```

#### Example using `tidyverse` {.unnumbered}

Use `dplyr` for:  
* **Filtering and selecting rows/columns:** `filter()`, `select()`.  
* **Creating new variables:** `mutate()`.  
* **Reshaping:** `pivot_longer()` and `pivot_wider()`.  

```{r, warning=FALSE}
library(dplyr)
library(tidyr)

# Example dataset
data <- data.frame(
  id = 1:3,
  Q1 = c(10, 20, 30),
  Q2 = c(15, 25, 35)
)
print(data)

# Perform all operations sequentially in a single step 
transformed_data <- data %>%
  pivot_longer(cols = starts_with("Q"), names_to = "quarter", values_to = "value") %>%
  filter(value > 15) %>%  # Filter rows where value > 15
  mutate(value_scaled = value / max(value))  # Add a new scaled column
print(transformed_data)
```

By mastering these data preparation steps, you ensure a clean and well-structured dataset, setting the stage for effective analysis and visualization.

::: {.callout-note collapse="true"}
#### See also {.unnumbered}

- @knoxville_2_nodate  
- @rapp_6_2024  
- @kopra_r_2024  

:::

## Exploratory Data Analysis

### Univariate Statistics

#### Numeric variables {.unnumbered}

This section equips you to explore univariate distributions of numeric variables, uncovering insights from centrality to variability with both statistical and visual techniques.

* **Histograms**: Exploring a single variable involves visualizing its distribution to identify patterns such as central tendency, spread, and outliers. Histograms are one of the most effective tools for this.  

#### Example: Using `ggplot2` for histograms {.unnumbered}

```{r}
library(ggplot2)

# Example dataset - variable with normal distribution
data <- data.frame(value = rnorm(1000, mean = 50, sd = 10))

# Create a histogram
ggplot(data, aes(x = value)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Values", x = "Value", y = "Frequency")
```

* **Range**: The range of a variable provides a simple measure of the spread of the data. It is calculated as the difference between the maximum and minimum values. Base R already has a function named `range()` to handle this calculation: 

```{r}
cat("Min:", min(data$value), "Max:", max(data$value))

range_val <- range(data$value)

cat("Range (Max - Min):", range_val)
```

* **Central tendency measures**: the mean, median, and mode describe the centre of the distribution.  
* **Dispersion measures**: variance and standard deviation describe the spread of the data.  

```{r}
mean_val <- mean(data$value)
median_val <- median(data$value)
variance <- var(data$value)
std_dev <- sd(data$value)

# Print results
cat("Mean:", mean_val, "Median:", median_val, "Variance:", variance, "SD:", std_dev)
```

#### Example: Plotting a Histogram and Marking the Mean {.unnumbered}

Overlay the mean on a histogram to visualize its position relative to the distribution.  

```{r}
ggplot(data, aes(x = value)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1) +
  labs(title = "Histogram with Mean Marked", x = "Value", y = "Frequency")
```

* **Box plots**: A box plot of a single variable can be useful to visualise central tendency and dispersion measures at the same time. It offers the same information of an histogram, though in a more analytical form, by default plotting the median (strong line), the 1st and 3rd or interquartile range (box), the (pseudo)range or 1.5-interquartile range (lines), and *outliers* (points outside the 1.5x interquartile range).

#### Example: Using `ggplot2` for an univariate box plot {.unnumbered}

```{r}
library(ggplot2)

# Example dataset - variable with normal distribution
data <- data.frame(value = rnorm(1000, mean = 50, sd = 20))

# Create a histogram
ggplot(data, aes(x = value)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Values", x = "Value", y = "Frequency")
```

#### Non-numeric variables {.unnumbered}

These approaches allow for comprehensive exploration of univariate statistics for categorical variables, emphasizing both numeric summaries and visual insights.

Univariate statistics for non-numeric (categorical) variables focus on summarizing and visualizing the distribution of categories. Here’s a breakdown with examples:

* **Frequency Tables**: A frequency table lists the counts of each category, helping to understand the distribution.

```{r}
# Example Dataset
data <- data.frame(category = sample(c("A", "B", "C"), size = 100, replace = TRUE))

# Frequency Table
table(data$category)
```

* **Bar Plots**: Bar plots visually represent the frequency distribution of categories.

```{r}
library(ggplot2)

# Bar Plot
ggplot(data, aes(x = category)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Category Distribution", x = "Category", y = "Count")
```

* **Proportion Visualization**: Proportions provide relative frequencies, useful for comparing categorical data.

```{r}
# Proportion Table
prop_table <- prop.table(table(data$category))

# Pie Chart
ggplot(data, aes(x = "", fill = category)) +
  geom_bar(width = 1) +
  coord_polar("y") +
  labs(title = "Category Proportions")
```

* **Mode**: The mode is the most frequently occurring category.

```{r}
# Mode Calculation
mode_category <- names(which.max(table(data$category)))
cat("Mode:", mode_category)
```

### Bivariate statistics

Loading `DartPoints` dataset from `archdata`:

```{r}
library(archdata)
data(DartPoints)
```

* **Scatter Plots**: Visualize relationships between two **numerical variables**.
  
```{r}
ggplot(DartPoints, aes(x = H.Length, y = Weight)) +
   geom_point() +
   labs(x = "Haft element length (mm)", y = "Weight (gm)")
```

* **Box Plots**: Compare a **numerical variable** across categories of a **categorical variable**. Ideal for comparing central tendency and dispersal measurements between groups or categories.
  
```{r}
ggplot(DartPoints, aes(x = Haft.Sh, y = H.Length)) +
   geom_boxplot() +
   scale_x_discrete(labels = c("Angular", "Excurvate", "Incurvate", "Recurvate", "Straight")) +
   labs(x = "Shape lateral haft element", y = "Haft element length (mm)")
```

* **Bar Plots with two variables (stacked)**: Compare counts or proportions of **categorical variables**.

```{r}
ggplot(DartPoints, aes(x = Haft.Sh, fill = Should.Sh)) +
   geom_bar() +
   scale_x_discrete(labels = c("Angular", "Excurvate", "Incurvate", "Recurvate", "Straight")) +
   scale_fill_discrete(labels = c("Excurvate", "Incurvate", "Straight", "None")) +
   labs(x = "Shape lateral haft element", y = "Count", fill = "Shoulder shape")
```

* **Contingency Tables**: Quick assessment of the distribution of counts among two **categorical variables**.

```{r}
table(DartPoints$Haft.Sh, DartPoints$Should.Sh)
```

* **Mosaic Plots**: A hybrid between contingency tables and stacked bar plots, mosaic plots are useful especially when counts in cross-category groups (i.e. cells in contingency table) are greater than 0. A mosaic plot represents the conditional relative frequency for a cell in the contingency table as the area of rectangular tiles. Adding a shaded color, it can also be used to visualise the deviation from the expected frequency (residual) from a Pearson Chi-square or Likelihood Ratio G^2 test.

```{r, fig.height=6}
mosaicplot(table(DartPoints$Haft.Sh, DartPoints$Should.Sh),
           xlab = "Shape lateral haft element",
           ylab = "Shoulder shape",
           main = "",
           shade = TRUE)
```

* **Correlation**: Measure the direction and strength of linear relationships between **numerical variables**. In base R, the function `cor()` will return the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) by default.

```{r}
cor(DartPoints$H.Length, DartPoints$Weight)
```

* [**Simple linear Regression**](https://en.wikipedia.org/wiki/Simple_linear_regression): Calculate the parameters (intercept, slope) for a linear model with the minimum distance towards data points in two numerical variables. Geometrically, such model is equivalent to a line in a two-dimensional plane.

With Base R:
```{r}
model <- lm(Weight ~ H.Length, data = DartPoints,)
summary(model)

plot(DartPoints$H.Length, DartPoints$Weight,
     xlab = "Haft element length (mm)", 
     ylab = "Weight (gm)")
abline(model, col = "red", lwd = 5)
# or
abline(a = model$coefficients["(Intercept)"], 
       b = model$coefficients["H.Length"],
       col = "blue", lty = 3, lwd = 5)
```

With `ggplot2`, a linear model can be added directly to a plot with `geom_smooth(method = "lm")`:
```{r}
ggplot(DartPoints, aes(x = H.Length, y = Weight)) +
   geom_point() +
   geom_smooth(method = "lm", color = "red")
```

The function `geom_smooth()` will add by default a shaded area around the line, representing the confidence interval (see argument `se` and `level` in `?geom_smooth()`).

* **Visualizing multiple correlation pairs**

Quick visualisation of a correlation matrix using `cor()`
```{r}
cor(DartPoints[, c("Length", "Width", "Thickness")])
```

Build a larger correlation matrix (only numerical variables and excluding cases with missing values) and plot it using `corrplot()` from the `corrplot` package:

```{r}
library(corrplot)

selected_variables <- c("Length", "Width", "Thickness", "B.Width", "J.Width", "H.Length", "Weight")

corr_matrix <- cor(DartPoints[, selected_variables],
                   use = "complete.obs")

corrplot(corr_matrix, method = "circle")
```

* **Hypothesis Testing**:

**t-Test**: Compare means of **numerical variables** across two **categories**. Conventionally, P-value < 0.05 means that the null hypothesis (there are *no differences between the means* of these variables) is very unlikely.

```{r}
# consider only cases in blade shape categories "E" (excurvate) and "S" (straight)
DartPoints_IandS <- subset(DartPoints, Blade.Sh == "E" | Blade.Sh == "S")

# apply test for Weight between the blade shape two categories
t.test(Weight ~ Blade.Sh, data = DartPoints_IandS)
```
In this case, the evidence is insufficient for demonstrating that there is a consistent difference in weight between dart points with excurvate and straight blades.

**Chi-Square Test**: Test independence between **categorical variables**. Conventionally, P-value < 0.05 means that the null hypothesis (the variables *are* independent) is very unlikely.

```{r}
chisq.test(table(DartPoints$Haft.Sh, DartPoints$Haft.Or))
```

In this case, the evidence supports, with 95% confidence, that haft shape and orientation are not independent.

* **Quasi-multivariate approaches**: 

Visualise **multiple subsets of a bivariate relationship** by splitting plots by a **categorical variable**.

"Faceting" **scatter plots** with `ggplot2`:
```{r}
ggplot(DartPoints, aes(x = Thickness, y = Weight)) +
   geom_point() +
   facet_wrap(~ Blade.Sh)
```

Visualise **multiple pairwise bivariate relationships** with `pairs()` (only numeric variables):
```{r}
selected_variables <- c("Length", "Width", "Thickness", "B.Width", "J.Width", "H.Length", "Weight")

pairs(DartPoints[, selected_variables])
```
Example of further customisation:
```{r, warning=FALSE}
reg <- function(x, y, ...) {
  points(x,y, ...)
  abline(lm(y~x)) 
 }

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
 usr <- par("usr"); on.exit(par(usr))
 par(usr = c(0, 1, 0, 1))
 r <- abs(cor(x, y, use = "complete.obs"))
 txt <- format(c(r, 0.123456789), digits = digits)[1]
 txt <- paste0(prefix, txt)
 text(0.5, 0.5, txt, cex = 1.1, font = 4)
}

pairs(DartPoints[, selected_variables], 
      upper.panel = reg, 
      lower.panel = panel.cor,
      cex = 1.5, pch = 19, col = adjustcolor(4, .4))
```

* [**Logistic Regression**](https://en.wikipedia.org/wiki/Logistic_regression): a statistical method used for binary classification problems. It estimates the probability of an observation being in one or another category (*binary variable* or categorical variable with two possible values) based on one or more independent (explanatory) variables. As the linear regression, the analysis involves calculating the parameters of a equation corresponding to a geometric object, in this case a sigmoid or logistic curve. 

```{r}
# consider only cases in blade shape categories "E" (excurvate) and "S" (straight)
DartPoints_IandS <- subset(DartPoints, Blade.Sh == "E" | Blade.Sh == "S")

model <- glm(Blade.Sh ~ Length + Width + J.Width, data = DartPoints_IandS, family = "binomial")
summary(model)
```
A logistic regression model with significant coefficients (p-values < 0.05) can be considered good classifiers, and could help us predict or infer the binary classification from additional combinations of explanatory variables. However, a truly good predictor will normally require a higher number of cases when building up the model.

To visualize a logistic regression model in R, you can use `ggplot2` to create a curve showing the predicted probabilities alongside the observed binary outcomes in relation to one explanatory variable (e.g., `Length`).

```{r}
# Prepare data for visualization
# Generate a sequence of Length values and keep Width and J.Width fixed at their mean
new_data <- data.frame(
  Length = seq(min(DartPoints_IandS$Length), max(DartPoints_IandS$Length), length.out = 100),
  Width = mean(DartPoints_IandS$Width, na.rm = TRUE),
  J.Width = mean(DartPoints_IandS$J.Width, na.rm = TRUE)
)

# Add predicted probabilities
new_data$predicted_prob <- predict(model, newdata = new_data, type = "response")

# Plot the predicted probabilities
ggplot(new_data, aes(x = Length, y = predicted_prob)) +
   geom_line(color = "blue") +
   labs(
      title = "Predicted Probability of Blade Shape 'E' by Length",
      x = "Length",
      y = "Predicted Probability"
   ) +
   theme_minimal()
```

## (EXTRA)Basic Machine Learning Concepts

* **Introduction to Machine Learning**  
  * Overview of supervised and unsupervised learning.  
  * Example: Differentiating between regression and classification tasks.  
* **K-Nearest Neighbors (KNN)**  
  * Understanding KNN for classification.  
  * Example: Implementing KNN using `class` package.  
* **Clustering**  
  * Introduction to clustering techniques (e.g., k-means clustering).  
  * Example: Performing k-means clustering with `kmeans` and visualizing clusters.

## (EXTRA)Model Evaluation

* **Train/Test Split**  
  * Splitting data into training and testing sets.  
  * Example: Using `caret` package to split data and train models.  
* **Model Performance Metrics**  
  * Evaluating model performance: accuracy, confusion matrix, ROC curve.  
  * Example: Calculating and interpreting metrics using `caret` and `pROC`.

::: {.callout-note icon=false}
## Hands-on Practice {.unnumbered}

* **Building a Simple Data Science Workflow**: data import, cleaning, exploration, and basic regression and hypothesis testing.  
   1. Import a dataset of your choosing. Consider the ones included in the `archdata` package (install and load the package, then consult `?archdata`).
   2. Check if the given dataset has any missing values, normally appearing as `NA` in R. Notice that some databases might use specific conventions, such as coding missing values as an odd extreme number (e.g., `-999`) or a specific text (e.g., `indeterminate`).  
   3. Get a general overview of the data using one or more options shown above, depending on the dataset structure and variable types. For example, using `pairs()` with numeric variables or applying `table()` to categorical variables.
   4. Consider visualising and testing the relationships between the most interesting variables. For example, does the distribution of the numerical variable A varies along the categories of the categorical variable B? Use a combination of plots, pair-wise simple linear regression models, and T tests or Chi-Square tests. 

* **Q\&A and Troubleshooting**  
   * Addressing challenges in implementing basic data science methods in R.

:::
