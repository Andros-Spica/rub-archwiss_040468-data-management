# Best practices in programming {#sec-r-programming}

## Code organisation

### Modular programming

**Importance of modularity**  
Breaking down your code into functions and modules enhances readability, maintainability, and reusability. This approach helps isolate specific tasks and allows for easier debugging and testing.

::: {.callout-note}
#### Single-responsability Principle {.unnumbered} 

The Single-responsibility Principle (SRP) is a key concept in software design that ensures each function or module in your code has one, and only one, reason to change. For students learning R, this means breaking down your code into smaller, well-defined functions where each function does one specific task. This makes your code easier to understand, test, and maintain [@noauthor_single-responsibility_nodate].

For example, instead of writing one long script that loads data, cleans it, and plots it, you can write separate functions for each task:
- `load_data()` handles loading the data.
- `clean_data()` takes care of data cleaning.
- `plot_data()` is responsible for plotting.

By adhering to SRP, you reduce the chance of introducing bugs when modifying or extending your code. If the way data is loaded changes, you only need to adjust `load_data()` without worrying about unintended side effects on `clean_data()` or `plot_data()`.

:::

#### **Example: Creating in-script custom functions** {.unnumbered}  
```r
# Custom function to calculate mean
calculate_mean <- function(data) {
  mean(data, na.rm = TRUE)
}

# Usage
numbers <- c(1, 2, 3, NA, 5)
calculate_mean(numbers)
```

#### **Example: Wrapping messy code into clear functions** {.unnumbered}  
**Messy Code Example:**
```{r, eval=FALSE}
#| eval: false
# Messy and hard to follow
data <- read.csv("data.csv")
data$cleaned <- na.omit(data$column)
data$scaled <- (data$cleaned - mean(data$cleaned)) / sd(data$cleaned)
hist(data$scaled)
```

**Wrapped in functions clearly defined:**
```{r, eval=FALSE}
#| eval: false
# Function to load data
load_data <- function(filepath) {
  read.csv(filepath)
}

# Function to clean data
clean_data <- function(data, column) {
  na.omit(data[[column]])
}

# Function to scale data
scale_data <- function(data) {
  (data - mean(data)) / sd(data)
}

# Function to plot histogram
plot_histogram <- function(data) {
  hist(data)
}

# Main execution sequence
main <- function() {
  raw_data <- load_data("data.csv")
  cleaned_data <- clean_data(raw_data, "column")
  scaled_data <- scale_data(cleaned_data)
  plot_histogram(scaled_data)
}

# Run the main function
main()
```

By organizing the operations into clear, single-responsibility functions, the code becomes more readable, maintainable, and easier to debug.

#### **Example: Creating and importing custom R scripts** {.unnumbered}  
1. **Creating a script:** Save the following function in a file named `utility_functions.R`.
   ```{r, eval=FALSE}
   #| eval: false
   # utility_functions.R
   calculate_sum <- function(data) {
     sum(data, na.rm = TRUE)
   }
   ```
2. **Importing the script:**
   ```{r, eval=FALSE}
   #| eval: false
   source("utility_functions.R") # loads all functions defined inside the script
   numbers <- c(1, 2, 3, NA, 5)
   calculate_sum(numbers)
   ```

### Code structuring

**Structuring a data science project**  
A well-organized project structure separates code, data, and outputs, which facilitates efficient project management.

#### **Example: Setting up a basic project structure in R and RStudio** {.unnumbered}   
1. **Folder Structure:**
   ```
   my_project/  
   ├── data/  
   │   └── raw_data.csv  
   ├── scripts/  
   │   ├── 01_load_data.R  
   │   └── 02_analyse_data.R  
   │   └── 03_visualise_data.R  
   ├── outputs/  
   │   └── analysis_results.csv  
   └── workflow.R  
   └── my_project.Rproj  
   ```
   
2. **Script Example:**  
   - `01_load_data.R`
     ```{r, eval=FALSE}
     #| eval: false
     load_data <- function(file_name, dir = "data", save_rds = FALSE) {
       file_path <- paste(dir, file_name, sep = "/")
       # load raw data
       raw_data <- read.csv(paste(file_path, "csv", sep = "."))
       if (save_rds) {
         # save a copy as a R dataset (.rds)
         saveRDS(raw_data, file = paste(file_path, "rds", sep = "."))
       }
     }
     ```

   - `02_analyse_data.R`
     ```{r, eval=FALSE}
     #| eval: false
     analyze_data <- function() {
       summary_stats <- summary(raw_data)
       print(summary_stats)
       # Save the results to an output file
       write.csv(summary_stats, "outputs/analysis_results.csv")
     }
     ```

   - `03_visualise_data.R`
     ```{r, eval=FALSE}
     #| eval: false
     visualise_data <- function(dataset, plot_name, dir = "outputs", width = 480, height = 480) {
       file_path <- paste(dir, plot_name, sep = "/")
       file_name <- paste(file_path, "png", sep = ".")
       png(file_name, width = 480, height = 480)
       pairs(dataset) # matrix of scatterplots
       dev.off()
     }
     ```
     
   - `workflow.R`
     ```{r, eval=FALSE}
     source("01_load_data.R")
     source("02_analyze_data.R")
     
     dt <- load_data("raw_data")
     
     analyze_data(dt)
     
     visualise_data(dt, "Variables overview", width = 560, height = 560)
     ```

This structure ensures clarity, with each component of the project clearly demarcated, promoting better workflow and collaboration.

::: {.callout-note collapse="true"}
#### See also {.unnumbered}

- @grolemund_welcome_nodate  
- @gillespie_efficient_nodate  
- @noauthor_welcome_nodate  

:::

## Writing clean and readable code

Code is effectively written for a computer to "understand" and execute (i.e., *machine readable*). However, humans revise, learn, and expand code (*geek* humans, but humans all the same!). Therefore, when writing code, you should remember a few practices that will make it easier to read and understand by humans (i.e., more *human-readable*).

### Naming conventions

#### **Bad practices:** {.unnumbered}    
1. **Non-descriptive variable names**  
   Using vague names makes it hard to understand the purpose of a variable.  
   ```{r, eval=FALSE}
   #| eval: false
   # Bad naming
   a <- 22.5
   b <- function(x) {
     mean(x)
   }
   ```
   
2. **Inconsistent naming styles**  
   Mixing different styles (e.g., camelCase, snake_case, and inconsistent capitalization) leads to confusion.  
   ```{r, eval=FALSE}
   #| eval: false
   # Inconsistent naming
   calcMean <- function(X) {
     mean(X)
   }
   calculate_mean <- function(x) {
     mean(x)
   }
   ```

#### **Best practices:** {.unnumbered}  
1. **Using descriptive and consistent names**  
   Choose clear and consistent names to convey purpose.  
   ```{r, eval=FALSE}
   #| eval: false
   # Good naming conventions
   average_temperature <- 22.5
   calculate_mean <- function(values) {
     mean(values)
   }
   ```
   For more guidance, refer to the tidyverse style guide [@noauthor_tidyverse_nodate-1].

### Commenting and documentation

#### **Bad practices:** {.unnumbered}   
1. **Lack of comments**  
   Skipping comments leads to difficulty in understanding the purpose or logic.  
   ```{r, eval=FALSE}
   #| eval: false
   # No comments
   data <- read.csv("data.csv")
   result <- data[data$score > 10, ]
   ```
2. **Over-commenting obvious code**  
   Commenting on trivial lines unnecessarily clutters the code.  
   ```{r, eval=FALSE}
   #| eval: false
   # Adding 1 to x
   x <- x + 1
   ```

#### **Best practices:** {.unnumbered}  
1. **Meaningful comments and using `roxygen2`**  
   Document non-obvious logic and function purpose clearly.
   ```{r, eval=FALSE}
   #| eval: false
   #' Filter data by score
   #'
   #' This function filters data for scores greater than 10.
   #' @param data A dataframe with a score column
   #' @return Filtered dataframe
   filter_high_scores <- function(data) {
     data[data$score > 10, ]
   }
   ```
   For more about the `roxygen2` package, see its "vignette" [@noauthor_learn_nodate-1].

### Avoiding magic numbers and hardcoding

::: {.callout-note}
#### Magic numbers in programming {.unnumbered} 

**Magic numbers** refer to unique numeric values embedded directly in the code without context or explanation. These numbers often appear arbitrary and can make the code difficult to understand and maintain. They are problematic because:
1. **Lack of clarity**: Without meaningful names, the purpose of the number is unclear to others (or to you in the future).  
2. **Hard to update**: If the same number appears in multiple places, updating it becomes error-prone and tedious.  
3. **Reduced readability**: Magic numbers obscure the intent of the code, making it harder to follow and debug.  

:::

#### **Bad practices:** {.unnumbered}    
1. **Using magic numbers**  
   Hardcoding values without explanation can confuse users about their significance.  
   ```{r, eval=FALSE}
   #| eval: false
   # Hardcoded magic number
   for (i in 1:7) {
     print(i)
   }
   ```

#### **Best practices:** {.unnumbered}    
1. **Defining constants**  
   Clearly define constants for better clarity and easy updates.  
   ```{r, eval=FALSE}
   #| eval: false
   # Using constants
   DAYS_IN_WEEK <- 7
   for (i in 1:DAYS_IN_WEEK) {
     print(i)
   }
   ```

::: {.callout-note collapse="true"}
#### See also {.unnumbered}

- @noauthor_tidyverse_nodate-1  
- @alboukadel_r_2020  
- @noauthor_r_nodate-2  
- @noauthor_style_nodate  
- @noauthor_r_nodate-3  
- @noauthor_googles_nodate  

:::

### Writing efficient and scalable code

Efficient and scalable R code is crucial when dealing with large datasets or computationally intensive tasks. Scalability refers to the code's ability to handle increasing amounts of data or complexity without significant performance degradation.

#### **Vectorization** {.unnumbered}  

Vectorization involves replacing explicit loops with vectorized operations, which are more efficient and concise. This approach leverages R's optimized internal functions to operate on entire vectors or matrices in one go, reducing execution time.

**Examples:**
- **Base R**:  
  ```{r, eval=FALSE}
  #| eval: false
  # Loop approach
  result <- numeric(1000)
  for (i in 1:1000) {
    result[i] <- i^2
  }

  # Vectorized approach
  result <- (1:1000)^2
  ```

- **Using `dplyr`**:  
  ```{r, eval=FALSE}
  #| eval: false
  library(dplyr)
  data <- tibble(x = 1:1000)
  
  # Vectorized operation with dplyr
  data <- data %>%
    mutate(square = x^2)
  ```

#### **Memory Management** {.unnumbered}  

Efficient memory management is essential when working with large datasets to prevent memory exhaustion and improve performance. `data.table` is a package specifically designed for handling large datasets efficiently by minimizing memory usage and speeding up data operations.

**Example:**
- **Using `data.table`**:  
  ```{r, eval=FALSE}
  #| eval: false
  library(data.table)
  
  # Creating a large data.table
  dt <- data.table(x = rnorm(1e7), y = rnorm(1e7))
  
  # Efficiently computing on large datasets
  dt[, mean_x := mean(x)]
  ```

`data.table` optimizes memory usage by modifying data in place and avoids unnecessary copies, making it ideal for large-scale data processing.

::: {.callout-note collapse="true"}
#### See also {.unnumbered}

- @anderson_19_nodate  
- @noauthor_r_nodate-4  
- @lipkin_writing_2022  

:::

## Refactoring

Refactoring refers to the process of improving existing code without changing its external behaviour. The goal is to make the code cleaner, more efficient, and easier to maintain. Refactoring enhances readability, reduces complexity, and simplifies future modifications.

### Why Refactor?
- **Improves code readability**: Well-structured code is easier to understand.
- **Enhances maintainability**: Simplified codebase reduces the time required for future updates or debugging.
- **Promotes reusability**: Modularized and clean code is more reusable in different contexts.

### Examples of Refactoring in R

#### **1. Removing Redundant Code** {.unnumbered}  
Before:  
```{r, eval=FALSE}
#| eval: false
data <- read.csv("data.csv")
data_cleaned <- na.omit(data)
data_cleaned <- data_cleaned[data_cleaned$score > 10, ]
```
After:  
```{r, eval=FALSE}
#| eval: false
data <- read.csv("data.csv") %>%
  na.omit() %>%
  filter(score > 10)
```
Using `dplyr` pipelines makes the code concise and easier to read.

#### **2. Breaking Down Long Functions** {.unnumbered}  
Before:  
```{r, eval=FALSE}
#| eval: false
calculate_metrics <- function(data) {
  mean_value <- mean(data$score)
  sd_value <- sd(data$score)
  return(list(mean = mean_value, sd = sd_value))
}
```
After:  
```{r, eval=FALSE}
#| eval: false
calculate_mean <- function(data) {
  mean(data$score)
}

calculate_sd <- function(data) {
  sd(data$score)
}

calculate_metrics <- function(data) {
  list(mean = calculate_mean(data), sd = calculate_sd(data))
}
```
Breaking down long functions into smaller ones improves readability and reusability.

#### **3. Replacing Hardcoded Values with Constants** {.unnumbered}  
Before:  
```{r, eval=FALSE}
#| eval: false
if (length(data) > 1000) {
  print("Large dataset")
}
```
After:  
```{r, eval=FALSE}
#| eval: false
THRESHOLD <- 1000
if (length(data) > THRESHOLD) {
  print("Large dataset")
}
```
Using constants improves clarity and ease of updating values.

::: {.callout-note collapse="true"}
#### See also {.unnumbered}

- @noauthor_introduction_nodate-1    
- @noauthor_programming_nodate  

:::

## EXTRA: Testing and validation

### Writing Unit Tests

Unit tests are essential for ensuring code correctness by verifying that each function behaves as expected. They help catch errors early and make code more robust by facilitating easy modifications and refactoring.

#### **Example: Writing basic unit tests in R (`testthat`)** {.unnumbered}  
1. Install `testthat` package:  
   ```{r, eval=FALSE}
   #| eval: false
   install.packages("testthat")
   ```
2. Create a test script (`test_calculate_mean.R`):  
   ```{r, eval=FALSE}
   #| eval: false
   library(testthat)

   calculate_mean <- function(x) {
     if (!is.numeric(x)) stop("Input must be numeric")
     mean(x, na.rm = TRUE)
   }

   test_that("calculate_mean works correctly", {
     expect_equal(calculate_mean(c(1, 2, 3)), 2)
     expect_equal(calculate_mean(c(NA, 2, 3)), 2.5)
     expect_error(calculate_mean("string"))
   })
   ```
3. Run the tests:  
   ```{r, eval=FALSE}
   #| eval: false
   test_file("test_calculate_mean.R")
   ```

### Data Validation

Validating data inputs and outputs is crucial for maintaining data integrity, especially in data processing pipelines. It ensures that the data conforms to expected formats and values, avoiding downstream errors.

#### **Example: Implementing data validation checks in data processing scripts** {.unnumbered}  
1. Install `validate` package:  
   ```{r, eval=FALSE}
   #| eval: false
   install.packages("validate")
   ```
2. Validate data: 
   ```{r, eval=FALSE}
   #| eval: false
   library(validate)

   rules <- validator(
     age >= 0,
     salary > 0,
     !is.na(name)
   )

   data <- data.frame(
     age = c(25, -5, 30),
     salary = c(50000, 0, 60000),
     name = c("Alice", NA, "Bob")
   )

   # Validate the data
   check <- confront(data, rules)
   summary(check)
   ```  
   This checks that `age` is non-negative, `salary` is positive, and `name` is not missing.

::: {.callout-note collapse="true"}
#### See also {.unnumbered}

- @noauthor_prerequisites_nodate  
- @noauthor_best_nodate  
- @noauthor_validation_nodate  
- @kennedy_best_2019  

:::

## (EXTRA)Error Handling and Debugging

* **Error Handling Techniques**  
  * Using tryCatch in R.  
  * Writing meaningful error messages.   
  * Example: Implementing error handling in a data processing script.  
* **Debugging Tools**  
  * Introduction to debugging tools: `browser` in R.  
  * Example: Walkthrough of a debugging session in R.

## (EXTRA)Code Reusability and Sharing

* **Creating Reusable Code**  
  * Writing functions and libraries for reuse across projects.  
  * Example: Creating a simple R package.  
* **Sharing Code**  
  * Sharing code with others: publishing packages, sharing notebooks.  
  * Example: Publishing an R package on CRAN/GitHub.

::: {.callout-note icon=false}
## Hands-on Practice {.unnumbered}

* **Refactoring Code (30min)**  
  * First attempt: Refactoring a sample script to follow best practices (clean code, modularity, documentation).  
  * Second attempt: try using a Large Language Model (LLM) to refactor.  
* **Collaborative Exercise (40min)**  
  * Simulating a collaborative workflow with Git: making and reviewing pull requests.  
  * Groups of two or three  
  * Re-use one of the repositories created in GitHub (Session 2\) and populated by R code and output files (Session 3).  
  * Mutual reviews and change suggestions.  
  * Discussion, accepting/rejecting changes, and merge decision  
* **Open discussion (10min)**  
  * Addressing common challenges in applying best practices to real-world projects.

:::
